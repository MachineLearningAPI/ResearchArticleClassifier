A Survey on Policy Search for Robotics
Work Package 3 (Policy Search Methods and Cost Functions Better Suited for
Real-World Reinforcement Learning), Deliverable D3.1.2

Marc Peter Deisenroth1∗ , Gerhard Neumann1∗ , Jan Peters1,2
1

Department of Computer Science, Technische Universit¨at Darmstadt, Germany
2
Max Planck Institute for Intelligent Systems, Germany

February 22, 2013

∗

Both authors contributed equally.

2

Contents
1 Introduction
1.1 Robot Control as a Reinforcement Learning Problem
1.2 Policy Search Categorization . . . . . . . . . . . . .
1.3 Typical Policy Representations . . . . . . . . . . . .
1.4 Model-free and Model-based Policy Search . . . . . .
1.5 Outline . . . . . . . . . . . . . . . . . . . . . . . . .
2 Model-free Policy Search
2.1 Policy Evaluation Strategies . . . . . . . . . . .
2.1.1 Step-Based Policy Evaluation . . . . . .
2.1.2 Episode-Based Policy Evaluation . . . .
2.1.3 Qualitative Comparison . . . . . . . . .
2.2 Policy Update Strategies . . . . . . . . . . . .
2.2.1 Policy Gradient Methods . . . . . . . .
2.2.2 Expectation Maximization Policy Search
2.2.3 Information-Theoretic Approaches . . .
2.2.4 Miscellaneous Important Methods . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

1
1
3
4
5
6

. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
Approaches
. . . . . . .
. . . . . . .

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

7
8
8
9
11
12
13
24
33
43

.
.
.
.
.
.
.
.
.
.
.
.
.
.

49
53
54
55
56
56
58
61
61
62
64
64
65
67
70

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

3 Model-based Policy Search
3.1 Probabilistic Forward Models . . . . . . . . . . . . . . . . . . . . .
3.1.1 Locally Weighted Bayesian Regression . . . . . . . . . . . .
3.1.2 Gaussian Process Regression . . . . . . . . . . . . . . . . .
3.2 Long-Term Predictions with a Given Model . . . . . . . . . . . . .
3.2.1 Sampling-based Trajectory Prediction: PEGASUS . . . . .
3.2.2 Deterministic Long-Term Predictions . . . . . . . . . . . . .
3.3 Policy Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 Model-based Policy Updates without Gradient Information
3.3.2 Model-based Policy Updates with Gradient Information . .
3.3.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Model-based Policy Search Algorithms with Robot Applications . .
3.4.1 Sampling-based Trajectory Prediction . . . . . . . . . . . .
3.4.2 Deterministic Trajectory Predictions . . . . . . . . . . . . .
3.4.3 Overview of Model-based Policy Search Algorithms . . . . .
i

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

3.5

Discussion . . . . . . . . . . . . . . . . . . . . .
3.5.1 Deterministic and Stochastic Long-Term
3.5.2 Treatment of Model Uncertainty . . . .
3.5.3 Extrapolation Properties of Models . . .

.
.
.
.

71
71
72
72

4 Conclusion
A
Gradients of Frequently Used Policies . . . . . . . . . . . . . . . . . . . .
B
Weighted ML Estimates of Frequently Used Policies . . . . . . . . . . . .
C
Derivations of the Dual Functions for REPS . . . . . . . . . . . . . . . . .

74
82
83
84

ii

. . . . . . .
Predictions
. . . . . . .
. . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

Abstract
Policy search methods have been successfully applied to learning robot controllers, where
the state and control spaces are typically high dimensional and continuous. We review
recent successes of both model-free and model-based policy search in robot learning.
Model-free policy search is a general, sampling-based approach to learn policies in
the absence of a model of the robot’s dynamics. Therefore, in our overview on modelfree policy search, we focus on evaluation and update strategies of policies based on
sampled trajectories. The flexibility of model-free policy search comes with a price:
For each sampled trajectory, it is necessary to interact with the robot, which can be
time consuming and practically challenging. Model-based policy search addresses this
problem by first learning a simulator of the robot’s dynamics from data. Subsequently,
the simulator generates trajectories that are used for policy learning. Since the quality of
the simulator is at the core of model-based policy search, in our overview of model-based
policy search, we focus on commonly used simulators and different ways of using them
for generating trajectories. For both model-free and model-based policy search methods,
we review their respective properties and their applicability to robotic systems.

Chapter 1

Introduction
Robots have been playing an increasingly important role in our lives. From simple
house-cleaning robots over robotic wheelchairs and general transport robots the number
and variety of robots are rapidly increasing. So far, the controllers for these robots are
largely pre-programmed. Programming robots is a tedious task that requires years of
experience. The resulting programmed controllers are based on exact modeling of the
robot’s behavior and its environment. Therefore, programming robots has its limitations
when a robot has to adapt to new situations or when the robot (or the environment) itself
cannot be modeled sufficiently accurately. In robot learning, machine learning methods
are used to automatically extract relevant information from data. Using the power and
flexibility of modern machine learning techniques the field of robot control can be further
automated, and the gap toward robots for general assistance in households, elderly care,
and public service can be narrowed substantially.

1.1

Robot Control as a Reinforcement Learning Problem

In most tasks, robots are in a high dimensional state x composed of both internal
states (e.g., joint angles, joint velocities, end-effector, and body positions/orientations)
and external states (e.g., the surface, wind conditions, or other robots). Robots are
always guided by a parametrized control policy π(u|x, θ), which allows them to generate motor commands u that alter the internal state of the robot and may also affect the environment. Jointly, the states and actions of the robot form trajectories
τ = (x0 , u0 , x1 , u1 , . . . ), which are also often called rollouts or paths.
We assume that a numeric performance scoring system exists that evaluates the
performance of the robot system for a whole trajectory R(τ ), e.g., the sum of a high
reward for task achievement in the final stage and many lower intermediate costs (i.e.,
negative rewards) that can punish high energy consumption as well as proximity to the
joint limits. Typically, the long-term reward R(τ ) of a trajectory is given as
T −1

R(τ ) = rT (xT ) +

r(xt , ut ) ,
t=0

1

(1.1)

where r is an instantaneous reward function and rT a final reward. Instead of this
finite-horizon case, the infinite-horizon case is also frequently considered, where
∞

γ t r(xt , ut ) ,

R(τ ) =

(1.2)

t=0

where γ ∈ [0, 1) is a discount factor, essentially assigning rewards in far future a discounted value. Hence, many tasks in robotics can be accomplished by choosing a (locally)
optimal control policy π ∗ that maximizes the expected accumulated reward
J π = E[R(τ )|π] =

R(τ )p(τ |π)dτ ,

(1.3)

where
p(τ |π) = p(x0 )

t

p(xt+1 |xt , ut )πθ (ut |xt , t)

(1.4)

is the distribution over trajectories τ for a given policy πθ . Note that the policy in
Equation (1.4) can explicitly depend on the time step t. Moreover, the formulation in
Equation (1.4) allows for stochastic and deterministic policies. In the case of a stochastic
policy, π is a probability distribution with πθ (ut |xt , t) = p(ut |xt , t, θ), whereas for a
deterministic policy, the policy π is a function of the state, the time index, and the
policy parameters, such that the control is given as ut = π(xt , t, θ).
With this general problem set-up, many tasks in robotics can be naturally formulated
as Reinforcement Learning (RL) problems. However, high-dimensional continuous state
and action spaces, strong real-time requirements, and a general lack of measured realrobot data pose major challenges for the application of Value Function reinforcement
learning methods [67] in robotics. Value function methods, such as Q-learning or TDlearning , are strongly affected by these challenges as they require filling the complete
state-action space with data. Moreover, simply searching the optimum of the value
function for a single action, as required by many approaches, can be as hard as finding
an optimal parametrized policy. Hence, Policy Search, which directly operates in policy
space, is often the preferred alternative in robotics since it can cope better with those
constraints while it domain-appropriate pre-structured policies as well as a teacher’s
demonstration can easily be integrated. In robotics, the control policies π can often
be restricted to a class of parametrized policies πθ determined fully by a set of policy
parameters θ: First, the restriction to policies parametrized by θ reduce the policy
search space, which often simplifies the optimization. Second, there are often known
well-suited policy representations such as motor primitives [31], which are useful when
only incomplete low-level software access to the robot is available or the component
designed by the algorithm is just a small subcomponent of a bigger control system.
Third, depending on the policy parametrization, stability and robustness guarantees
can be given [11]. Over the last decade, a series of fast policy search algorithms have
been proposed and shown to work well on real systems [7, 45, 49, 72, 19, 17]. In this
review, we provide a general overview and summarize the main concepts behind current
policy search approaches.
2

Data

Model Building

Stoch. Traj.
PG EM Inf.Th.
Model-free policy search

Stoch. Traj.

Det. Traj.

PG EM Inf.Th. PG EM Inf.Th.
Model-based policy search

Figure 1.1: Categorization of policy search into model-free policy search and model-based
policy search. In the model-based case (right sub-tree), data from the robot is used to
learn a model of the robot (blue box). This model is then used to generate trajectories.
Here, we distinguish between stochastic trajectory generation by means of sampling and
deterministic trajectory generation using linearization or moment matching, for instance.
Model-free policy search (left sub-tree) uses data from the robot directly as a trajectory
for updating the policy. The policy updates in both model-free and model-based policy
search (green blocks) are based on either policy gradients (PG), EM updates (EM), or
information-theoretic insights (Inf.Th.).

1.2

Policy Search Categorization

As the name indicates, policy search methods search directly in the policy space, which
makes them are particularly well-suited for robot RL. Numerous policy search methods
have been proposed in the last decade, and several of them have successfully scaled
into the domain of robotics. In this focused survey paper, we review several important
recent developments in policy search for robotics. We distinguish between model-free
policy search methods (Chapter 2) that learn policies directly based on sampled data and
model-based approaches (Chapter 3) that use the sampled data to first build a model of
the state dynamics and, subsequently, use this model for policy improvement.
Model-based and model-free policy search methods can straightforwardly be combined by using the learned simulator (model-based policy search) for trajectory generation and, subsequently, use the update and exploration strategies common to model-free
policy search for improving the policy.
Fig. 1.1 classifies policy search into model-free policy search and model-based policy
search. In the model-based case (right sub-tree), data from the robot is used to learn
a model of the robot (blue box). This model is then used to generate trajectories.
Here, we distinguish between stochastic trajectory generation by means of sampling and
deterministic trajectory generation using linearization or moment matching, for instance.
Model-free policy search (left sub-tree) uses data from the robot directly as a trajectory

3

for updating the policy. The policy updates in both model-free and model-based policy
search (green blocks) are based on either policy gradients (PG), EM updates (EM),
or information-theoretic insights (Inf.Th.). While all three update strategies are fairly
well explored in model-free policy search, model-based policy search almost exclusively
focuses on PG policy updates.

1.3

Typical Policy Representations

We present three typical policy representations: linear policies, nonlinear radial-basisfunction policies, and time-dependent dynamic movement primitives.
Linear Policies. Typical representations of the policy π are a linear policy, where
n

πθ (x) =

θi φi (x)

(1.5)

i=1

where φi are basis functions, e.g., φi (x) = exp(− 12 x − µi ) 2 ), where µi is known. Note
that in Equation (1.5), the policy only depends linearly on the policy parameters.
Nonlinear Radial Basis Functions. A typical nonlinear policy representation is the
radial basis function (RBF) network representation, where
n

πθ (x) =

wi φi (x, ψ) ,

(1.6)

i=1

where the parameters ψ of the basis functions themselves are considered free parameters
that need to be learned. For instance, in the case that φi (x) = exp(− 12 x − µi ) 2 ), we
would obtain ψ = {µi , i = 1, . . . , n} and θ = {w, ψ}.
Dynamic Movement Primitives. Another important class of policy representations
are time-dependent representations that use basis-functions which explicitly depend on
time. The most prominent time-dependent representation used in robotics are the Dynamic Movement Primitives (DMP) [60, 26]. DMPs are used as trajectory generators
which use non-linear dynamical systems for creating the trajectories. The basic idea
is to a use a second-order linear dynamical system which is modulated by a learn-able
non-linear function ft
y¨ = τ 2 αy (βy (g − y) − y)
˙ + τ 2 ft
for each degree-of-freedom (DoF) of the robot. The desired final position of the joint
is denoted by g, which also specifies the globally stable point-attractor of the DMP.
The variable y corresponds to the desired joint position. The temporal scaling factor is
denoted by τ and αz and βz define the damping properties of the linear system. The
non-linear function f directly adds to the desired acceleration joint.
4

For each degree-of-freedom (DoF) of the robot, an individual dynamical system,
and, hence, an individual function ft is used. The function ft depends on the phase
xt of a movement, which represents time. The evolution of the phase signal is given
by x˙ = −τ αx x. The phase variable x is initially set to 1 and converges to 0 for a
proper choice of τ and αx . The temporal scaling parameter τ can be used to modulate
the desired movement speed. The function f is constructed by the weighted sum of K
Gaussian basis functions ψi
f (x) =

K
i=1 ψi (x)wi x
,
K
i=1 Ψi (x)

Ψi (x) = exp(−

1
(x − ci )2 ).
2σi2

As the phase variable x converges to 0 with t → ∞, the influence of f vanishes with
increasing time, and, hence, the dynamical system is globally stable. Typically, the
linear weights wi are parameters of the primitive are learned to modulate the shape
of the movement. The centers ci specify at which phase of the movement the basis
function becomes active. The centers are usually equally spaced in the range of x and
not modified during learning. The bandwidth of the basis functions is given by σi2 .
Integrating the dynamical systems for each DoF results into a desired trajectory
∗
τ = (y t )t=0...T which is, subsequently, followed by feedback control laws [47]. Learning
with DMPs often takes place in two phases [31]. In the first phase, imitation learning is
used to reproduce recorded trajectories. Subsequently, reinforcement learning is used to
improve the movement.
The generation of the trajectory for DMPs is typically an off-line process and does
not incorporate proprioceptive (i.e., the actual joint position qt does not influence the
dynamical systems) or other sensory feedback. Exceptions are presented in [30] and [34].
In [30], an additional feedback controller has been learned to modify the shape of the
trajectory. In [34], the DMPs were extended such that also the feedback-gains αy and
βy of the spring-damper system explicitly dependent on time. This approach was used
to achieve variable stiffness during the movement execution.

1.4

Model-free and Model-based Policy Search

Model-free policy search methods as described in Chapter 2 are inherently based on
sampling trajectories τ i using the robot to find good policies π ∗ . Sampling trajectories
is relatively straightforward in computer simulation. However, when working with mechanical systems, such as robots, each sample corresponds to interacting directly with
the robot, which often requires substantial experimental time and causes wear and tear
in non-industrial robots. Depending on the task, it can either be easier to learn a model
or to learn a policy directly. Model-based policy search methods attempt to address the
problem of sample inefficiency by using observed data to learn a forward model of the
robot’s dynamics. Subsequently, this forward model is used for internal simulations of
the robot’s dynamics, based on which the policy is learned.
Model-based policy search algorithms typically assume the following set-up: The

5

state x evolves according to the Markovian dynamics
xt+1 = f (xt , ut ) + w ,

w ∼ p(x0 ) ,

(1.7)

where f is a nonlinear function, u is a control signal (action), and w is additive noise.
For some problems, model-based RL methods have the promise of requiring fewer interactions with the robot by learning a model of the transition mapping in Equation (3.1),
while efficiently generalizing to unforeseen situations using a model learned from observed data [6].
While the idea of using models in the context of robot learning is well-known since
the 1980s [2], it has been limited by its strong dependency on the quality of the learned
model: The learned policy is inherently based on internal simulations using the learned
model. However, in practice, the learned model is not exact, but only a more or less
accurate approximation to the real dynamics. Inaccurate models can lead to control
strategies that are not robust to model errors. This can have a drastic effect in robotics
since it can mean to have negative masses or negative friction. These implausible effects are often exploited by the learning system. In Chapter 3, we introduce models
that explicitly account for model errors to make policy learning more robust to model
inaccuracies.

1.5

Outline

The structure of this review is as follows: In Chapter 2, we give a detailed overview
of model-free policy search methods, where we follow the categorization in Fig. 1.1.
Chapter 3 surveys model-based policy search methods in robotics. Here, we introduce
two models that are commonly used in policy search: locally weighted regression and
Gaussian processes. Furthermore, we detail stochastic and deterministic inference algorithms to compute a probability distribution p(τ ) over trajectories (see the red boxes in
Fig. 1.1). We conclude this chapter with examples of model-based policy search methods and their application to robotic systems. In Chapter 4, we give problem-specific
recommendations for the practitioner and conclude this survey.

6

Chapter 2

Model-free Policy Search
Model-free policy search (PS) methods update the policy directly based on sampled
trajectories τ [i] , where i always denotes the index of the sample, and the obtained rewards
[i] [i]
[i]
r0 , r1 , . . . , rT for the trajectories. Model-free PS methods try to update the parameters
θ such that trajectories with higher reward become more likely when following the new
policy, and, hence, the average return Jθ increases. Learning a policy is often easier than
learning a model of the robot, and, hence, model-free policy search methods are more
often used than model-based policy search methods, which we will introduce in Section
3. We categorize model-free policy search approaches based on their policy evaluation,
their policy update [49, 48] and their exploration strategy [56, 31].
The policy evaluation strategy decides how to evaluate the quality of the executed
policy. Step-based evaluation strategies decompose the trajectory τ in its single steps
(x0 , u0 , x1 , u1 , . . . ) and aim at evaluating the quality of single actions while episodebased evaluation directly uses the returns of the whole trajectories to evaluate directly
the quality of the used policy parameters θ. The policy update strategy uses the quality
assessment of the evaluation strategy to determine the policy update. Update strategies can be categorized by the optimization methods employed by the algorithm at hand.
While the most common update strategy are based on gradient ascent, resulting in policy
gradient methods [75, 49, 52], inference-based approaches use expectation maximization
[31, 41] and information theoretic approaches [48, 17] use insights from information
theory to update the policy. Finally, the exploration strategy determines how new trajectories are created for the subsequent policy evaluation step. The exploration strategy
is essential for efficient model-free policy search as, on the one hand, we need need variability in the generated trajectories to determine the policy update; on the other hand,
an excessive exploration is likely to damage the robot. Most model-free methods model
use a stochastic policy for exploration. Exploration strategies can be categorized into
step-based and episode-based exploration strategies. While step-based exploration uses
an exploratory action in each time step, episode-based exploration directly changes the
parameter vector θ of the policy before the start of the episode.
In the following section, we will discuss policy evaluation strategies in more detail.
Subsequently we will review policy update methods such as policy gradients, inference7

Algorithm 1 Model-Free Policy Search
repeat
Explore: Generate trajectories τ [i] using policy πk
Evaluate: Assess quality of trajectories or actions
Update: Compute πk+1 given trajectories τ [i] and evaluations
until Policy converges πk+1 ≈ πk
based and information theoretic policy updates as well as update strategies based on
optimal control theory. Many policy update strategies have been implemented for both
policy evaluation approaches, and, hence, we will discuss the combinations that have
been explored so far.

2.1

Policy Evaluation Strategies

Policy evaluation strategies are used to assess the quality of the executed policy. Policy
search algorithms either try to assess the quality of single state-action pairs xt , ut , which
we will refer to as step-based evaluation, or the quality of a parameter vector θ that
has been used during the whole episode, which we will refer to as episode-based policy
evaluation. The policy evaluation strategy is used to transform sampled trajectories τ [i]
[i]
[i]
into a data-set D that contains samples of either the state-action pairs xt , ut or the
parameter vectors θ [i] and an evaluation of these samples. The data-set D is subsequently
processed by the policy update strategy to determine the new policy.

2.1.1

Step-Based Policy Evaluation

In step-based policy evaluation, we decompose the sampled trajectories τ [i] into its
[i]
[i]
single time steps xt , ut , and estimate the quality of the single actions. The quality of
[i]
[i]
an action is given by the expected future reward when executing ut in state xt at time
step t and subsequently following policy πθ (u|x),
T
[i]
Qt

=

Qπt

[i]
[i]
xt , ut

[i]

[i]

rh xt = xt , ut = ut

= Epθ (τ )

,

h=t

which corresponds to the state-action value function Qπ . However, as estimating the
state-action value function is a difficult problem in high-dimensional continuous spaces,
[i]
most policy search methods rely on Monte-Carlo estimates of Qt instead of using value
[i]
function approximation. Yet, most methods can cope with noisy estimates of Qt ,
[i]
and, hence, solely the reward to come for the current trajectory τ [i] is used Qt ≈
[i]
T
h=t rh which saves the expensive averaging that would be needed for an accurate
Monte-Carlo estimate. Algorithms based on step-based policy evaluation use a dataset
Dstep = {x[i] , u[i] , Q[i] } to determine the policy update step. Some step-based policy
search algorithms [75, 49] also use the returns R[i] = Epθ (τ )
8

[i]
T
h=0 rh

of the whole

Algorithm 2 Policy Search with Step-Based Policy Evaluation
repeat
Exploration:
Create samples τ [i] ∼ πθk (τ ) following πθk (u|x), i = 1 . . . N
Policy Evaluation:
[i]
[i]
Evaluate actions: Qt = Th=t rh for all t and all i
[i]

[i]

[i]

Compose Dataset: Dstep = xt , ut , Qt

i=1...N, t=1...T −1

Policy Update:
Compute new policy parameters θ k+1 using D.
Algorithms: REINFORCE, G(PO)MDP, NAC, eNAC,
PoWER, PI2
until Policy converges θ k+1 ≈ θ k
episode as evaluation for the single actions of the episode, however, as the estimate of
the returns suffer from a higher variance than the reward to come, such strategy is not
preferable. Pseudo-code for a general step-based policy evaluation algorithm is given in
Algorithm 2.

2.1.2

Episode-Based Policy Evaluation

Episode-based update strategies have become more popular [64, 65, 17] in recent years.
Episode-base strategies directly use the expected return
T

R

[i]

=R θ

[i]

= Epθ (τ )
t=0

rt |θ = θ [i]

to evaluate the quality of a parameter vector θ [i] . The expected return R[i] for θ [i] can
be estimated by performing multiple roll-outs on the real system. However, a few approaches [57, 32] can avoid such expensive operation as they cope with noisy estimates
[i]
of R[i] , and, hence, can directly use the return Tt=0 rt of a single trajectory τ [i] to
estimate R[i] . Other algorithms, such as stochastic optimizers, require an accurate estimate of R[i] , and, thus, either require multiple roll-outs, or suffer from a bias in the
subsequent policy update step. Episode-based policy evaluation produces a data-set
Dep = {θ [i] , R[i] }i=1...N , which is subsequently used for the policy updates.
An underlying problem of episode-based evaluation is the variance of the R[i] estimates. The variance depends on the stochasticity of the system, the stochasticity of the
policy and the number of time steps. Hence, in order to reduce the variance, the policy
πθ (u|x) is often modeled as deterministic policy and exploration is directly performed
in parameter space.

9

Algorithm 3 Learning an Upper-Level Policy
repeat
Exploration:
Sample parameter vector θ [i] ∼ πωk (θ), i = 1 . . . N
Sample trajectory τ [i] ∼ pθ[i] (τ )
Policy Evaluation:
[i]
Evaluate policy parameters R[i] = Tt=0 rt
Compose dataset Dep = θ [i] , R[i]
i=1...N
Policy Update:
Compute new policy parameters ω k+1 using Dep
Algorithms: Episode-based REPS, Episode-based PI2
PEPG, NES, CMA-ES, RWR
until Policy converges ω k+1 ≈ ω k
ω

ω

πω (θ) πθ (ut|xt)
ut

θ

xt

πω (θ) πθ (ut|xt)

s

t = 0...T

(a)

ut

θ

xt
t = 0...T

(b)

Figure 2.1: (a) Graphical model for learning an upper-level policy πω (θ). The upper
level policy chooses the parameters θ of the lower-level policy πθ (u|x) that is executed
on the robot. The parameters of the upper-level policy are given by ω. (b) Learning
an upper-level policy πω (θ|s) for multiple contexts s. The context is used to select the
parameters θ, but typically not be the lower-level policy itself. The lower-level policy
πθ (u|x) is typically modelled as deterministic policy in both settings.
Learning Upper-Level Policies
Many algorithms based on episode-based policy evaluation can be formalized as learning
an upper-level policy πω (θ) that chooses the parameters of the actual control policy
πθ (u|x). Hence, we will denote the latter in this hierarchical settings as lower-level
policy. Instead of directly finding the parameters θ of the lower-level policy, we want to
find the parameter vector ω which now defines a distribution over θ. Using a distribution
over θ has the benefit that we can use this distribution directly explore in parameter
space. The optimization problem for learning upper-level policies can be formalized as
maximizing
Jω =

πω (θ)
θ

τ

p(τ |θ)R(τ )dτ dθ =

πω (θ)R(θ)dθ.

(2.1)

θ

The graphical model for learning an upper-level policy is given in Figure 2.1(a) and the
general algorithm is given in Algorithm 3.

10

Algorithm 4 Learning an Upper-Level Policy
repeat
Exploration:
Sample context s[i] ∼ p(s)
Sample parameter vector θ [i] ∼ πωk (θ|s[i] ), i = 1 . . . N
Sample trajectory τ [i] ∼ pθ[i] (τ |s[i] )
Policy Evaluation:
[i]
Evaluate policy parameters R[i] = Tt=0 rt
Compose dataset Dep = θ [i] , s[i] , R[i]
i=1...N
Policy Update:
Compute new policy parameters ω k+1 using Dep
Algorithms: Episode-based REPS, CRKR, PEPG, RWR
until Policy converges ω k+1 ≈ ω k
Learning Upper-Level Policies for Multiple Contexts
The problem of learning an upper-level policy can also be extended to generalize the
lower-level policy πθ (u|x) to different tasks. Here, we will characterize a task by a
context vector s. The context describes all variables which do not change during the
execution of the task but it might change from task to task. For example, the context s
can describe the objectives of the agent or physical properties such as a mass to lift. For
this contextual settings, we can learn an upper-level policy πω (θ|s) which is conditioned
on the context s. The problem of learning πω (θ|s) can be characterized by maximizing
the expected returns over all contexts, i.e.,
Jω =

p(s)
s

πω (θ|s)
θ

p(s)

=
s

τ

p(τ |θ, s)R(τ , s)dτ dθds

πω (θ|s)R(θ, s)dθds,

(2.2)
(2.3)

θ

where R(θ, s) = τ p(τ |θ, s)R(τ , s)dτ is the expected return for executing the lowerlevel policy with parameter vector θ in context s, p(s) is the given distribution over
the contexts. Note that the trajectory distribution p(τ |θ, s) now might also depend
on the context as such context can contain physical properties of the environment. In
the case of multiple contexts, the dataset Dep = s[i] , θ [i] , R[i]
also includes the
i=1...N

corresponding context s[i] when executing the lower-level policy with parameters θ [i] .
The graphical model for learning an upper-level policies with multiple contexts is given
in Figure 2.1(b) and the general algorithm is given in Algorithm 4.

2.1.3

Qualitative Comparison

Step-based policy evaluation exploits the structure that the return is typically composed
of the sum of the immediate rewards. Single actions can now be evaluated by the reward
11

to come in that episode, instead of the whole reward of the episode, and, hence, the
variance of the evaluation can be significantly reduced as the reward to come contains
less random variables as the total reward of the episode. In addition, as we evaluate
single actions instead of the whole parameter vector, the evaluated sampled can be used
more efficiently as several parameter-vectors θ might produce a similar action ut at
time step t. Most policy search algorithms, such as the most common policy gradient
algorithms [75, 52], the PoWER [31] algorithm or the PI2 [68] algorithm employ such
a strategy. A drawback of most step-based updates is that they often rely on a linear
parametrization of the policy πθ (u|x). They also cannot be applied if the reward is not
decomposable in isolated time steps.
Episode-based policy evaluation strategies do not decompose the returns, and, hence,
might suffer from a large variance of the estimated returns. However, episode-based
policy evaluation strategies typically employ more sophisticated exploration strategies
which directly explore in parameter space of the policy [17, 64, 24], and, thus, can often
compete with their step-based counter-parts. So far, there has not been a clear answer
which of the strategies should be preferred. The choice of the methods often depends
on the problem at hand.

2.2

Policy Update Strategies

In this section, we will different policy update strategies used in policy search, such as
policy gradient methods, expectation-maximization based policy updates, information
theoretic policy updates and policy updates which can by derived from the path-integral
theory. In the case where the policy update method has been introduced for both policy
evaluation strategies, we will present both resulting algorithms. Policy updates for
the step-based evaluation strategy use the dataset Dstep to determine the policy update
while algorithms based on episode-based policy updates employ the dataset Dep . We will
qualitatively compare the algorithms with respect to their sample efficiency, the number
of algorithmic parameters that have to be tuned by the user, the type of reward-function
that can be employed and also how safe it is to apply the method on the real robot.
Methods which are safe to apply on a real robot should not allow big jumps in the policy
updates as such jumps might result in unpredictable behavior which damage the robot.
We will now discuss the different policy update strategies.
Whenever it is possible, we will describe the policy search methods for the episodic
reinforcement learning formulation with time-dependent policies as this most robot learning task are episodic and not infinite horizon tasks. However, most of the derivations
also hold for the infinite horizon formulation if we introduce a discount factor for the
return and cut the trajectories after an horizon of T time steps. In this case, T needs
to be sufficiently large such that the influence of future time-steps with t > T vanishes
as γ T approaches zero.

12

2.2.1

Policy Gradient Methods

Policy gradient (PG) methods use gradient-ascent for maximizing the expected return.
In gradient ascent, the parameter update direction is given by the gradient ∇θ Jθ as it
points in the direction of steepest ascent of the expected return. The policy gradient
update is therefore given by
θ k+1 = θ k + α∇θ Jθ ,
where α is a user-specified learning rate and the policy gradient is given by
∇θ Jθ =

τ

∇θ pθ (τ )R(τ )dτ .

(2.4)

We will now discuss different ways to compute the gradient ∇θ Jθ .
Finite Difference Methods
The finite difference policy gradient [33, 49] is the simplest way of obtaining the policy
gradient. It is typically used with the episode-based evaluation strategy. The finite
difference method estimates the gradient by applying small perturbations ∆θ [i] to the
parameter vector θ k . We can either perturb each parameter value separately or use
a probability distribution with small variance to create the perturbations. For each
perturbation, we obtain the change of the return ∆R[i] = R(θ k + ∆θ [i] ) − R(θ k ). For
finite difference methods, the perturbations ∆θ [i] implement the exploration strategy in
parameter space. However, the generation of the perturbations is typically not adapted
during learning but predetermined by the user. The gradient ∇FD
θ Jθ can be obtained by
using a first order Taylor-expansion of Jθ and solving for the gradient in a least-squares
sense, i.e., is then determined numerically from the samples
T
−1
T
∇FD
θ Jθ = (∆Θ ∆Θ) ∆Θ ∆R,

(2.5)

T

where ∆Θ = ∆θ [1] , . . . , ∆θ [N ] and ∆R = [∆R[1] , . . . , ∆R[N ] ]. Finite difference methods are very powerful as long as the evaluations R(θ) are not too noisy.
Likelihood-Ratio Policy Gradients
Likelihood-ratio methods were among the first policy search methods introduced in the
early 90s by Williams [75] with the REINFORCE algorithm. These methods make
use of the so called ‘likelihood-ratio’ trick that is given by the identity ∇pθ (y) =
pθ (y)∇ log pθ (y). We can easily confirm this identity by using the chain rule to calculate
the derivative of log pθ (y), i.e., ∇ log pθ (y) = ∇pθ (y)/pθ (y). Inserting the likelihoodratio trick into the policy gradient from Equation (2.4) yields
∇θ Jθ =

pθ (τ )∇θ log pθ (τ )R(τ )dτ = Epθ (τ ) [∇θ log pθ (τ )R(τ )] ,

(2.6)

where the expectation over pθ (τ ) is approximated by using a sum over the samples
τ [i] = (x0 , u0 , x1 , u1 , . . . ).
13

[i]

Baselines. As the evaluation R[i] of a parameter θ [i] or the evaluation Qt of an action u[i] is typically performed by inherently noisy Monte-Carlo estimates, the resulting
gradient estimates are also afflicted by a large variance. The variance can be reduced by
introducing a baseline b for the trajectory reward R(τ ), i.e.,
∇θ Jθ = Epθ (τ ) [∇θ log pθ (τ )(R(τ ) − b)] .

(2.7)

Note that the policy gradient estimate remains unbiased as
Epθ (τ ) [∇θ log pθ (τ )b] = b

τ

∇θ pθ (τ )dτ = b∇θ

pθ (τ )dτ = 0,

(2.8)

τ

where we first applied the reverse of ‘likelihood-ratio’ trick and subsequently the identity
τ pθ (τ )dτ = 1. Since the baseline b is a free parameter, we can choose it such that it
minimizes the variance of the gradient estimate. We will denote the variance-minimizing
baseline as optimal baseline. As the likelihood gradient can be estimated in different
ways, the corresponding optimal baseline will change with it. We now first discuss the
step-based likelihood-ratio PG algorithms, and, discuss their optimal baselines if present
in the literature. Subsequently, we will cover the episode-based likelihood-ratio variant.
Step-Based Algorithms
Step-based algorithms exploit the structure of the trajectory distribution, i.e.,
T

pθ (τ ) = p(x1 )
t=1

p(xt+1 |xt , ut )πθ (ut |xt , t)

to decompose ∇θ log pθ (τ ) into the single time steps. As the product is transformed
into a sum by a logarithm, all terms which do not depend on the policy parameters θ
disappear during differentiation. Hence, ∇θ log pθ (τ ) is given by
T −1

∇θ log pθ (τ ) =

t=0

∇θ log πθ (ut |xt , t).

(2.9)

Equation (2.9) reveals a key result for policy gradients that ∇θ log pθ (τ ) does not depend
on the transition model p(xt+1 |xt , ut ). Note that this result holds for any stochastic
policy. However, for deterministic policies πθ (xt , t), the gradient ∇θ log pθ (τ ) includes
∇θ p(xt+1 |xt , πθ (xt , t)) =

∂p(xt+1 |xt , ut ) ∂ut
|
,
∂ut
∂θ ut =πθ (xt ,t)

and, hence, the transition model needs to be known. Consequently, stochastic policies
play a crucial role for policy gradient methods.

14

The REINFORCE Algorithm. Equation (2.9) is used by one of the first introduced
PG algorithm in the machine learning literature, the REINFORCE algorithm [75]. The
REINFORCE policy gradient is given by
T −1

∇RF
θ Jθ

= Epθ (τ )
t=0

∇θ log πθ (ut |xt , t)(R(τ ) − b) ,

(2.10)

where b denotes the baseline.
RF . The
To minimize the variance of ∇RF
θ Jθ , we estimate the optimal baseline b
optimal baseline also depends on which element h of the gradient ∇θ Jθ we want to
evaluate, and, hence needs to be computed for each dimension h separately. The optimal
RF
baseline bRF
h for the REINFORCE algorithm minimizes the variance of ∇θ h Jθ , i.e., it
satisfies the condition
∂
Var ∇RF
θ h Jθ
∂b

=
=

∂
)2 − Epθ (τ ) JθRF
Epθ (τ ) (JθRF
h
h
∂b
∂
E
(JθRF
)2 = 0.
h
∂b pθ (τ )

2

(2.11)
(2.12)

Solving this equation for b yields

bRF
h =

T −1
t=0 ∇θ h

Epθ (τ )

log πθ (ut |xt , t)

T −1
t=0 ∇θ h

Epθ (τ )

2

log πθ (ut |xt , t)

R(τ )
2

.

(2.13)

The REINFORCE algorithm with its optimal baseline is summarized in Algorithm 5.
The G(PO)MDP Algorithm. From Equation 2.10, we realize that REINFORCE
uses the returns R(τ ) of the whole episode as evaluations of single actions despite using
the step-based policy evaluation strategy. As already discussed before, the variance of the
returns can grow with trajectory length, and, hence, deteriorate the performance of the
algorithm even if used with the optimal baseline. However, by decomposing the return
in the rewards of the single time steps, we can use the observation that rewards from the
past do not depend on actions in the future, and, hence, Epθ (τ ) [∂θ log πθ (ut |xt , t)rj ] = 0
for j < t1 . If we look at a reward rj of a single time-step j, we realize that we can neglect
all derivatives of future actions. This intuition has been used for the G(PO)MDP algorithm [9, 10] to decrease the variance of policy gradient estimates. The policy gradient
of G(PO)MDP is given by


T −1 j

∇GMDP
Jθ
θ

= Epθ (τ ) 
j=0 t=0

∇θ log πθ (ut |xt , t)(rj − bj ) ,

1

(2.14)

We can follow the same argument as in Equation 2.8 for introducing the baseline to proof this
identity.

15

Algorithm 5 REINFORCE
Input: policy parametrization θ,
[i]
[i]
[i]
data-set D = x1:T , u1:T −1 , r1:T

i=1...N

[i]

Compute returns: R[i] = Tt=0 rt
for each dimension h of θ do
Estimate optimal baseline:

bRF
h

=

N
i=1

T −1
t=0 ∇θ h

T −1
t=0 ∇θ h

N
i=1

[i]

[i]

log πθ ut xt , t
[i]

2

R[i]
2

[i]

log πθ ut xt , t

Estimate derivative for dimension h of θ:
∇RF
θ h Jθ

1
=
N

N T −1
[i]

[i]

∇θh log πθ ut xt , t (R[i] − bRF
h )

i=1 t=0

end for
Return ∇RF
θ Jθ
where bj is a time-dependent baseline. The optimal baseline for the G(PO)MDP algo(x) for time step j and dimension h of θ can be obtained similarly as for
rithm bGMDP
h,j
the REINFORCE algorithm and is given by

=
bGMDP
h,j

j
t=0 ∇θ h

Epθ (τ )

log πθ (ut |xt , t)

j
t=0 ∇θ h

Epθ (τ )

log πθ (ut |xt , t)

2

rj
2

.

(2.15)

The G(PO)MDP algorithm is summarized in Algorithm 6.
The Policy Gradient Theorem Algorithm. Instead of using the returns R(τ ) we
can also use the expected reward to come at time step t, i.e., Qπt (xt , ut ), to evaluate
an action ut . Mathematically, such evaluation can be justified by the same observation
that has been used for the G(PO)MDP algorithm, i.e., that rewards are not correlated
with future actions. Such evaluation is used by the Policy Gradient Theorem (PGT)
algorithm [66], which states that



T −1


∇PG
θ Jθ = Epθ (τ )

t=0

T

∇θ log πθ (ut |xt ) 

rj  ,
j=t

T −1

= Epθ (τ )
t=0

∇θ log πθ (ut |xt )Qπt (xt , ut ) .
16

(2.16)

Algorithm 6 G(PO)MDP Algorithm
Input: Policy parametrization θ,
[i]
[i]
[i]
Dataset D = x1:T , u1:T −1 , r1:T −1
i=1...N
for each time step t = 0 . . . T − 1 do
for each dimension h of θ do
Estimate optimal time-dependent baseline:

bGMDP
=
h,j

j
t=0 ∇θ h

N
i=1
N
i=1

T −1
t=0

[i]

2 [i]
rj

[i]

log πθ uh xh , h

j
t=0 ∇θ h

[i]

[i]

log πθ uk xk , k

2

end for
Estimate gradient for dimension h:
N T −1

Jθ
∇GMDP
θh

j
[i]

=
i=1 j=0

t=0

[i]

[i]

rj − bGMDP
h,j

∇θh log πθ ut xt , t

end for
Jθ
Return ∇GMDP
θ
We can again subtract an arbitrary baseline bt (x) from Qπt (xt , ut ), which now depends
on the state x as well as on the time step.
While Qπt (xt , ut ) can be estimated by Monte-Carlo roll-outs, the PGT algorithm can
be used in combination with function approximation as covered in Section 2.2.1.
Episode-Based Policy Gradient Algorithms
Episode-based likelihood-ratio methods directly update the upper-level policy πω (θ) for
choosing the parameters θ of the lower-level policy πθ (ut |xt , t). They optimize the
expected return Jω defined in Equation (2.1). The likelihood gradient of Jω can be
directly obtained by replacing pθ (τ ) with πω (θ) and R(τ ) with R(θ) = τ pθ (τ )R(τ )dτ
in Equation (2.6),
∇PE
ω Jω = Eπω (θ) [∇ω log πω (θ)(R(θ) − b)] .

(2.17)

Such an approach was first introduced by [62, 63] with the Parameter Exploring Policy
Gradient (PEPG) algorithm. The optimal base-line bPGPE
for the hthe element of the
h
PEPG gradient is obtained similarly as for the REINFORCE algorithm and given by
bPGPE
h

=

Eπω (θ) (∇ωh πω (θ))2 R(θ)
Epθ (τ ) (∇ωh πω (θ))2

The PEPG algorithm is summarized in Algorithm 7.
17

.

(2.18)

Algorithm 7 Parameter Exploring Policy Gradient Algorithm
Input: Policy parametrization ω
Dataset D = θ [i] , R[i]
i=1...N
for each dimension h of ω do
Estimate optimal baseline:

bPGPE
h

N
i=1

=

∇ωh πω (θ [i] )

N
i=1

2

∇ωh πω (θ [i] )

R[i]
2

Estimate derivative for dimension h of ω:
∇PE
ω h Jω =

1
N

N
i=1

)
∇ω πωh (θ [i] )(R[i] − bPGPE
h

end for
Natural Gradients
The natural gradient [3] is a well known concept from supervised learning to optimize
parametrized probability distributions pθ (y), where y is a random variable, which often
achieves faster convergence than the traditional gradient. Traditional gradient methods
typically use an Euclidean metric ∆θ T ∆θ to determine the direction of the update
∆θ, i.e., they assume that all parameter dimensions have similarly strong effect on the
resulting distribution. However, already small changes in θ might result in large changes
of the resulting distribution pθ (y). As the gradient estimation typically depends on pθ (y)
due to the sampling process, the next gradient estimate can also change dramatically.
To achieve a stable behavior of the learning process, it is desirable to enforce that the
distribution pθ (y) does not change too much in one update step. This intuition is the key
concept behind the natural gradient which limits the distance between the distribution
pθ (y) before and pθ+∆θ (y) after the update. To measure the distance between pθ (y)
and pθ+∆θ (y), the natural gradient uses an approximation of the Kullback-Leibler (KL)
divergence. The KL-divergence is a similarity measure of two distributions. It has been
shown that the Fisher information matrix
Fθ = Ep(y) ∇θ log p(y)∇θ log p(y)T

(2.19)

can be used to approximate KL divergence between pθ+∆θ (y) and pθ (y) for small enough
∆θ, i.e.,
KL (pθ+∆θ (y)||pθ (y)) ≈ ∆θ T Fθ ∆θ.
(2.20)
The natural gradient update ∆θ NG is now defined as the update ∆θ that is most similar
to the traditional gradient ∆θ VG update that has a bounded distance
KL(pθ+∆θ (y)||pθ (y)) ≤
18

in the distribution space. Hence, we can formulate the following optimization program
∆θ N G = argmax∆θ ∆θ T ∆θ VG

s.t.

∆θ T Fθ ∆θ ≤ ε.

(2.21)

VG
The solution of this program is given by ∆θ NG ∝ F−1
up to a scaling factor. The
θ ∆θ
proportionality factor for the update step is typically subsumed into the learning rate.
The natural gradient rotates the traditional gradient by the inverse Fisher matrix, which
renders the parameter update also invariant to linear transformations of the parameters
of the distribution — if two parameterizations have the same representative power, the
natural gradient update will be identical. As the Fisher information matrix is always
positive definite, the natural gradient always rotates the traditional gradient for less
than 90 degrees, and, hence, all convergence guarantees from standard gradient-based
optimization remain. In contrast to the traditional gradient, the natural gradient avoids
premature convergence on plateaus and overaggressive steps on steep ridges due to its
isotropic convergence properties [3, 65].

Natural Policy Gradients. The intuition of the natural gradients to limit the distance between two subsequent distributions is also useful for policy search—here we want
to maintain a limited step-width in the trajectory distribution space, i.e.,
KL(pθ (τ )||pθ+∆θ (τ )) ≈ ∆θ T F θ ∆θ T ≤ .
The Fisher information matrix
Fθ = Epθ (τ ) ∇θ log pθ (τ )∇θ log pθ (τ )T
is now computed for the trajectory distribution pθ (τ ). The natural policy gradient
∇NG
θ Jθ is therefore given by
−1
∇NG
(2.22)
θ Jθ = Fθ ∇θ Jθ ,
where ∇θ Jθ can be estimated by any traditional policy gradient method.
The difference between the natural and traditional policy gradient for learning a
simple linear feedback policy is shown in Figure 2.2. In this example, a scalar controller
gain and the variance of the policy are optimized. While the traditional gradient quickly
reduces the variance of the policy, and, hence will stop exploring, the natural gradient
only gradually decreases the variance, and, in the end, finds the optimal solution faster.
Step-Based Algorithms
Similar as the gradient, also the Fisher information matrix can be decomposed in the
policy derivatives of the single time steps [8]. Peters and Schaal [53, 52] showed that the
Fisher information matrix of the trajectory distribution can be written as the average
Fisher information matrices for each time step, i.e.,
T −1

Fθ = Epθ (τ )
t=0

∇ log πθ (ut |xt , t)∇ log πθ (ut |xt , t)T .
19

(2.23)

(b) Natural Policy Gradient
1

0.8

0.8

Controller variance θ2=σ

Controller variance θ2=σ

(a) Vanilla Policy Gradient
1

0.6

0.4

0.2

0

-1.6

-1.4

-1.2

-1
-0.8
-0.6
Controller gain θ = k

-0.4

0.6

0.4

0.2

0

-0.2

1

-1.6

-1.4

-1.2

-1
-0.8
-0.6
Controller gain θ = k

-0.4

-0.2

1

Figure 2.2: Comparison of the natural gradient to the traditional gradient on a simple
linear task with quadratic cost function. The controller has two parameters, the feedback
gain k and the variance σ 2 . While the traditional gradient quickly reduces the variance
of the policy, the natural gradient only gradually decreases the variance, and therefore
continues to explore.
Consequently, as it is the case for estimating the policy gradient, the transition model
is not needed for estimating Fθ . Still, estimating Fθ from samples can be difficult,
as Fθ contains O(d2 ) parameters, where d is the dimensionality of θ. However, as it
turns out the Fisher information matrix Fθ does not need to be estimated directly if we
use compatible function approximation for the Policy Gradient Theorem algorithm (c.f.
Section 2.2.1).
Compatible Function Approximation.
was given by

In the PGT algorithm, the policy gradient

T −1

∇PG
θ Jθ = Epθ (τ )

t=0

∇θ log πθ (ut |xt ) (Qπt (xt , ut ) − bt (xt )) .

Instead of using the future rewards of a single roll-out to estimate Qπt (xt , ut ) − bt (x),
we can also use function approximation [66], i.e., estimate a function A˜w (xt , ut , t) =
ψ t (xt , ut )T w such that A˜w (xt , ut , t) ≈ Qπt (xt , ut )−bt (x). The quality of the approximation is determined by the choice of the basis functions ψt (xt , ut ), which might explicitly
depend on the time step t. A good function approximation does not change the gradient
in expectation, i.e., it does not introduce a bias. To find basis functions ψt (xt , ut ) that
fulfill this condition, we will first assume that we already found a parameter vector w
which approximates Qπt . For simplicity, we will for now assume that the baseline bt (x)
is zero. A parameter vector w, which approximates Qπt , also minimizes the squared

20

approximation error. Thus, w has to satisfy
∂
E
∂w pθ (τ )

T −1

Qt (xt , ut ) − A˜w (xt , ut , t)

2

t=0
T −1

= 2Epθ (τ )
t=0

(Qt (xt , ut ) − A˜w (xt , ut , t))

= 0,
∂ ˜
Aw (xt , ut , t) = 0,
∂w

with ∂ A˜w (xt , ut , t)/∂w = ψ t (xt , ut ).
By subtracting this equation from the Policy Gradient Theorem in Equation (2.16),
it is easy to see that
T

∇PG J(θ) = Epθ (τ )

t=1

∇θ log πθ (ut |xt , t)A˜w (xt , ut , t) ,

(2.24)

if we use ∇θ log πθ (ut |xt , t) as basis functions ψt (xt , ut ) for A˜w .
Using ψ t (xt , ut ) = ∇θ log πθ (ut |xt , t) as basis functions is also called compatible
function approximation [66], as the function approximation is compatible with the policy
parametrization. The policy gradient using compatible function approximation can now
be written as
T −1

∇FA
θ Jθ

= Epθ (τ )
t=0

∇θ log πθ (ut |xt , t) log πθ (ut |xt , t)T w,

= Gθ w.

(2.25)

Hence, we have to estimate the parametrization w of the advantage function and the
matrix Gθ .
Step-Based Natural Policy Gradient. The result given in Equation (2.25) implies
that the policy gradient ∇FA
θ Jθ using the compatible function approximation already
contains the Fisher information matrix as Gθ = F θ . Hence, the calculation of the
step-based natural gradient simplifies to
−1 FA
∇NG
θ Jθ = F θ ∇θ Jθ = w.

(2.26)

The natural gradient still requires estimating the function A˜w . Due to the baseline bt (x),
the function A˜w (xt , ut , t) can be interpreted as advantage function, i.e., A˜w (xt , ut , t) ≈
Qπt (xt , ut ) − Vt (xt ). We can check that A˜w is an advantage function by observing that
Ep (τ ) A˜w (xt , ut , t) = Ep (τ ) [∇θ log πθ (ut |xt , t)] w = 0. The advantage function A˜w
θ

θ

can be estimated by using temporal difference methods [67, 13]. However, in order to
estimate the advantage function, such methods also require an estimate of the value
function Vt (xt ). While the basis functions from the advantage function are given by the
compatible function approximation, appropriate basis functions for the value function
are typically more difficult to specify.
21

Episodic Natural Actor Critic. For the episodic policy search such setup the estimation of the value function Vt can be avoided by considering whole sample paths, which
results in the Episodic Natural Actor Critic (eNAC) algorithm [51]. We first rewrite the
Bellman equation for the advantage function
A˜w (x, u, t) + Vt (x) = rt (x, u) +

p(x |x, u)Vt (x )dx,

(2.27)

where VT (x) = rT (x) is the reward for the final state. Equation (2.27) can be written
as
A˜w (xt , ut , t) + Vt (xt ) = rt (xt , ut ) + Vt (xt+1 ) + ,
(2.28)
for a single transition from xt to xt+1 where is a zero-mean noise term. We now sum
up Equation (2.28) along a sample path and get the following condition
T −1
t=0

T −1

∇θ log πθ (ut |xt , t)w + V0 (x0 ) =

rt (xt , ut ) + rT (xT ),

(2.29)

t=0

with ∇θ log πθ (ut |xt , t)w = A˜w (xt , ut , t). Now, the value function Vt needs to be estimated only for first time step. For a single start state x0 , estimating V0 (x0 ) = v0 reduces
to estimating a constant v0 . For multiple start states x0 , Vt needs to be parametrized
V0 (x0 ) ≈ V˜v (x0 ) = ϕ(x)T v. By using multiple sample paths τ [i] , we get a regression
problem whose solution is given by
w
v

= (ΨT Ψ)−1 ΨT R,

(2.30)

where the matrix Ψ contains the policy and value function features of the sample paths,
i.e.,
T −1

ψ [i] =
t=0

∇θ log πθ u[,] x[,] , ϕ(x[i] )T ,

Ψ = ψ [1] , . . . , ψ [N ]

T

and R contains the returns of the sample paths. The eNAC algorithm is illustrated in
Algorithm 8.
Natural Actor Critic. While the eNAC algorithm is efficient for the episodic reinforcement learning formulation, it uses the returns R[i] for evaluating the policy, and
consequently, gets less accurate for large time-horizons due to the large variance of the
returns. For learning problems with a large time horizon, especially, for infinite horizon
tasks, the convergence speed can be improved by directly estimating the value function
Vt . Such strategy is implemented by the Natural Actor Critic Algorithm (NAC) algorithm [52]. The NAC algorithm estimates the advantage function and the value function
by applying temporal difference methods [67, 13]. To do so, temporal difference methods
have first to be adapted to learn the advantage function.

22

Algorithm 8 Episodic Natural Actor Critic
Input: Policy parametrization θ,
[i]
[i]
[i]
data-set D = x1:T , u1:T −1 , r1:T
i=1...N
for each sample i = 1 . . . N do
[i]
Compute returns: R[i] = Tt=0 rt
Compute features: ψ

[i]

=

T −1
t=0 ∇θ

[i]

[i]

log πθ ut xt , t
[i]

ϕ(x0 )
end for
Fit advantage function and initial value function
R = R[1] , . . . , R[N ]
w
v

T

,

Ψ = ψ [1] , . . . , ψ [N ]

T

= (ΨT Ψ)−1 ΨT R

return ∇eNAC
Jθ = w
θ
We start this derivation by first writing down the Bellman equation in terms of the
advantage function in the infinite horizon formulation
Qπ (x, u) = Aπ (x, u) + V π (x) = r(x, u) + γ

p(x |x, u)V π (x )dx

(2.31)

Note that we now discuss the infinite horizon case, i.e., all functions are time independent
˜ u) ≈ ∇θ log πθ (u|x)w and
and we introduced the discount factor γ. By inserting A(x,
π
T
V (x) ≈ ϕ(x) v, we can rewrite the Bellman equation as a set of linear equations
∇θ log πθ u[i] x[i] w + ϕ(x[i] )T v = r(x[i] , u[i] ) + γϕ(x [i] )T v + .
One efficient method to estimate w and v is to use the LSTD-Q(λ) [13, 52] algorithm.
A simplified version of the NAC algorithm which uses LSTD-Q(0) to estimate w and
v is given in Algorithm 9. For a more detailed discussion of the LSTD-Q algorithm we
refer to the corresponding papers [13, 52, 35].
Episode-Based Natural Policy Gradients
The beneficial properties of the natural gradient were also exploited in parameter-space
[74, 65]. While such methods come from the area of evolutionary algorithms as they
always maintain a population of parameter-samples, they perform gradient ascent on a
fitness function which is in the reinforcement learning context the expected long-term
reward Jω of the upper-level policy. Hence, we categorize these methods as Policy
Gradient methods.
Existing natural gradient methods in parameter space do not use a compatible function approximation to estimate the natural gradient but directly try to estimate the
23

Algorithm 9 Natural Actor Critic
Input: Policy parametrization θ,
data-set D = x[i] , u[i] , r[i] , x [i] i=1...N
for each sample i = 1 . . . N do
Compute features for current and successor state:
ψ [i] =

∇θ log πθ u[i] x[i]
ϕ(x[i] )

,

ψ [i] =

0
ϕ(x [i] )

end for
Compute LSTD-Q solution
N

N
[i] [i]

b=

ψ r ,A =
i=1

i=1

w
v

ψ [i] ψ [i] − γψ [i]

T

= A−1 b

return ∇NAC
Jθ = w
θ
Fisher information matrix which is subsequently multiplied with the likelihood gradient
∇PE
ω Jω given in Eq. (2.17) in parameter space.
−1 PE
∇NES
ω Jω = F ω ∇ω Jω

(2.32)

The natural gradient for parameter space was first used in the Natural Evolution
Strategy by [74] where the Fisher information matrix was determined empirically. However, the empirical estimation of the Fisher information matrix is problematic as the
matrix may not be invertible due to redundant parameters or sampling errors. In [65],
the authors compute the Fisher information matrix in closed form for Gaussian policies
in parameter space. The authors also give derivations of an optimal baseline for their
method. As these derivations are rather complex we refer to the corresponding paper for
both derivations. The NES strategy has also been compared with the PEPG algorithm
[57], indicating that NES is more efficient for low-dimensional problems while PEPG has
advantages for high-dimensional parameter spaces as the second-order type update of
the natural gradient gets more difficult.

2.2.2

Expectation Maximization Policy Search Approaches

Setting the learning rate of a policy gradient method can be problematic and often
results in an unstable learning process or slow convergence [31]. This problem can
be avoided by formulating policy search as an inference problem with latent variables,
and, subsequently, using the Expectation-Maximization (EM) algorithm to infer a new
policy. As in the standard Expectation-Maximization algorithm, the parameter update
24

is computed as weighted maximum likelihood estimate which can be obtained in closed
form for most used policies. Hence, no learning rate is required.
We will first review the general EM-algorithm, its sample-based variant, MonteCarlo Expectation Maximization (MC-EM) and weighted maximum likelihood estimates.
Subsequently, we will reformulate policy search as inference problem by treating the
reward as improper probability distribution and state the resulting EM-based policy
search algorithms.
The Expectation Maximization Algorithm in General
The EM-algorithm is a well known algorithm for estimating the maximum likelihood
solution of a probabilistic latent variable model. Let us assume that y defines an observed
random variable, z an unobserved random variable and pθ (y, z) is the parametrized joint
distribution of observed and unobserved variables with parameters θ. As z is unobserved,
it needs to be marginalized out to compute the likelihood of the parameters, i.e., pθ (y) =
pθ (y, z)dz. Given a dataset Y = [y [1] , . . . y [N ] ]T , we now want to maximize the logmarginal-likelihood
N

N

log pθ (y [i] ) =

log pθ (Y ) =
i=1

log

pθ (y [i] , z)dz

(2.33)

i=1

with respect to the parameters θ, where we assumed i.i.d. data-points, and, hence,
pθ (Y ) = i pθ (y i ). Since the log is acting on the marginal distribution pθ (y, z)dz
instead of non the joint distribution pθ (y, z), we can not obtain a closed form solution
for pθ .
The EM-algorithm is an iterative procedure for estimating the maximum likelihood
solution of latent variable models where the parameter updates of every iteration can be
obtained in closed form. We will closely follow the derivation of EM from [12] as it can
directly be applied to the policy search setup.
The costly marginalization over the hidden variables can be avoided by introducing
a variational distribution q(Z). The variational distribution q(Z) is used to decompose
the marginal log-likelihood
pθ (Y , Z)
pθ (Z|Y )
− q(Z) log
q(Z)
q(Z)
= Lθ (q) + KL (q(Z)||pθ (Z|Y )) .

log pθ (Y ) =

q(Z) log

(2.34)

We can easily verify Equation (2.34) by using Bayes theorem pθ (Z|Y ) = pθ (Y , Z)/pθ (Y ).
Since the KL-divergence is always larger or equal to zero, the term Lθ (q) is a lower bound
of the log marginal-likelihood log pθ (Y ). Equation 2.34 already reveals the two update
steps for the lower bound Lθ (q).
Expectation Step. In the expectation step (E-step), we update the variational distribution q(Z) by minimizing the KL-divergence KL (q(Z)||pθ (Z|Y )) which is equivalent
25

to setting q(Z) = pθ (Z|Y ). Note that the lower bound Lθ (q) is tight after each E-step,
i.e., log pθ (Y ) = Lθ (q), as the KL-divergence KL (q(Z)||pθ (Z|Y )) has been set to zero
by the E-step. As log pθ (Y ) is unaffected by the change of q(Z), we observe from Eq.
(2.34) that the lower bound Lθ (q) has to increase if we decrease the KL-divergence.
Maximization Step.
with respect to θ, i.e.,

In the maximization step (M-step), we optimize the lower bound

θ new = argmaxθ Lθ (q) = argmaxθ

q(Z) log pθ (Y , Z) + H(q),

= argmaxθ Eq(Z) [log pθ (Y , Z)] = Qθ (q),

(2.35)

where the term H(q) denotes the entropy of q and can be neglected for estimating θ new .
The term in Equation (2.35) is also denoted as expected complete data-log likelihood
Qθ (q). The log now directly acts on the joint distribution pθ (Y , Z), and, hence, the
M-step can be obtained in closed form. By examining the expected complete data loglikelihood
N

Qθ (q) =

qi (z) log pθ (y [i] , z),
i=1

(2.36)

z

in more detail, we can see that the M-step is based on a weighted maximum likelihood
estimate of θ on the complete data points [y i , z] weighted by qi (z).
The EM-algorithm is guaranteed to converge to a local maximum of the marginal
log-likelihood log pθ (Y ) as the lower bound is increased in each E-step and the M-step
and it is tight after each E-step.
Policy Search as an Inference Problem
We will first formulate policy search as latent variable
inference problem and then show how EM can be applied
to solve this latent variable problem. To do so, we define
a binary reward event R as observed variable. As we
want to maximize the reward, we always observe the
reward event, i.e., R = 1. The probability of this reward
event is given by p(R = 1|τ ). The trajectories τ are the
latent variables in our model. As we have always the
same observation, R = 1, we will write p(R|τ ) instead
of p(R = 1|τ ).
If the return R(τ ) of a trajectory is bounded, it can
be directly transformed into an non-normalized probability distribution, i.e., p(R|τ ) ∝ R(τ ) − minτ j R(τ j ).
Otherwise, an exponential transformation of the reward
signal can be used [50, 70], i.e., p(R|τ ) ∝ exp (βR(τ )).
The parameter β denotes the inverse temperature of the
exponential transformation.
26

p(R|τ )

pθ (τ )

R

τ

θ

Figure 2.3: Graphical Model
for inference-based policy
search.
We introduce a
binary reward event R = 1
as observation, the latent
variables are given by the
trajectories τ . We want to
find the maximum likelihood
solution for the parameters θ
of observing the reward, i.e.,
θ new = argmaxθ log pθ (R)

We want to find parameter vectors θ that maximize the probability of the reward
event. In other words, we want to find the maximum likelihood solution for the log
marginal-likelihood
log pθ (R) =

p(R|τ )pθ (τ )dτ .

(2.37)

τ

As for the standard EM algorithm, a variational distribution q(τ ) is used to decompose
the log-marginal likelihood
log pθ (R) = Lθ (q) + KL (q(τ )||pθ (τ |R)) ,

(2.38)

where p(τ |R) is denoted as reward-weighted trajectory distribution
pθ (τ |R) =

p(R|τ )pθ (τ )
∝ p(R|τ )pθ (τ )
p(R|τ )pθ (τ )dτ

(2.39)

In the E-step, we minimize KL (q(τ )||pθ (τ |R)), and, hence, set q(τ ) to the reward
weighted model distribution. In the M-step, we maximize the expected complete data
log-likelihood
argmaxθ Qθ (q) = argmaxθ
= argmaxθ

q(τ ) log p(R|θ)pθ (τ ) dτ

(2.40)

q(τ ) log pθ (τ )dτ + f (q)

= argminθ KL (q(τ )||pθ (τ )) .
We distinguish between two different update procedures for the E- and the M-step,
called Monte-Carlo EM approaches [32, 50] and Variational Inference for Policy Search
[41]. The methods differ in the type of projection they use for approximating the reward
weighted trajectory distribution pθ (τ |R).
Approximating a Distribution. To approximate a distribution p(y) by a distribution p˜θ (y), we can minimize either KL-divergence, i.e., argminθ KL (p(y)||˜
pθ (y)) or
argminθ KL (˜
pθ (y)||p(y)). The natural question arises ‘which KL-divergence is better
suited for policy search?’
Minimizing argminθ KL (p(y)||˜
pθ (y)) is called the Moment-Projection of the distribution p(y) as it matches the moments of p˜θ (y) with the moments of p(y). It forces
the approximate distribution p˜ to have probability mass everywhere where p has nonnegligible probability mass. Consequently, if p˜ is a Gaussian, the M-projection averages
over all modes of p. Note that every maximum likelihood estimate is computed by using
the M-projection as
KL (p(y)||˜
pθ (y)) = Ep(y) [˜
pθ (y)] + H(p),
where H(p) denotes the entropy of the distribution p can be neglected for determining
θ.
27

Minimizing argminθ KL(˜
pθ (y)||p(y)) is called the Information projection of p(y).
The I-projection forces the approximate distribution p˜θ (y) to be zero everywhere where
p is zero. Unfortunately, it can not be determined in closed form for most distributions.
When using a Gaussian distribution p˜, the I-projection will concentrate on a single mode
of p(y) and loose information about the other modes contained in the samples.
We will first discuss the more common M-projection based algorithms for policy
search. Subsequently, we briefly highlight the differences to an I-projection algorithm,
called Variational Inference for Policy Search.
Monte-Carlo EM-based Policy Search.
Some of the most efficient policy search methods are Monte-Carlo EM methods [32,
50, 31, 71]. The Monte-Carlo (MC)-EM algorithm [39] is a variant of EM that uses a
sample based approximation for the variational distribution q, i.e., in the E-step MCEM minimizes the KL-divergence KL(q(Z)||pθ (Z|Y ) by the use samples Zi ∼ pθ (Z|Y ).
Subsequently, these samples Z i are used to estimate the expectation of the complete
data log-likelihood
K

Qθ (q) =

log pθ (Y , Z i ).

(2.41)

j=1

In terms of policy search, MC-EM methods use samples τ [i] from the trajectory distribution pθ (τ ) to represent the variational distribution q(τ ) ∝ p(R|τ )pθ (τ ) over trajectories. These samples are then used in the M-step for estimating the complete-data
log-likelihood. Consequently, we have to maximize
argmaxθ Qθ (θ ) = argmaxθ

p(R|τ i ) log pθ (τ i )

(2.42)

τ [i] ∼pθ (τ )

in the M-step. We observe that the maximization defined in Equation (2.42) corresponds
to minimizing KL(p(R|τ )pθ (τ ) pθ (τ )), and, hence, MC-EM based methods use the
Moment-Projection of the reward weighted trajectory distribution to obtain the new
policy parameters θ.
Episode-based EM-Algorithms
The episode-based version of MC-EM policy search algorithms has been used to learn
[i]
an upper-level policy πω (θ|s) for multiple contexts. As the returns R[i] = T rt are
typically not given as improper probability distribution, the weights d[i] ∝ exp(βR[i] ) are
computed by the exponential transformation of the rewards. Subsequently, the policy
update is given by the weighted maximum likelihood estimate of the policy parameters
ω. The general setup for Episode-Based EM-updates is given in Algorithm 10.
Reward Weighted Regression. Reward Weighted Regression (RWR) uses a linear
policy and, hence, the weighted maximum likelihood estimate performed by the EMupdate is given by a weighted linear regression. RWR was introduced in [50] to learn an
28

Algorithm 10 Episode-Based MC-EM Policy Updates
Input: inverse temperature β
data-set Dep = s[i] , θ [i] , R[i]
i=1...N

Compute weighting d[i] = f (R[i] ) for each sample i
e.g., d[i] ∝ exp(βR[i] )
Compute weighted ML solution, see Equation (2.43) and (2.44)
N T −1

ω new = argmaxω
i=1 t=0

d[i] log πω (θ [i] |s[i] )

return ω new
inverse dynamics model for operational space control. However, the algorithm can be
straight-forwardly applied to episode-based policy search with multiple contexts. The
policy πω (θ|s) = N θ|W T φ(s), Σθ is represented as Gaussian linear model. Given
the data-set Dep and the weightings d[i] for each sample in Dep , the weighted maximum
likelihood solution for W is given by
W = (ΦT DΦ + λI)−1 ΦT DΘ,

(2.43)

where λ is a ridge factor, Φ = φ(s[1] ), . . . , φ(s[N ] ) contains the feature vectors of the
contexts, the diagonal matrix D contains the weights d[i] , and Θ = θ [1] , . . . , θ [N ]

T

the parameter vectors θ [i] that have been executed on the real system. The covariance
matrix Σθ can be updated accordingly by a weighted maximum likelihood estimate. The
update equations for Σθ are given in the appendix.
Cost-Regularized Kernel Regression. Cost-Regularized Kernel Regression (CRKR)
is the kernelized version of Reward-Weighted-Regression, which was one of the first algorithms used to learn upper-level policies for multiple contexts [31]. As most kernelregression methods, CRKR uses individual regressions for the individual output dimensions. The policy in CRKR πω (θ|s) = N (ω|µω (s), diag(σ ω (s)) is modeled as Gaussian
Process. The mean and the variance for the hth output dimension are given by
µh (s) = k(s)(K + λC)−1 Θh .,
σh2 (s)

T

(2.44)
−1

= k(s, s) + λ − k(s) (K + λC)

k(s).

(2.45)

The term K = ΦT Φ denotes the kernel matrix and k(s) = φ(s)ΦT represents the kernel
vector for a new query point s. The matrix C is denoted as cost matrix because it is
inversely related to the reward weighing used in RWR, i.e., C = D −1 . It is used as
input dependent noise prior for the Gaussian process [32].
As the standard kernel-regression formulation (for example, see Bishop [12], chapter
6.1), CrKR can be derived from linear regression by the use of the Woodbury Identity
29

[32]. Instead of standard linear regression, reward weighted regression is used to derive
CrKR. From Equations (2.44) and (2.45) we can see that CRKR defines a Gaussian
Process [55] where the costs determine an input dependent noise prior.
As CRKR is a non-parametric method it does not update a parameter vector. The
policy is determined by the given dataset. As CrKR is kernel-based, we do not need to
specify a feature vector φ(s), but can use a kernel. Kernels typically offer more flexibility
in modelling an function than user-specified feature vectors. For more detail about
kernel-methods for regression we refer to [55]. The disadvantage of using a kernel-method
is that the output dimensions of the policy πω (θ|s) are typically modelled as independent
Gaussian distributions, and, hence, no correlations can be modelled. Such uncorrelated
exploration strategies might result in a decreased performance of the algorithm.
Step-based EM-Algorithms
Step-based EM-Algorithms decompose the complete data log-likelihood Qθ (θ ) into the
single steps of the episode, which results in a similar decomposition used by other stepbased algorithms for the returns. We first show that Qθ (θ ) is a lower bound of the log
of the expected return log Jθ where we will assume that no reward transformation has
been used, i.e., p(R|τ ) ∝ R(τ ),
log Jθ = log

pθ (τ )R(τ )dτ = log pθ (R)

≥ Qθ (θ ) = Epθ

(τ ) [r(τ ) log pθ (τ )] .

(2.46)
(2.47)

By differentiating Qθ (θ ) with respect to θ, we get
∇θ Qθ (θ ) = Epθ
= Epθ

(τ ) [R(τ )∇θ

log pθ (τ )]

(2.48)

T −1
(τ )

R(τ )
t=0

∇θ log πθ (ut |xt ) .

(2.49)

Using the same observation as we used for the policy gradient theorem, i.e., past rewards
are independent of future actions, we obtain
T −1

∇θ Qθ (θ ) = Epθ

(τ )
t=0

Qπt (xt , ut )∇θ log πθ (ut |xt ) .

(2.50)

Setting Equation (2.50) to zero corresponds to performing a weighted maximum likelihood estimate on the step-based data-set Dstep for obtaining the parameters θ new of
[i]
[i]
policy πθ (ut |xt , t). The weighting is in the step-based case given by Qπt (xt , ut ) ≈
[i]
[i]
T −1
h=t r(xh , uh ). From Equation 2.50, we can see that step-based EM algorithms reduce policy search to an iterative reward-weighted imitation learning procedure. This
formulation is used to derive the most common EM-based policy search algorithm, Policy
learning by Weighting Exploration with Returns (PoWER) which has been introduced
in [31]. The general algorithm for step-based EM algorithms is illustrated in Algorithm
11.
30

Algorithm 11 Step-Based MC-EM Policy Updates
Input: Policy parametrization θ
[i]
[i]
[i]
data-set D = x1:T , u1:T −1 , Qt
[i]

i=1...N

[i]

Compute weighting dt ∝ Qt
Compute weighted maximum ML estimate, see Eqs. (2.51) and (2.53)
N T −1
[i]

θ new = argmaxθ

[i]

[i]

dt log πθ ut xt , t
i=1 t=0

Relation to Policy Gradients. There is a tight connection between the step-based
gradient methods introduced in Section 2.2.1 and the step-based EM-based approach.
In the limit, if θ is close to θ , we obtain the policy gradient theorem update rule from
the lower bound Qθ (θ ),
T −1

lim ∇θ Qθ (θ ) = Epθ (τ )

θ→θ

t=0

Qπt (xt , ut )∇θ log πθ (ut |xt ) = ∇PG
θ Jθ .

Unlike policy gradient methods, the EM-based approach allows to use a different parameter vector θ for the expectation of the trajectories as for the calculation of the gradient
∇θ log pθ (τ ). Consequently, the influence of the policy update ∆θ on the trajectory distribution pθ+∆θ (τ ) can be neglected as long as actions with high future reward Qπt (x, u)
get more likely.
However, such relationship can only be obtained, if we can linearly transform the
reward to an improper distribution. If we need to use an exponential transformation for
the rewards, such a direct relationship to policy gradient methods can not be established.
Episodic Reward Weighted Regression. Episodic Reward Weighted Regression
[31] is the step-based extension of RWR which we have presented in the previous section.
Similar to RWR, episodic RWR assumes a linear model for the policy, which is in the
step-based case given as πθ (ut |xt , t) = N (ut |W T φt (x), Σu ). The weighted maximum
likelihood estimate for θ = {W , Σu } is now performed on the step-based dataset Dstep ,
W new = (ΦT DΦ)−1 ΦT DU ,
[1]

[1]

[N ]

(2.51)

[N ]

where Φ = φ0 , . . . , φT −1 , . . . , φ0 , . . . , φT −1 contains the feature vectors for all time
steps t of all trajectories τ [i] , D is the diagonal weighting matrix containing the weight[i]
ings dt of each sample and the matrix U contains the control vectors u[i] . The covariance
matrix Σu of πt (ut |xt ) can be obtained accordingly.
Policy learning by Weighting Exploration with Returns. The policy update
of the PoWER algorithm [31] is similar to episodic RWR, however, PoWER uses a
31

structured exploration strategy which typically results in a beneficial performance of
PoWER in comparison to episodic RWR. To simplify our discussion we will for now
assume that the control action ut is one dimensional. RWR directly perturbs the controls ut = wT φt + t with zero-mean Gaussian noise. Instead, PoWER applies the
perturbation to the parameters w, i.e. ut = (w + t )T φt . Such policy can be written as
πt (ut |xt ) = N (ut |wT φt , φTt Σw φt ).
The weighted maximum likelihood solution for the parameters w yields
−1

T −1

Lt (x)Qπt (xt , ut )

wnew = wold + E
t=0

T −1

Lt (x)Qπt (xt , ut )

E

t

,

(2.52)

t=0

where Lt (x) = φt (x)φt (x)T (φt (x)T Σw φt (x))−1 and t is the noise term applied to the
parameter vector at times step t. As we can see, the exploration t is weighted by the
returns Qπt to obtain the new parameter vector w. The update rule of PoWER can also
be written in terms of matrices where we use the action vector U instead of the noise
terms t , i.e,
˜ −1 ΦT DU
˜ ,
wnew = (ΦT DΦ)
(2.53)
where U contains the actions of all time steps and all trajectories, ΦT is defined as
˜ ∈ RN T ×N T is a diagonal weighting matrix with the entries
in Equation (2.43) and D
T
[i]
[i]
d˜t = φt x[i] Σw φt x[i] Qt for each sample i and time step t. We observe that
PoWER is similar to episodic RWR, the only difference is that the data-points are
additionally weighted by the variance of the policy for state x[i] . As this notation does
[i]
not contain the noise terms t , it is also compatible with Algorithm 11.
Variational Inference for Policy Search
As we have seen, the Monte-Carlo EM approach uses the M-Projection of the reward weighted trajectory distribution to update the policy parameters. While the Mprojection can be computed efficiently using weighted maximum likelihood estimates, it
might also suffer from a major caveat: it averages over several modes of the reward function. Such a behavior might result in slow convergence to good policies as the average
of several modes might contain areas with low reward. Alternatively, the I-projection
can be used to update the policy, as introduced in the Variational Inference for Policy
Search [41] algorithm.
In the variational approach, a parametric representation of the variational distribution qθ (τ ) is used instead of the sample-based approximation used in the MC-EM
approach. It is convenient to choose qθ (τ ) from the same family of distributions as pθ (τ ).
Now, a sample-based approximation is used to replace the integral in the KL-divergence
KL (qθ (τ )||p(R|τ )pθ (τ )) needed for the E-step, i.e.,
θ = argminθ˜ KL qθ˜ (τ )||p(R|τ )pθ (τ ) .

(2.54)

The minimization of this KL-divergence is equivalent to the I-projection of the rewardweighted trajectory distribution p(R|τ )pθ (τ ). In the variational approach, the M-step
now trivially reduces to setting the new parameter vector θ new to θ .
32

M−Projection
I−Projection

Figure 2.4: Comparision of using the I versus the M-projection for inference based policy
search. While the M-projection averages over several modes of the reward function, the
I-projection concentrates on a single mode, and, therefore, avoids including areas of low
reward in the policy distribution.
Hence, the MC-EM and the variational inference algorithm only differ in the employed projections of pθ (τ |R). As the projections are in general different, they converge
to a different (local) maximum of the lower bound Lθ (q). The Variational Inference algorithm has been used for learning the upper-level policy πω (θ|s). If we use a Gaussian
distribution for πω (θ|s), the I-projection concentrates on a single mode. Such behavior
can be beneficial if all modes are almost equally good, however, the I-projection might
also choose a sub-optimal mode (which has a lower reward). The M-projection always
averages over all modes, and, therefore, might also include large areas of low reward in
the distribution. The behavior of both approaches for a simple multi-modal toy problem
is illustrated in Figure 2.4.
If the target distribution is uni-modal, both projections yield almost the same solutions. However, using the I-projection is computationally demanding, and, hence, the
variational inference approach is generally not the method of choice. In addition, if
we use a more complex distribution for modeling the policy, for example a mixture of
Gaussians, the difference between the I- and the M-projection becomes less pronounced.

2.2.3

Information-Theoretic Approaches

Information-Theoretic approaches bound the distance between the old trajectory distribution q(τ ) and the newly estimated trajectory distribution p(τ ) at each update step.
Such regularization of the policy update limits the information loss of the updates, and,
hence, avoids that the new distribution p(τ ) prematurely concentrates on local optima
of the reward landscape. The first type of algorithms to implement this insight from
information theory were the Natural Policy Gradient algorithms [52], which have been
discussed in the policy gradient section. However, as policy gradient algorithms always
require a user-specified learning rate, such algorithms have severe disadvantages in comparison to EM-based methods. Yet, the information theoretic insight was taken up in
33

[48] with the Relative Entropy Policy Search (REPS) algorithm. REPS uses the same
information theoretic bound as the NAC algorithm but simultaneously updates its policy by weighted maximum likelihood estimates as it is done for EM-based approaches.
Hence, it combines the advantage of a fast and uniform convergence from the NAC algorithm with the benefits of EM-approaches which do not require a learning rate and
can take larger policy update steps.
REPS for Infinite Horizon Problems
Relative Entropy Policy Search (REPS) [48] uses the KL divergence KL(p(τ )||q(τ )), also
called relative entropy, as distance measure to the old trajectory distribution q(τ ). REPS
formulates the policy search problem as optimization problem where the optimization is
done directly in the space of trajectory distributions p(τ ) without considering a direct or
indirect parametrization of p(τ ). As we will see, the REPS optimization problem allows
for a closed form solution of p(τ ). The used distance measure KL(p(τ )||q(τ )) forces
p(τ ) to be low everywhere where q(τ ) is also low. Intuitively, bounding KL(p(τ )||q(τ ))
prevents p(τ ) to ‘move outside’ the old distribution q(τ ) as such behavior is potentially
dangerous for the robot. The use of the opposite KL-divergence KL(q(τ )|p(τ )) would
not show this favourable property and also does not allow for a closed form solution for
p(τ ).
We will first discuss the most general formulation of REPS which have been developed
for the infinite horizon case [48]. Subsequently, we will introduce REPS for policy search
with episode-based policy evaluation and an hierarchical policy search approach based
on REPS [17].
Infinite Horizon Formulation. REPS formulates the policy update step as constrained optimization problem. In the infinite horizon formulation, REPS maximizes
the average reward per time step, given as
µπ (x)π(u|x)r(x, u)

Jπ,µπ =

(2.55)

x,u

For simplicity, we will for now only consider a discrete set of state and actions, but, as we
will show later, it generalizes straightforwardly to continuous states and actions. Necessary extensions for continuous state and actions are discussed subsequently. Also note
that we are directly optimizing over probability distributions instead of over parameters
of a distribution. The distribution µπ (x) denotes the stationary state distribution of the
MDP with policy π. The state distribution µπ (x) has to fulfill the following constraint
∀x : µπ (x ) =

x,u

µπ (x)π(u|x)p(x |x, u)

(2.56)

in order to be a valid stationary distribution. In other words, the state distribution has
to be consistent with the policy π and the state transition model p(x |x, u).
34

The key idea of REPS is now to bound the relative entropy between the old stateaction distribution q(x, u) and the new state action distribution µπ (x)π(u|x), i.e,
≥

µπ (x)π(u|x) log
x,u

µπ (x)π(u|x)
.
q(x, u)

(2.57)

This equation limits the loss of information and ensures a smooth learning progress.
Information Theoretic Policy Updates.
as follows:

µπ (x)π(u|x)r(x, u),

max
π
π,µ

s. t.

The resulting program can be formalized

x,u

≥

∀x : µπ (x ) =

µπ (x)π(u|x) log
x,u

x,u

µπ (x)π(u|x)
q(x, u)

µπ (x)π(u|x)p(x |x, u),
µπ (x)π(u|x),

1=

(2.58)

x,u

where the last constraint ensures that µπ (x)π(u|x) is a normalized probability distribution. This constrained optimization problem can be solved efficiently by the method
of Lagrangian multipliers. Please consult the appendix for more details. From the Lagrangian, we can also obtain a closed form solution for the state-action distribution
µπ (x)π(u|x) which is given as
µπ (x)π(u|x) ∝ q(x, u) exp

r(x, u) + Ex [V (x )] − V (x)
η

(2.59)

for the joint distribution, and as
π(u|x) ∝ q(u|x) exp

r(x, u) + Ex [V (x )]
η

(2.60)

for the policy π(u|x). The parameter η denotes the Lagrangian multiplier for the relative entropy bound and the parameters V (x ) denote the Lagrangian multipliers for the
stationary distribution constraint from Equation (2.58). The Lagrangian parameters can
be efficiently obtained by minimizing the dual function g(η, V ) of the optimization problem. Intuitively, V (x) can be seen as a value-function as expected value Ep(x |x,u) [V (x )]
of the next state is added to the reward. With this interpretation, we can see that
the term δV (x, u) = r(x, u) + Ep(x |x,u) [V (x )] − V (x) is equivalent to the advantage
function or average temporal difference error of state-action pair (x, u). Hence, the
advantage function is used to determine the exponential weighting of the state-action
pairs. The baseline V (x) is highly connected to the policy gradient baseline. However,
the REPS baseline directly emerged out of the derivation of the algorithm while it has
to be artificially added for the policy gradient algorithms.
35

The Dual Function. The dual function g(η, V ) of the REPS optimization problem
is given in log-sum-exp form
g(η, V ) =

q(x, u) exp

η + η log
x,u

δV (x, u)
η

.

(2.61)

Due to its log-sum-exp form, the dual-function is convex in V . Furthermore, we can
approximate this sum over all states and actions with a sum over samples (x[i] , u[i] ) from
the distribution q(x, u), i.e.,
g(η, V ) =

η + η log

1
N

exp
x[i] ,u[i]

δV (x[i] , u[i] )
η

.

(2.62)

Consequently, we do not need to know the distribution q(x, u) as a function, we just
need to be able to sample from it.
Handling Continuous States. In continuous state spaces, we will replace the sum
over all states x as the sum over samples from the q(x, u). However, the stationary
distribution constraint can no longer remain fulfilled for every state x as there is an
uncountable amount of states. Still, we can require that the expected state-features
ϕ(x) from the distribution µπ (x ) matches the expected features of the distribution
µ
˜π (x ) =
x,u

µπ (x)π(u|x)p(x |x, u),

which corresponds to µπ (x ) after applying the policy and the system dynamics. Such
a constraint can be formalized as
µπ (x )ϕ(x ) =
x

x ,u,x

µπ (x)π(u|x)p(x |x, u)ϕ(x ).

(2.63)

For example, if the state features contain all linear and squared terms of state x, this
constraint ensures that the mean and the variance of µπ (x ) match the mean and variance of µ
˜π (x ). By replacing the constraint from Equation (2.63), we obtain the same
structure of the solution for µπ (x)π(u|x), where the value function V (x) appears as linear feature-based representation Vv (x) = ϕ(x)T v. The parameter vector v denotes the
Lagrangian multiplier introduced by the constraint given in 2.63.
Continuous state spaces also have the consequence that we can only optimize the
REPS problem with respect to the joint distribution µπ (x)π(u|x) as we have only seen
one action u[i] for each state sample x[i] .
Handling Continuous Actions. To deal with continuous actions, we need to use a
parametrized policy πθ (u|x). However, in the REPS optimization problem can still be
solved for the non-parametric distribution π(u|x) if it was defined on a discrete set of
sampled state action pairs. After having obtained the probabilities µπ (x[i] )π(u[i] |x[i] )
36

for the state action pairs (x[i] , u[i] ), a parametric policy πθ (u|x) is fitted to π(u[i] |x[i] )
by minimizing the conditional Kullback-Leibler divergence
µπ (x)KL(π(u|x)|πθ (u|x)),

θ new = argminθ

(2.64)

x

q(x, u) exp

= argmaxθ
x,u

= argmaxθ

exp
x[i] ,u[i]

δV (x, u)
η

δV (x[i] , u[i] )
η

log πθ (u|x)
log πθ (u[i] |x[i] ).

(2.65)

(2.66)

Clearly, this minimization corresponds to a weighted maximum likelihood fit with the
weighting being equal to d[i] = exp δV (x[i] , u[i] )/η .
From this result, we observe the close relationship to EM-based policy search algorithms. For general reward functions, EM-based algorithms use a exponential transformation of the expected future return, i.e., d[i] = exp βQπ (x[i] , u[i] ) , where the inverse
temperature β has to be chosen by the user. In contrast, REPS always returns the
optimized temperature η of the exponential weighting which exactly corresponds to the
desired KL-bound. Note that the scaling η will be different for each iteration of policy
search depending on the distribution of the current reward samples. Furthermore, REPS
uses the value-function as baseline to account for different achievable values in different
states.
Representing the Model. A single sample xt+1 from the observed transition can be
used to approximate Ep(x |x,u) [V (x )], and hence, no model needs to be known. However,
such approximation causes a bias, and, hence, the policy does not optimize the average
reward anymore. We summarize the REPS algorithm for the infinite horizon formulation
in Algorithm 12. The algorithm computes the expected feature change for each state
action pair. However, for continuous states and actions, each state action pair is typically
only visited once, and, hence the expectation is approximated using a single sample
estimate. Instead of using q as the state-action distribution of the old policy, we can
also reuse samples by assuming that q(x, u) is the state-action distribution of the last
K policies.
Episode-based Relative Entropy Policy Search
It is easy to reformulate the REPS algorithm as a model-free algorithm in the episodebased formulation [17]. In the episode-based formulation we need to learn an upper-level
policy πω (θ) for selecting the parameters of the lower-level policy πθ (ut |xt ) to maximize
the average return Jω , which has been defined in Equation (2.1). As the upper-level
policy directly searches for parameters θ and is only executed for a single ’step’, we do
not need to incorporate the state distribution in our model, and, hence, we also do not

37

Algorithm 12 REPS for infinite horizon problems
Input: KL-bounding
data-set D = x[i] , u[i] , r[i] , x [i] i=1...N
for i = 1 . . . N do
State-action visits: n(x[i] , u[i] ) = j Iij
Summed reward: r˜(x[i] , u[i] ) = j Iij r[j]
˜ x[i] , u[i] = j Iij ϕ(x [j] ) − ϕ(x[j] )
Summed features: ∆ϕ
end for (Iij is 1 if x[i] = x[j] and u[i] = u[j] , 0 elsewhere)
Compute sample bellman error
˜ [i] , u[i] )
r˜(x[i] , u[i] ) + v T ∆ϕ(x
n(x[i] , u[i] )

[i]

δv =

Optimize dual-function [η, v] = argminη ,v g(η , v ),
g(η, v) = η + η log

N

1
N

s.t. η > 0
[i]

exp
i=1

δv
η

Obtain parametric policy πθ (u|x) by weighted ML estimate
N

θ k+1 = argmaxθ

[i]

exp
i=1

δv
η

log πθ u[i] x[i]

need to know the transition probabilities p(x |x, u). The optimization problem becomes
π(θ)R(θ),

max
π

s. t.

θ

≥

π(θ) log
θ

π(θ),

1=

π(θ)
,
q(θ)
(2.67)

θ

The dual function for this optimization problem is given in the Appendix. The nonparametric policy π(θ) can be obtained in closed form for a discrete set of samples and
is given by
R(θ)
π(θ) ∝ q(θ) exp
.
(2.68)
η
The temperature η of the exponential distribution is determined by optimizing the dual
function g(η).
The parametric distribution πω (θ) is subsequently obtained by a weighted maximum
likelihood estimate on the samples θ [i] with the weightings d[i] = exp R(θ [i] )/η . To
38

increase the sample efficiency, the expected return R(θ [i] ) = pθ (τ )R(τ )dτ is typically
approximated with a single sample. However, such strategy might again introduce a bias
to the optimization problem.
It seems natural to define q(x, u) as the old policy π(ω k−1 ), however, we can use the
last K policies as q(x, u) to reuse samples from previous iterations.
Extension to Multiple Contexts. We can choose the upper-level policy πω (θ|s) to
select the policy parameters θ based on the context s. The objective is to maximize
the average return Jω defined in Equation (2.3) over the given distribution p(s) over
the context. Since REPS is based on samples, and we can typically only access samples
from the joint distribution q(s, θ) = p(s)q(θ|s), REPS can only optimize for the joint
distribution p(s, θ) = p(s)πω (θ|s). To ensure that the estimated joint distribution
still reproduces the given distribution p(s) over contexts, we have to add the following
constraint
∀s :
p(s, θ) = p(s).
(2.69)
θ

This constraint can be reformulated for continuous context variables using featureaverages
p(s, θ)r(θ, s),

max
p

s. t.

s,θ

≥

p(s, θ) log
s,θ

ˆ =
ϕ

p(s, θ)
,
q(s, θ)

p(s, θ)ϕ(s),
s,θ

p(s, θ),

1=

(2.70)

s,θ

ˆ is the average feature vector observed for the contexts. Note that the bound
where ϕ
for the relative entropy can, due to the constraint for the context distribution given in
Equation (2.69), be written as
≥

p(s)π(θ|s) log
θ,s

π(θ|s)
,
q(θ|s)

(2.71)

which is equivalent to bounding the conditional relative entropy between π(θ|s) and
q(θ|s). The resulting distribution p(s, θ) is given as
p(s, θ) ∝ q(s, θ) exp

r(θ, s) − V (s)
η

,

(2.72)

where V (s) = ϕ(s)T v. As before, the Lagrangian multipliers η and v are obtained by
minimizing the dual function g(η, v). The function V (s) denotes a context dependent
baseline which is subtracted from the reward. The episode-based REPS algorithm for
generalizing the upper-level policy to different contexts is given in Algorithm 13
39

Algorithm 13 Episode-Based REPS Updates for Multiple Contexts
Input: KL-bounding
data-set Dep = s[i] , θ [i] , R[i]
i=1...N

Optimize dual-function [η, v] = argminη ,v g(η , v ),
N

ˆ + η log
g(η, v) = η + v T ϕ
i=1

1
exp
N

s.t. η > 0

R[i] − v T ϕ s[i]
η

Obtain parametric policy πω (θ|s) by weighted ML estimate
N

ω new = argmaxω

exp
i=1

R[i] − v T ϕ s[i]
η

log πω θ [i] |s[i]

Learning Hierarchical Policies with REPS
Using maximum likelihood estimates for the parameter updates is beneficial for learning
hierarchical policies [17]. REPS can be extended to hierarchical policies by reformulating
the problem at hand as latent variable estimation problem. The resulting HiREPS
algorithm has been used to learn different options for one motor skill. The policy consists
of a gating-policy π(o|x) which selects the option to execute based on the current state.
Subsequently, the option policy π(u|x, o) selects the actions which are executed by the
robot. With such a hierarchical representation, we can represent multiple solutions for
the same motor task element, such as, fore-hand and back-hand strokes in robot table
tennis. The gating policy allows inferring which options are feasible for which state or
context, allowing us to construct complex policies out of simpler ‘option policies’.
Hierarchical Policy Learning as Latent Variable Estimation Problem. For
efficient data-usage, we have to allow that actions from other options o can also be used
to update the option policy π(u|x, o) of option o. To achieve this goal, the options are
treated as latent variables which are unobserved. Hence, only q(x, u) can be accessed and
not q(x, u, o). Still, the KL-bounding can be written in terms of the joint distribution
µπ (x)π(o|x)π(u|x, o) by
≥

µπ (x)π(o|x)π(u|x, o) log
x,u,o

µπ (x)π(o|x)π(u|x, o)
q(x, u)p(o|x, u)

,

(2.73)

where p(o|x, u) is yielded by Bayes theorem. Furthermore, options should not overlap
as we want to learn distinct solutions for the motor task. As a measure for the overlap
of the options, the expected entropy of p(o|x, u) is used, i.e.,
Ex,u [H(p(o|x, u)] = −

µπ (x)π(o|x)π(u|x, o) log p(o|x, u).
x,u,o

40

(2.74)

Figure 2.5: Comparison REPS and HiREPS with and without bounding the overlap
of the options on a simple bimodal reward function. The standard REPS approach
only uses one option which averages over both modes. The HiREPS approach can use
multiple options (two are shown). However, if we do not bound the overlap (κ = ∞),
the options do not separate and concentrate on both modes. Only if we use the overlap
constraint, we get a clear separation of the policies.
The overlap of the options should decrease for a certain percentage at each policy update
step. Hence, the following constraint is introduced
κ ≥ Ex,u [H(p(o|x, u)] .

(2.75)

The upper bound κ is usually set as a percentage of the currently measured overlap
ˆ q , i.e., κ = H
ˆqκ
H
˜ , where 1 − κ
˜ denotes the desired decrease of the overlap. Figure
2.5 illustrates the resulting policy updates with and without bounding the overlap on
a simple multi-modal reward function. Without bounding the overlap of the options,
both options will concentrate on both modes of the reward function. As a consequence,
the quality of both options is rather poor. By introducing the overlap constraint, both
options separate early in the optimization process concentrating on the separate modes.

41

Lower Bound. Putting together the new constraints, HiREPS is defined as the following optimization problem:
µπ (x)π(o|x)π(u|x, o)r(x, u),

max
π
π,µ

s. t.

x,u,o

≥

∀x : µπ (x ) =

µπ (x)π(o|x)π(u|x, o) log
x,u,o

x,u,o

µπ (x)π(o|x)π(u|x, o)
q(x, u)p(o|x, u)

µπ (x)π(o|x)π(u|x, o)p(x |x, u),

ˆ q ≥ Ex,u [H(p(o|x, u)] ,
κH

µπ (x)π(o|x)π(u|x, o),

1=

(2.76)

x,u,o

Unfortunately, this optimization problem can not be solved in closed form as it contains
the conditional distribution p(o|x, u) inside the logarithm. However, a lower bound of
this optimization problem can be obtained using an EM-like update procedure [17]. In
the E-step, we estimate
p˜(o|x, u) =

µπ (x)π(o|x)π(u|x, o)
.
π
o µ (x)π(o|x)π(u|x, o)

In the M-step, we use p˜(o|x, u) for p(o|x, u) in the optimization problem, and, therefore,
neglect the relationship between p(o|x, u) and the joint distribution µπ (x)π(o|x)π(u|x, o).
Similar as for EM, it can be shown that this iterative optimization procedure updates a
lower bound on the original optimization problem which is tight after each E-step [17].
The resulting joint distribution has the following solution:
µπ (x)π(o|x)π(u|x, o) ∝ q(x, u)˜
p(o|x, u)1+κ/η exp

δV (x, u)
η

,

(2.77)

where κ denotes the Lagrangian multiplier from bounding the overlap of the options.
Hierarchical Episode-Based REPS. The episode-based HiREPS formulation can
be used directly in parameter-space of the policy, which results in the following solution
for the policy
p(s)π(o|s)π(θ|o, s) ∝ q(s, θ)˜
p(o|s, θ)1+κ/η exp

r(s, θ) − V (s)
η

,

where s is the context. The episode-based HiREPS algorithm is summarized in Algorithm 14. The dual function g(η, v) needed to obtain the parameters η and v is given
in the appendix.

42

Algorithm 14 Episode-Based HiREPS for Multiple Contexts
Input: KL-bounding , overlap-bounding κ
data-set Dep = s[i] , θ [i] , R[i]
j=1...N

old gating: q(o|s), old option-policies q(θ|s, o)
Compute p(o|s, θ) for all options and samples i
p˜ (o|i) =

q(θ [i] |s[i] , o)q(o|s[i] )
o

q(θ [i] |s[i] , o )q(o |s[i] )

Optimize dual-function, see Equation (4.18)
[η, v] = argminη ,v g(η , v ),

s.t. η > 0

O (θ|s, o) for all options o
Obtain option policies πω
N

ω onew = argmaxω
i=1

O
do[i] log πω
θ [i] |s[i] , o

G (o|s) by weighted ML estimate
Obtain gating policy πω
N

ωG
new

G
do[i] log πω
o|s[i]

= argmaxω
i=1

[i]

with do = p˜ (o|i)1+ξ/η exp

2.2.4

o

R[i] −v T ϕ s[i]
η

Miscellaneous Important Methods

In this section, we will cover two types of algorithms, stochastic optimization and policy
improvements with path integrals, which lead to promising results in robotics. For the
stochastic optimization algorithm, we will discuss the Covariance Matrix Adaption Evolutionary Strategy (CMA-ES) algorithm while for the path integral approach, we
will discuss the Policy Improvements with Path Integral (PI2 ) algorithm.
Stochastic Optimization
Stochastic optimizers are black-bock optimizers, and, hence, can be straightforwardly
applied for policy search in the episode-based formulation. As many episode-based algorithms, they model a upper-level policy πω (θ) to create samples in the parameter-space,
which are subsequently evaluated on the real system.
The Covariance Matrix Adaptation - Evolutionary Strategy. The Covariance
Matrix Adaptation - Evolutionary Strategy (CMA-ES) is considered as the state of
43

the art in stochastic optimization [23]. CMA-ES was also applied to policy search in
robotics [25] and yielded promising results on standard benchmark tasks. The procedure
of CMA-ES is similar to many episode-based policy search methods such as episode-based
EM or episode-based REPS. CMA-ES maintains a Gaussian distribution πω (θ) over the
parameter vector θ, and uses the data-set Dep for the policy updates. Similar to the EMbased approaches, CMA-ES also uses a weight for each sample. However, for estimating
the weight d[i] and updating the distribution πω (θ) CMA-ES uses heuristics, which often
work well in practice but are not founded on a theoretical basis. For estimating the weight
d[i] , CMA-ES first sorts the samples θ [i] according to their return, and, subsequently,
computes the weight of the best l samples by d[i] = log(l + 1) − log(i). All other samples
are neglected, i.e., get zero weight. The policy, from which CMA-ES draws its samples,
is given by N (θ|µ, σ 2 Σ). The parameter σ 2 controls the step-size and Σ the direction of
exploration. Similar to weighted ML updates, the new mean of policy πω (θ) is computed
by the weighted average of the data points. However, the update of the step-size σ 2 and
the covariance matrix Σ is based on a combination of weighted samples and information
about the ’evolution path’ pσ2 and pΣ .
The advantage of such an approach is that the covariance matrix update is not only
dependent on the current set of samples, and, hence, requires only few samples θ [i] . The
number of samples N to evaluate for CMA-ES is typically fixed to max(4 + 3 log D, 5),
where D is the dimensionality of θ and the number of samples l used for the weighting
is typically set to N/2. While CMA-ES is a black-box optimizer and, therefore, simple
to use, it also has severe disadvantages. It cannot be used for generalizing the upperlevel policy to multiple contexts. Furthermore, several roll-outs have to be evaluated
if the evaluation R[i] is noisy. The minimum number of required roll-outs for a given
parameter θ [i] can be computed by Hoeffding and Bernstein races [24]. For a more
detailed discussion about the CMA-ES updates we refer to [23].
Policy Improvement by Path Integrals
The Policy Improvements by Path Integrals (PI2 ) algorithm [68] is based on the path integral approach to optimal control. The path integral approach is designed for obtaining
optimal control laws for non-linear continuous time systems of the form
x˙ t = f (xt ) + G(xt )(ut +

t)

= f t + Gt (ut +

t ).

(2.78)

where f t denotes a drift term, Gt the control matrix of the system and t is zero mean
Gaussian noise with variance Σu . Such assumption does not limit the generality of the
approach as it can be used to describe all robotic systems and it allows deriving an
efficient policy search method used for Dynamic Movement Primitives [26].
Path Integrals. Path integrals can be used to obtain optimal controls for finite horizon
control problems. The derivation of the path integrals is based on the continuous time
Hamilton-Jacobi Bellman Equation (HJB) for the optimal value function Vt . For a
detailed explanation of the HJB equation, and the connection to path integrals we refer
44

to [68]. In this section, we will only briefly review concepts which are relevant for the
resulting policy search algorithm. The HJB equation is transformed to a system of linear
partial differential equations by performing an exponential transformation of the optimal
value function Vt = λ log Ψt . Under the assumption of quadratic control costs uT Ru
where R has to match the system noise variance Σu by λR−1 = Σu , the optimal controls
ut can be computed in closed form with the Feynman-Kac theorem [46]
ut =

pco (τ t )uL (τ t )dτ ,

(2.79)

exp (S(τ t |xt )/λ)
,
exp (S(τ t |xt )/λ) dτ

(2.80)

with
pco (τ |xt ) =

T −1

S(τ |xt ) =

k=t

rk (xk ) + rT (xk ) + log puc (τ t |xt )

and
uL (τ t ) = R−1 Gt Gt R−1 GTt

−1

Gt t .

(2.81)

(2.82)

The distribution pco (τ |xt ) is represented by a soft-max distribution which has the path
integrals S(τ |xt ) of each trajectory in its exponent. The path integral S(τ |xt ) of a
trajectory is defined as the return of the trajectory plus the log probability of the trajectory with the uncontrolled process log puc (τ t |xt ). The uncontrolled process dynamics
are given by the system dynamics without control, i.e., the system is only driven by the
drift term and the system noise, i.e., x˙ t = f t + Gt ( t ). The term log puc (τ t |xt ) can
intuitively be understood as punishment term for trajectories which are less likely. Due
to the assumption of λR−1 = Σu , the return and the probability of a trajectory can be
treated in a unified way.
The correction action uL (τ t ) is the action which follows the trajectory τ t while
minimizing the immediate control costs. The term t is the noise term applied at time
−1
step t for trajectory τ t . The matrix R−1 Gt Gt R−1 GTt
Gt projects the applied noise
term into the null-space of the control matrix Gt and, hence eliminates the unnecessary
part of the control action u.
By combining Equations (2.82) and (2.79), the optimal control law is given in form of
a soft-max distribution pco (τ ) which weights relevant part of the noise term t according
to their path integrals S(τ ). Equation (2.79) also reveals the main result of path integrals
— the optimal action can be obtained by performing Monte-Carlo roll-outs instead of
applying dynamic programming. Moreover, the max-operation (which is typically needed
to obtain the optimality of the action) can be replaced by a soft-max operator.
Iterative Path Integral Control. In practice, to solve the integral in Equation (2.79)
over the trajectory space, we have to sample from the uncontrolled process. However,
this sampling process can be inefficient for high-dimensional systems and a high number
of samples is needed to obtain accurate estimate. The number of required samples can
45

be reduced by using an iterative approach. At each iteration k, we only compute the
optimal change ∆k ut in the action for time step t. Subsequently, the mean action uk,t
for time step t is updated by uk,t = uk−1,t + ∆k ut . As we now search for an optimal
change of the action ∆u, the current estimate uk,t of the action is subsumed in the
drift-term, i.e., f˜ k,t = f˜t + Gt uk,t . However, the explicit dependence of uk,t from the
current state is ignored in this iterative formulation.
Note that the path integral approach does only estimate the mean control action of
the policy and not the variance. Exploration is solely performed by the user-specified
uncontrolled dynamics.
Policy Improvements by Path Integrals. The Policy Improvements by Path Integral (PI2 ) algorithm [68] applies the path integral theory to the problem of learning
Dynamic Movement Primitives (DMPs) as policy representation. In this paragraph we
will restrict ourself to learning a DMP for a single joint, however, for multiple joints,
learning can be performed straightforwardly by applying the discussed update rules for
each joint separately. The path integral theory can be applied directly to DMPs by
treating the DMP parameter vector w as control action for the dynamical system
T

y¨ = τ 2 αy (βy (g − y) − y)
˙ + τ 2 φt (wt +

t)

(2.83)

Gt

ft

which defines the DMP trajectory. The drift term ft is given by the spring damper
system of the DMP and the control matrix Gt is given by the basis functions φt of the
DMP. The control action ut for the DMP is defined as acceleration of the joint y¨. Since
we apply the exploration noise t at each time step to the parameter vector wt , the
exploration policy is given by N (ut |ft , Ht ) with Ht = φTt Σw φt , which is equivalent to
the exploration policy used by PoWER [31].
To define the path-integral update rule, we need to compute the path integral S(τ )
for a given trajectory. This step requires the knowledge of the uncontrolled trajectory
distribution puc (τ |xt ), and, hence, the knowledge of the system model. However, if we
assume the rewards rt to solely depend on the state of the DMP, and not on the real
state of the robot or its environment, i.e. rt (xt ) = rt (yt , y˙ t ), the uncontrolled dynamics
puc (τ |xt ) = Tt=0 p(yt , y˙ t |yt−1 , y˙ t−1 ) can be determined straightforwardly. After a few
transformations, the path integral can be written as
T −1

rl + (wl + M l l )T R(wl + M l l ))

S(τ t ) = rT +

(2.84)

l=t

where M l = φl Hl−1 φTl . The new parameter vector wnew,t is acquired by the iterative
path integral update rule. Using Equation (2.79) for the optimal control action yields
wnew,t = wt +

pco (τ t )Mt t dτ ,

46

(2.85)

where pco (τ t ) is given as the soft-max distribution in Equation (2.81). So far, we used
a different parameter vector wt for each time step. However, in practice, we can only
use a single parameter vector w for one episode, and, thus, we need to average over the
parameter update for all time-steps. The average update is computed by weighting each
parameter vector wnew,t with the number of time steps to go. Additionally, the update
for the j-th dimension of time step t is weighted by the activation of the j-th feature
function [φt ]j ,
[wnew ]j

=

T −1
t=0 (T − t) [φt ]j [w new,t ]j
,
T −1
t=0 (T − i) [φt ]j

(2.86)

T −1 N

≈ [wold ]j +
dt,j

[i]

pco τ t dt,j

[i]
t

t=0 i=1
T

= (T − t) [φt ]j /(

(T − t ) [φt ]j .

j

, with

(2.87)

(2.88)

t =0

Equation (2.88) defines the update rule of the original PI2 algorithm given in [68]. We
observe that PI2 fits into our categorization of using a step-based policy evaluation
strategy, which uses the the reward to come in the current trajectory plus a punishment
term for unlikely trajectories as evaluation. Similar to the PoWER [31] algorithm, this
evaluation is used by the soft-max policy to obtain a weighting for each of the samples.
Episode-Based PI2 . The PI2 algorithm has also been used in the episode-based formulation [64] which also allows for updating the exploration strategy. The basic PI2
algorithm does not update its exploration strategy and has to rely on the uncontrolled
process dynamics. However, the noise-variance of the uncontrolled process dynamics has
a large impact on the learning performance and, hence needs to be chosen appropriately
by the user. To do so, the PI2 algorithm was reformulated in the episode-based policy search formulation and the condition that of the noise covariance needs the match
the control costs was omitted. Due to the episode-based formulation, the policy can
be directly estimated in parameter space. Instead of using the uncontrolled process for
exploration, the previously estimated policy is used for exploration. Furthermore, the
log-term for the uncontrolled dynamics puc (τ |xt ) in the path integral S(τ ) is neglected as
this term does not seem to improve the performance. Consequently, the returns R[i] are
directly used as path integrals. The covariance matrix was updated by a weighted maximum likelihood estimate, where the soft-max distribution pco (τ t ) was used as weighting.
The resulting PI2 variant is simplified version of PI2 , but shows an improved learning
performance. However, the theoretical justifications for many of these simplifications
are missing.
Relation to Information-Theoretic and EM Approaches. Despite that information theoretic and EM approaches have been developed from different principles as the
path integral approach, all these approaches share the same characteristics.
47

The episode-based formulation of PI2 shares many similarities with the episode-based
REPS formulation. By pulling the log puc (τ t |xt ) term outside the exp, we realize that
pco (τ |xt ) shares a similar soft-max structure as the closed form solution in REPS for
π(u|x). In REPS, the trajectory distribution q(τ ) of the old policy is used instead of the
uncontrolled process distribution puc (τ t |xt ). A similar strategy is emulated by using the
iterative path integrals update or the PI2 variant given in [64]. While REPS is designed
to be an iterative algorithm, the iterative sampling process of PI2 needs to be motivated
from a heuristic view point.
The parameter λ in path integral control corresponds to the Lagrangian parameter η
in REPS. This parameter is automatically set in REPS according to the relative entropy
bound, while for path integral control, this parameter is typically set by heuristics.
The path integral approach also relies on more assumptions than information theoretic
approaches. For example, the control cost matrix R is predefined as λR−1 = Σu .
Dropping this assumption even results in a more efficient algorithm [64], but theoretical
justifications for such an algorithm are lost. Similar update rules emerge naturally for
the REPS algorithm without the need for heuristics.
The original step-based version of the PI2 algorithm is also closely related to the
PoWER algorithm. If we also use an exponential transformation of the reward for
PoWER, the updates are essentially the same. While PoWER uses a weighted maximum
likelihood update to obtain the new policy, PI2 needs to rely on heuristics to average the
update rules for the single time steps. From this discussion we conclude that, while PI2
shares many beneficial properties of information theoretic and EM-based approaches, it
relies on many assumptions without offering additional benefits, and, hence, is not our
method of choice.

48

Chapter 3

Model-based Policy Search
Model-free policy search methods as described in Chapter 2 are inherently based on
sampling trajectories τ i using the robot to find good policies π ∗ . Sampling trajectories
is relatively straightforward in computer simulation. However, when working with mechanical systems, such as robots, each sample corresponds to interacting directly with
the robot, which often requires substantial experimental time and causes wear and tear
in non-industrial robots. Depending on the task, it can either be easier to learn a model
or to learn a policy directly. Model-based policy search methods attempt to address the
problem of sample inefficiency by using observed data to learn a forward model of the
robot’s dynamics. Subsequently, this forward model is used for internal simulations of
the robot’s dynamics, based on which the policy is learned.
Model-based policy search algorithms typically assume the following set-up: The
state x evolves according to the Markovian dynamics
xt+1 = f (xt , ut ) + w ,

w ∼ p(x0 ) ,

(3.1)

where f is a nonlinear function, u is a control signal (action), and w is additive noise,
often chosen to be i.i.d. Gaussian. Moreover, an episodic set-up is considered where the
robot is reset to an initial state x0 after executing a policy. The initial state distribution
p(x0 ) is often given by a Gaussian distribution N x0 | µx0 , Σx0 . Furthermore, we consider
finite horizon problems, i.e., the policy-search objective is to find a parametrized policy
πθ∗ that maximizes the expected long-term reward
T

πθ∗ ∈ arg max J π = arg max
πθ

π

γ t E[r(xt , ut )|πθ ] ,
t=1

γ ∈ [0, 1] ,

(3.2)

where r is an immediate reward signal, γ a discount factor, and the policy πθ is
parametrized by parameters θ. Therefore, finding πθ∗ in Equation (3.2) is equivalent
to finding the corresponding optimal policy parameters θ ∗ .
For some problems, model-based RL methods have the promise of requiring fewer
interactions with the robot than model-free RL by learning a model of the transition
mapping in Equation (3.1), while efficiently generalizing to unforeseen situations using
a model learned from observed data [6].
49

The general idea of model-based RL is depicted in Figure 3.1. The learned model is
used for internal simulations, i.e., speculations about how the real robot would behave
if it followed the current policy. Based on these internal simulations, the quality of
the policy is evaluated using Equation (3.2) and improved accordingly. Subsequently,
the updated policy is again evaluated using Equation (3.2) and improved. This policy
evaluation/improvement loop terminates when the policy is learned, i.e., it no longer
changes and a (local) optimum is attained. Once a policy is learned, it is applied to the
robot and a new data set is recorded. Combined with previously collected data, the data
is used to update and refine the learned dynamics model. In theory, this loop continues
forever. Note that only the application of the policy requires interacting with the robot;
internal simulations and policy learning only require the learned computer model of the
robot dynamics.
While the idea of using models in the
Internal
context of robot learning is well-known since
Simulations
Aboaf’s work in the 1980s [2], it has been
limited by its strong dependency on the quality of the learned model, which becomes also
clear from Figure 3.1: The learned policy
Policy Learning
is inherently based on internal simulations Model Learning
using the learned model. When the model
exactly corresponds to the true dynamics of
the robot, sampling from the learned model
is equivalent to sampling from the real robot.
Apply Policy
However, in practice, the learned model
to Robot
is not exact, but only a more or less accurate
approximation to the real dynamics. For example, in regions where the training data is Figure 3.1: General loop in model-based
sparse, the quality of the learned model can RL: The learned model is used for internal
be insufficient as illustrated in Figure 3.2. simulations (mental rehearsal) based on
There are multiple plausible functions that which the policy is improved. The learned
could have generated the observed function policy is then applied to the robot. The
values (black circles). In regions with sparse data from this interaction is used to refine
training data, the models and their predic- the model, and the loop continues until
tions differ significantly. Any single model the model and the learned policy converge.
leads to overconfident predictions and, subsequently, it can lead to control strategies that are not robust to model errors. This
can have a drastic effect in robotics since it can mean to have negative masses or negative friction. These implausible effects are often exploited by the learning system since
they insert energy into the system for free, causing the system to believe in “perpetuum
mobiles”. Therefore, instead of selecting a single model (e.g., the maximum likelihood
model), we should describe our uncertainty about the latent underlying function f by
a probability distribution p(f ) to be robust to such model errors [61, 7, 18]. By taking
model uncertainty into account, the perpetuum mobile-effect is substantially less likely.

50

xt+1

2
0
−2
−5 −4 −3 −2 −1

0
1
(xt, ut)

2

3

4

5

Figure 3.2: Model errors. In this example, six function values have been observed (black
circles). Three functions are shown that could have generated these observations. Any
single function (e.g., the maximum likelihood function) produces more or less arbitrary
predictions in regions with sparse training data, one example location of which is shown
by the dashed line. Instead of selecting a single function approximator, we should describe our uncertainty about the underlying function by a probability distribution to be
robust to such model errors.
Since the learned policy inherently relies on the quality of the learned forward model,
which essentially serves as a simulator of the robot, model errors can not only cause
degraded policies, but they also often drastically bias the learning process. Hence, the
literature on model-based policy search largely focuses on model building, i.e., explaining
what kind of model is used for the forward dynamics and how it is trained.
Approaches to Dealing with Uncertain Models. Dealing with inaccurate dynamics models is one of the biggest challenges in model-based RL since small errors in the
model can lead to large errors in the policy [6]. Inaccuracies might stem from an overly
restrictive model class or from the lack of sufficiently rich data sets used for training the
models, which can lead to undermodeling the true forward dynamics. Moreover, system
noise typically adds another source of uncertainty.
Typically, a certainty-equivalence assumption is made1 , and the maximum likelihood
model is chosen for planning [61, 7]. However, this certainty-equivalence assumption is
violated in most interesting cases and can lead to large policy errors. Moreover, as
mentioned already in [2], many approaches obtain derivatives of the expected return by
backpropagating derivatives through learned forward models of the system. This step
is particularly prone to model errors since gradient-based optimizers improve the policy
parameters along their gradients. The learned policy needs to be robust to compensate
for model errors such that it results in good performance when applied to the real system.
Learning faithful dynamics models is crucial for building robust policies and remains one
of the biggest challenges in model-based policy search.
In [38], the authors model unknown error dynamics using receptive-field weighted re1

It is assumed that the optimal policy for the learned model corresponds to the optimal policy for
the true dynamics. Uncertainty about the learned model is neglected.

51

gression [59]. Explicit modeling of unknown disturbances leads to increased robustness
of the learned controllers. The idea of designing controllers in the face of inaccurate (idealized) forward models is closely related to robust control in classical robotics. Robust
control aims to achieve guaranteed performance or stability in the presence of (typically bounded) modeling errors. For example, H∞ loop-shaping [37] guarantees that
the system remains close to its expected behavior even if (bounded) disturbances enter
the system. In adaptive control, parameter uncertainties are usually described by unbounded probability distributions [5]. Model parameter uncertainty is typically not used
in designing adaptive control algorithms. Instead, the estimates of the parameters are
treated as the true ones [76]. An approach to designing adaptive controllers that do take
uncertainty about the model parameters into account is stochastic adaptive control [5].
When reducing parameter uncertainty by probing, stochastic adaptive control leads to
the principle of dual control [22]. Adaptive dual control has been investigated mainly
for linear systems [76]. An extension of dual adaptive control to the case of nonlinear
systems with affine controls was proposed in [21]. A minimum-variance control law is
obtained, and uncertainty about the model parameters is penalized to improve their
estimation, eliminating the need of prior system identification.
RL approaches that explicitly address the problem of inaccurate models in robotics
have only been introduced recently [61, 7, 38, 43, 45, 1, 29, 19, 18]. For instance, in [1],
the key idea is to use a real-life trial to evaluate a policy, but then use a crude model of the
system to estimate the derivative of the evaluation with respect to the policy parameters
(and suggest local improvements). In particular, the suggested algorithm iteratively up(i)
(i)
(i)
dates the model f according to f (i+1) (xt , ut ) = f (i) (xt , ut )+xt+1 −f (i) (xt , ut ), where
(i)

(i)

t indexes time. Here, xt , ut are taken from an observed trajectory. It follows that the
(i)
(i)
(i)
updated model f i+1 predicts the observed trajectory exactly, i.e., f i+1 (xt , ut ) = xt+1 .
The algorithm evaluates the policy gradients along the trajectory of states and controls
in the real system. In contrast, a typical model-based approach evaluates the derivatives along the trajectory predicted by the model, which does not correspond to the
trajectory of the real system when the model is inexact. Note that the approach in [1]
does not directly generalize to stochastic transition dynamics or systems with hidden
states. Moreover, an approximate parametric model of the underlying dynamics need to
be known in advance.
Major Challenges in Model-based Policy Search. There are three general challenges that need to be addressed in model-based policy search methods: what model
to learn, how to use the model for long-term predictions, and how to update the policy based on the long-term predictions. These three challenges correspond to the three
components in Figure 3.1 that do not require physical interaction with the robot: model
learning, internal simulations, and policy learning.
In Sec. 3.1, we give a brief overview of two useful models that are frequently used in
model-based policy search [7, 43, 45, 29, 19, 18], locally weighted (Bayesian) regression
(LWBR) and Gaussian processes (GPs) [55]. In Sec. 3.2, we discuss two general methods
of how to use these models for long-term predictions: stochastic inference, i.e., sampling,
52

Table 3.1: Model-based policy search approaches grouped by the learned model and the
method of generating trajectories. Due to the simplicity of sampling trajectories, most
policy search methods follow this approach. Deterministic trajectory predictions can
only be performed in special cases where closed-form approximate inference is possible.
Although they are mathematically more involved than trajectory sampling, they do not
suffer from large variances of the samples. Moreover, they can allow for analytic gradient
computations, which is crucial in the case of hundreds of policy parameters.
Trajectory Prediction
Learned Forward Model stochastic deterministic
(Locally) linear models [7, 43, 45]
—
Gaussian Processes
[29]
[18, 19]
and deterministic approximate inference. In Sec. 3.3, we briefly discuss multiple options
of updating the policy.
After having introduced these general concepts, in Sec. 3.4, we discuss model-based
policy search algorithms that combine the learned models and inference algorithms shown
in Table 3.1. We focus on the episodic case, wherein the start state distribution p(x0 )
is known. In particular, we detail four model-based policy search methods. First, we
start with PEGASUS, a general concept for efficient trajectory sampling in stochastic
MDPs for a given model [44]. Second, we present two ways of combining PEGASUS with
LWBR for learning robust controllers to fly helicopters [7, 43, 45]. Third, we present an
approach for using the PEGASUS algorithm for sampling trajectories from GP forward
models [29] in the context of learning a blimp controller. Finally, as a fourth approach,
we outline the ideas of the pilco policy search framework [18, 19] that combines efficient
deterministic approximate inference for long-term predictions with GP dynamics models
for learning to control mechanical systems and robot manipulators.

3.1

Probabilistic Forward Models

To reduce the effect of model errors, probabilistic models that express uncertainty about
the underlying transition dynamics are preferable to deterministic models that imply
a certainty-equivalence assumption, e.g., maximum-likelihood models of the transition
dynamics.
In the following, we briefly introduce two promising nonparametric probabilistic models that are frequently used for the forward dynamics in the context of reinforcement
learning and robotics: Locally weighted Bayesian regression (LWBR), [7, 43, 45] and
Gaussian processes [29, 18, 19].

53

3.1.1

Locally Weighted Bayesian Regression

Let us start with the linear regression model, where the transition dynamics are given
as
xt+1 = [xt , ut ]T ψ + w ,

w ∼ N (0, Σw ) .

(3.3)

Here, ψ are the parameters for the Bayesian linear regression model, and w ∼ N (0, Σw )
is i.i.d. Gaussian system noise. The model is linear in the unknown parameters ψ that
weight the input (xt , ut ).
In Bayesian linear regression, we place prior distribum, S
η, Ξ tions on the parameters ψ and on the noise variance Σw .
Typically, the prior distribution on ψ is Gaussian with
mean m and covariance S, and the prior on the diagonal entries 1/σi2 of Σ−1
w is a Gamma distribution with scale
and shape parameters η and ξ, respectively, such that the
ψ
Σw overall model is conjugate, see Figure 3.3, where we denote
the training inputs by [X, U ] and the training targets by
y, respectively. In the (Bayesian) linear regression model
Equation (3.3), it is fairly straightforward to find maximum likelihood estimates or posterior distributions of the
y
X, U
parameters ψ. However, the model itself is not very expressive since it assumes an overall linear relationship between
the inputs (xt , ut ) and the successor state xt+1 .
Figure 3.3:
Graphical
The idea of locally weighted linear regression (LWR) is
model for Bayesian lin- to exploit the good properties of the linear regression model
ear regression: A Gaus- but to allow for a more general class of functions: locally
sian prior with parame- linear functions. LWR finds a locally linear approximation
ters m, S is placed on the of the underlying function [15]. For this purpose, every test
model parameters ψ and input (xt , ut ) is equipped with a weighting factor bi that deGamma priors with pa- termines how close training point (xi , ui ) is to (xt , ut ). An
rameters η, Ξ are placed example for such a weight is the Gaussian-shaped weighton the diagonal entries of ing bi = exp(− (xi , ui ) − (xt , ut ) 2 /κ2 ). If the distance
the precision matrix Σ−1
w . between (xi , ui ) and (x∗ , u∗ ) is much larger than κ, the
Training inputs and tar- corresponding weight bi declines to 0. Since these weights
gets are denoted by [X, U ] have to be computed for each query point, it is insufficient
and y, respectively.
to store only the parameters ψ, but the entire training data
set [X, U ] is required, resulting in a nonparametric approach.
As in Bayesian linear regression, we can place priors on the parameters and the noise
covariance. For simplicity, let us assume a known noise covariance matrix Σw and a
zero-mean prior Gaussian distribution N ψ | 0, S on the parameters ψ. For each query
point (xt , ut ), a posterior distribution over the parameters ψ is computed according to

54

Bayes’ theorem as
p(ψ|X, U , y) =

p(ψ)p(y|X, U , ψ)
∝ p(ψ)p(y|X, U , ψ) .
p(y|X, U )

(3.4)

˜ := [X, U ]. The posterior mean and covariance
For notational convenience, we define X
of ψ at (xt , ut ) are given as
T

−1
˜ y] = S XB
˜ (B X
˜ S XB
˜ + Σw )−1 y = S XBΩ
˜
E[ψ|X,
y

(3.5)

=Ω−1
T

−1
˜ y] = S − S T XBΩ
˜
˜ S,
cov[ψ|X,
BX

bi = exp(− (xi , ui ) − (xt , ut ) 2 /κ2 ) ,

(3.6)
(3.7)

respectively, where B = diag(b1 , . . . , bn ), and y are the training targets.
Predictive Distribution. The mean and covariance of the predictive distribution
p(xt+1 ) for a given state-control pair (xt , ut ) are
−1
˜ y] = [xt , ut ]T S XBΩ
˜
µxt+1 = [xt , ut ]T E[ψ|X,
y,
x
T
˜ y][xt , ut ] + Σw ,
Σ
= [xt , ut ] cov[ψ|X,
t+1

(3.8)
(3.9)

respectively. In practice, the posterior mean and covariance over the parameters ψ can
be computed more efficiently by applying matrix inversion lemmas [15] and exploiting
sparsity.
Let us have a look at the predictive covariance when [xt , ut ] is far away from the
training set [X, U ]: The weight matrix B is almost zero, which leads to a posterior
variance over the model parameters ψ that corresponds to the prior uncertainty S, see
Equation (3.6). This means that the predictive variance at [xt , ut ] are non-zero, unlike
in non-Bayesian locally weighted regression. Hence, the deterministic LWR model is not
useful for exploration strategies that are based on model uncertainty.

3.1.2

Gaussian Process Regression

A Gaussian process is a distribution p(f ) over functions f . Formally, a GP is a collection
of random variables f1 , f2 , . . . any finite number of which is Gaussian distributed [55].
In the context of this section, a GP is placed over transition functions. Since the GP is a
nonparametric model, it suffices to specify high-level assumptions, such as differentiability or periodicity, on the underlying function. These high-level properties are typically
easier to specify than an explicit exact parametric model.
A GP is completely specified by a mean function m( · ) and a positive semidefinite
covariance function/kernel k( · , · ). Standard assumptions in GP models are a prior
mean function m ≡ 0 and the covariance function
2
˜ q )T Λ−1 (˜
˜ q ) +δpq σw
˜ q ) = σf2 exp − 21 (˜
k(˜
xp , x
xp − x
xp − x

55

(3.10)

˜ := [xT uT ]T . In Equation (3.10), we defined Λ := diag([ 21 , . . . , 2D ]), which
with x
depends on the characteristic length-scales i , and σf2 is the prior variance of the latent
˜ = [˜
˜ n ] and corresponding training targets
function f . Given n training inputs X
x1 , . . . , x
T
y = [y1 , . . . , yn ] , the posterior GP hyper-parameters (length-scales i , signal variance
2 ) are learned using evidence maximization [36, 55].
σf2 , and noise variance σw
Predictive Distribution. The posterior GP is a one-step prediction model, and the
predicted successor state xt+1 is Gaussian distributed
p(xt+1 |xt , ut ) = N xt+1 | µxt+1 , Σxt+1
µxt+1

= Ef [f (xt , ut )] ,

Σxt+1

= varf [f (xt , ut )] ,

(3.11)
(3.12)

where the mean and variance of the GP prediction are
µxt+1 = kT∗ K −1 y = kT∗ β ,

(3.13)

kT∗ K −1 k∗ ,

(3.14)

Σxt+1

= k∗∗ −

˜ x
˜ t ), k∗∗ := k(˜
˜ t ), β := K −1 y, and where K is the
respectively, with k∗ := k(X,
xt , x
˜ j ).
kernel matrix with entries Kij = k(˜
xi , x
Note that far away from the training data, the predictive uncertainty in Equation (3.14) corresponds to the prior uncertainty about the function, i.e., even for deterministic systems where Σw = 0, we obtain k∗∗ > 0. Therefore, the GP is a nonparametric, non-degenerate model.

3.2

Long-Term Predictions with a Given Model

In the following, we assume that a model for the transition dynamics is known. Conditioned on this model, we distinguish between two approaches for generating long-term
predictions: approaches based on Monte-Carlo sampling or deterministic approximate
inference.

3.2.1

Sampling-based Trajectory Prediction: PEGASUS

PEGASUS (Policy Evaluation-of-Goodness And Search Using Scenarios) is a conceptual
framework for trajectory sampling in stochastic MDPs [44]. The key idea is to transform
the stochastic MDP into an augmented deterministic MDP. For this purpose, PEGASUS
assumes access to a simulator with no internal random number generator. When sampling from this model, PEGASUS provides the random numbers externally in advance.
In this way, PEGASUS reduces the sampling variance drastically. Therefore, sampling
following the PEGASUS approach is also commonly used in model-free policy search,
see Chapter 2.

56

Algorithm 15 PEGASUS algorithm for sampling trajectories
Init: g(x, u, w), reward r, random numbers w0 , w1 , . . . , initial state distribution
p(x0 ), policy πθ
for i = 1, . . . , m do
(i)
x0 ∼ p(x0 )
Sample “scenario” from initial state distribution
for t = 0, . . . , T − 1 do
(i)
(i)
xt+1 = g(xt , πθ (xt ), wt )
Succ. state in augmented MDP
end for
end for
(i)
m
T
1
t
J˜πθ = m
Estimate expected long-term reward
i=1
t=1 γ r(xt )
Trajectory Sampling and Policy Evaluation
Assume that a forward model of the system at hand is given that can be used for sampling
trajectories. If the state transitions are stochastic, computing the expected long-term
reward
T

J

πθ

γ t E[r(xt )|πθ ] ,

=

x0 ∼ p(x0 ) ,

t=0

γ ∈ [0, 1] ,

(3.15)

will require many sample trajectories for computing the approximation J˜ to J, where
1
J˜πθ =
m

m
(i)

J πθ (x0 )

(3.16)

i=1

(i)

with samples x0 from p(x0 ). Computing reliable policy gradients will require even more
samples for robust derivatives. However, as the limit of an infinite number of samples,
we obtain limm→∞ J˜πθ = J πθ .
For more efficient computations, PEGASUS augments the state x by an externally
given sequence of random values w0 , w1 , . . . . To draw a sample from p(xt+1 |xt , ut ),
PEGASUS uses a-priori given noise values wt to compute the sampled state xt+1 , such
that xt+1 = g(xt , ut , st ). Since the sequence of random numbers is fixed, repeating the
same experiment results in the identical sample trajectory.
PEGASUS can be described as generating m Monte Carlo trajectories and taking
their average reward, but the randomization is determined in advance. It can be shown
that solving the augmented deterministic MDP is equivalent to solving the original
stochastic MDP [44]. The PEGASUS algorithm is summarized in Algorithm 15.
Practical Considerations
While sampling using the PEGASUS algorithm can be performed relatively efficiently,
robust low-variance estimates of the expected long-term cost require a lot of samples.
Therefore, policy search methods based on trajectory sampling are practically limited
57

by a relatively small number of a few tens of policy parameters they can manage [42].2
Although the policy gradients can also be computed (e.g., with finite difference approximations), the sample-based nature of PEGASUS leads to derivatives with high variance,
which renders them largely useless. Thus, policy updates in the context of PEGASUS do
usually not rely on gradients [45, 7, 29]. Of course all model-free approaches from Chapter 2 for estimating the policy gradients can be used in conjunction with the PEGASUS
idea of sampling trajectories from a given model.
An alternative to sampling trajectories are deterministic approximate inference methods for predicting trajectories, such as linearization [4], moment matching, or the unscented transformation [27].

3.2.2

Deterministic Long-Term Predictions

Instead of performing stochastic sampling, a probability distribution p(τ ) over trajectories τ = (x0 , . . . , xT ) can also be computed using deterministic approximations, such
as linearization [4], sigma-point methods (e.g., the unscented transformation [27]), or
moment matching. These common inference methods approximate unwieldy predictive
distributions by Gaussians.
xu
,
Assuming a joint Gaussian probability distribution p(xt , ut ) = N [xt , ut ] | µxu
t , Σt
the problem of computing the successor state distribution p(xt+1 ) corresponds to solving
the integral
p(xt+1 ) =

p(xt+1 |xt , ut )p(xt , ut ) dxt dut dw ,

(3.17)

where xt+1 = f (xt , ut ) + w. If the transition function f is nonlinear, p(xt+1 ) is
non-Gaussian and we have to resort to approximate inference techniques. A convenient approximation of the unwieldy predictive distribution p(xt+1 ) is the Gaussian
N xt+1 | µxt+1 , Σxt+1 . The mean µxt+1 and covariance Σxt+1 of this predictive distribution can be computed in various ways. In the following, we outline three commonly used
approaches: linearization, the unscented transformation, and moment matching.
Linearization. One way of computing µxt+1 and Σxt+1 is to linearize the transition
function f ≈ F locally around (µxt , µut ) and, subsequently, estimate the predictive covariance by mapping the Gaussian input distribution through the linearized system. With
linearization, we obtain the predictive mean and covariance given by µxt+1 = f (µxu
t ) and
x
xu T
Σt+1 = F Σt F + Σw , respectively. Figure 3.4 illustrates the idea of linearization.
Linearization is conceptually straightforward and computationally efficient. Note
that this approach leaves the Gaussian input distribution p(xt , ut ) untouched but approximates the transition function f . A potential disadvantage is that the transition
function f needs to be differentiable to perform the linearization.3 Moreover, lineariza2

“Typically, PEGASUS policy search algorithms have been using [...] maybe on the order of ten
parameters or tens of parameters; so, 30, 40 parameters, but not thousands of parameters [...].” [42]
3
Differentiability assumptions can be problematic in robotics. For instance, contacts in locomotion
and manipulation can render this assumption invalid.

58

xt+1

xt+1
p(xt,ut)

−1

−0.5

0

0.5

1

−0.5

0
(xt,ut)

0.5

1

0

1

p(xt+1)

2

1

0
−1

Figure 3.4: Computing an approximate predicted distribution using linearization. A
Gaussian distribution p(xt , ut ) (lower-left panel) needs to be mapped through a nonlinear
function (black, upper-left panel). The true predictive distribution is represented by the
shaded area in the right panel. To obtain a Gaussian approximation of the unwieldy
shaded distribution, the nonlinear function is linearized (red line, upper-left panel) at
the mean of the input distribution. Subsequently, the Gaussian is mapped through this
linear approximation and yields the blue Gaussian approximate predictive distribution
p(xt+1 ) shown in the right panel.
tion can easily underestimate predictive variances, which can cause policies to be too
aggressive, causing damage on real robot systems.
Unscented Transformation. The key idea behind the unscented transformation [27]
is to represent the distribution p(xt , ut ) by a set of deterministically chosen sigma points
(i)
(i)
(Xt , Ut ). For these sigma points, the corresponding exact function values are computed. The mean µxt+1 and covariance Σxt+1 of the predictive distribution p(xt+1 ) are
computed from the weighted mapped sigma points. In particular, we obtain
2d
(i)

µxt+1 =
i=0
2d

(i)

Σxt+1 =
i=0

(i)

(i)
wm
f (Xt , Ut ) ,

(3.18)

(i)

(i)

(i)

wc(i) (f (Xt , Ut ) − µxt+1 )(f (Xt , Ut ) − µxt+1 )T ,
(i)

(i)

(3.19)

respectively, where d is the dimensionality of (x, u), (Xt , Ut ) are sigma points, i.e.,
(i)
(i)
deterministically chosen samples from the joint distribution p(xt , ut ), and wm and wc
are weights. In Equation (3.18)–(3.19), we assumed that the dimension of the statecontrol pair (x, u) is d. For further details on the unscented transformation, we refer
to [27, 69]. Figure 3.5 illustrates the idea of the unscented transformation.
59

xt+1

xt+1
p(xt,ut)

−1

−0.5

0

0.5

1

−0.5

0
(xt,ut)

0.5

1

0

1

p(xt+1)

2

1

0
−1

Figure 3.5: Computing an approximate predicted distribution using the unscented transformation. A Gaussian distribution p(xt , ut ) (lower-left panel) needs to be mapped
through a nonlinear function (black, upper-left panel). The true predictive distribution
is represented by the shaded area in the right panel. To obtain a Gaussian approximation of the unwieldy shaded distribution, the input distribution is represented by three
sigma points (red dots in lower-left panel). Subsequently, the sigma points are mapped
through the nonlinear function (upper-left panel) and their sample mean and covariance
yield the blue Gaussian approximate predictive distribution p(xt+1 ) shown in the right
panel.
Note that the unscented transformation approximates the Gaussian distribution
p(xt , ut ) by sigma points, which are subsequently mapped through the original transition function f . The unscented transformation does not require differentiability and
is expected to yield more accurate approximations of the predictive distribution p(xt+1 )
than an explicit linearization [77].
Moment Matching. The idea of moment matching is to compute the predictive mean
and covariance of p(xt+1 ) exactly and approximate p(xt+1 ) by a Gaussian that possesses
the exact mean and covariance. Here, neither the joint distribution p(xt , ut ) nor the transition function f are approximated. The moment-matching approximation is the best
unimodal approximation of the predictive distribution in the sense that it minimizes the
Kullback-Leibler divergence between the true predictive distribution and the unimodal
approximation. Figure 3.6 illustrates the idea of moment matching.
Practical Considerations
The exact moments can be computed only in special cases since the required integrals
for computing the predictive mean and covariance might be intractable. Moreover, an
exact moment-matching approximation is typically computationally more expensive than
approximations by means of linearization or sigma points.
60

xt+1

xt+1
p(xt,ut)

−1

−0.5

0

0.5

1

−0.5

0
(xt,ut)

0.5

1

0

1

p(xt+1)

2

1

0
−1

Figure 3.6: Computing an approximate predicted distribution using moment matching.
A Gaussian distribution p(xt , ut ) (lower-left panel) needs to be mapped through a nonlinear function (black, upper-left panel). The true predictive distribution is represented
by the shaded area in the right panel. To obtain a Gaussian approximation of the
unwieldy shaded distribution, the mean and covariance of the shaded distribution are
computed analytically. These first and second-order moments fully determine the blue
Gaussian approximate predictive distribution p(xt+1 ) shown in the right panel. The
contour lines in the upper-left panel represent the joint distribution between inputs and
prediction.
Unlike sampling-based approaches such as PEGASUS, deterministic approximate
inference methods for long-term planning can be used to learn several thousands of
policy parameters [18]. We will see examples in Sec. 3.4.2. The reason why deterministic
long-term predictions can learn policies with many parameters is that gradients can
be computed analytically. Therefore, these gradient estimates do not suffer from high
variances, a typical problem with sampling-based estimation. Nevertheless, deterministic
inference often requires more implementation effort than sampling approaches.

3.3

Policy Updates

Having introduced two major model classes and two general ways of performing longterm predictions with these models, in the following, we will discuss ways of updating
the policy. We distinguish between gradient-free and gradient-based policy updates.

3.3.1

Model-based Policy Updates without Gradient Information

Gradient-free methods are probably the easiest way of updating the policy since they
do not require the computation or estimation of policy gradients. By definition they
also have no differentiability constraints on the policy or the transition model. Standard

61

gradient-free optimization method are the Nelder-Mead method [40], a heuristic simplex
method, or hillclimbing, a local search method that is closely related to simulated annealing [58]. Due to their simplicity and the small required computational effort, they
are commonly used in the context of model-based policy search [7, 29, 43, 45], especially
in combination with sampling-based trajectory generation.
A clear disadvantage of gradient-free optimization is their relatively slow convergence rate. For faster convergence, we can use gradient-based policy updates, which are
introduced in the following sections.

3.3.2

Model-based Policy Updates with Gradient Information

Gradient-based policy updates are expected to yield faster convergence than gradientfree updates. We distinguish between two cases: a sample-based estimation of the policy
gradients and an analytic computation of the policy gradients dJ πθ (θ)/ dθ.
Sampling-based Policy Gradients
When we use sample trajectories τ i from the learned model to estimate the expected
long-term reward J πθ in Equation (3.2), we can numerically approximate the gradient
dJ πθ / dθ.
The easiest way of estimating gradients is to use finite difference methods. However,
finite difference methods require O(F ) many evaluations of the expected long-term reward J πθ , where F is the number of policy parameters θ. Since each of these evaluations
is based on the average of m sample rollouts, the required number of sample trajectories quickly becomes excessive. In the model-based set-up, this is just a computational
problem but not a problem of wearing the robot out since the samples are generated
from the model and not the robot itself.
There are several ways of making model-based gradient estimation more efficient:
First, for a more robust estimate of J πθ , i.e., an estimate with smaller variance, the
PEGASUS approach [44] can be used. Second, for more efficient gradient estimation
any of the model-free methods presented in Chapter 2 for gradient estimation can be
used in the model-based context. The only difference is that instead of the robot, the
learned model is used to generate trajectories. To the best of our knowledge, there are
currently not many approaches for model-based policy search based on sampling-based
gradients using the methods from Chapter 2.
Analytic Policy Gradients
Computing the gradients dJ πθ / dθ analytically requires the policy, the (expected) reward
function, and the learned transition model to be differentiable. Despite this constraint,
analytic gradient computations are a viable alternative to sampling-based gradients for
two major reasons: First, they do not suffer from the sampling variance, which is especially pronounced when computing gradients. Second, the computational effort scales
very favorably with the number of policy parameters, allowing for learning policies with

62

thousands of parameters. However, due to the repeated application of the chain-rule,
the computation of the gradient itself is often mathematically more involved than a
sampling-based estimate.
Let us consider an example where the immediate reward r only depends on the
state (generalizations to control-dependent rewards are straightforward) and the system
dynamics are deterministic, such that xt+1 = f (xt , ut ) = f (xt , πθ (xt , θ)), where f is
a (nonlinear) transition function, πθ is the (deterministic) policy, and θ are the policy
parameters. The gradient of the long-term reward J πθ = t γ t r(xt ) with respect to the
policy parameters is obtained by applying the chain-rule repeatedly:
dJ πθ
=
dθ

γt
t

γt

=
t

dr(xt )
=
dθ
∂r(xt )
∂xt

γt
t

∂r(xt ) dxt
∂xt dθ

∂xt dxt ∂xt dut
+
∂xt dθ
∂ut dθ

(3.20)
.

(3.21)

From these equations we observe that the total derivative dxt / dθ depends on the total
derivative dxt / dθ at the previous time step. Therefore, the derivative dJ πθ / dθ can be
computed iteratively.
Extension to Probabilistic Models and Stochastic MDPs. For the extension
to derivatives in stochastic MDPs and/or probabilistic models, we have to make a few
adaptations to the gradients in Equation (3.20)–(3.21): When the state xt is represented by a probability distribution p(xt ), we have to compute the expected reward
E[r(xt )] = r(xt )p(xt ) dxt . Moreover, we need to compute the derivatives with respect to the parameters of the state distribution, assuming that p(xt ) has a parametric
representation.
For example, if p(xt ) = N xt | µxt , Σxt , we compute the derivatives of E[r(xt )] with
respect to the mean µxt and covariance Σxt of the state distribution and continue applying
the chain-rule similarly to Equation (3.20)–(3.21): With the definition Et := Ext [r(xt )],
we obtain the gradient
dJ πθ
dEt
=
γt
,
t
dθ
dθ
dEt
∂Et dp(xt )
∂Et dµxt
∂Et dΣxt
:=
=
+
,
dθ
∂p(xt ) dθ
∂µxt dθ
∂Σxt dθ

(3.22)

where we used the shorthand notation ∂Et /∂p(xt ) = {∂Et /∂µxt , ∂Et /∂Σxt } for taking the
(total) derivative of Et with respect to the parameters of p(xt ) = N xt | µxt , Σxt , i.e., the
mean and covariance. The mean µxt and the covariance Σxt are functionally dependent
on the moments µxt−1 and Σxt−1 of p(xt−1 ) and the controller parameters θ. By applying

63

the chain-rule to Equation (3.22), we obtain
dµxt
∂µxt dµxt−1
∂µxt dΣxt−1 ∂µxt
=
+
+
,
dθ
∂µxt−1 dθ
∂Σxt−1 dθ
∂θ
dΣxt
∂Σxt dµxt−1
∂Σxt dΣxt−1 ∂Σxt
=
+
+
.
dθ
∂µxt−1 dθ
∂Σxt−1 dθ
∂θ

(3.23)
(3.24)

Note that the total derivatives dµxt−1 / dθ and dΣxt−1 / dθ are known from time step t − 1.
If all these computations can be performed in closed form, the policy gradients
dJ˜πθ / dθ can be computed analytically by repeated application of the chain-rule without
the need of sampling. Therefore, standard optimization techniques (e.g., BFGS or CG)
can be used to learn policies with thousands of parameters [18].

3.3.3

Discussion

Using the gradients of the expected long-term reward J πθ with respect to the policy
parameters θ often leads to faster learning than gradient-free policy updates. Moreover,
gradient-free methods are typically limited to a few tens of policy parameters [42]. Computing the gradients can be unwieldy and requires additional computational resources.
When computing gradients, exact analytic gradients are preferable over sampling-based
gradients since the latter ones often suffer from large variance. These variances can
even lead to slower convergence than gradient-free policy updates [7, 29]. For analytic
gradients, we impose assumptions on the differentiability of the reward function r and
the transition function f .4 Moreover, for analytic gradients, we rely on deterministic approximate inference methods, e.g., moment matching or linearization, such that only an
approximation J˜πθ to J πθ can be computed; but with the exact gradients dJ˜πθ / dθ.
For updating the policy we recommend using gradient information to exploit better
convergence properties. Ideally, the gradients are determined analytically without any
approximations. Since this aim can only be achieved for linear systems, we have to resort
to approximations, either by using sampling-based approaches or analytic approximate
gradients. Sampling-based approaches are practically limited to fairly low-dimensional
policy parameters θ ∈ Rk , k ≤ 50. For high-dimensional policy parameters with k > 50,
we recommend using analytic policy gradients if they are available.

3.4

Model-based Policy Search Algorithms with Robot Applications

In this section, we briefly describe policy search methods that have been successfully
applied to learning policies for robots. We distinguish between approaches that evaluate the expected long-term reward J πθ using either sampling methods as described in
Sec. 3.2.1 or deterministic approximate inference methods as described in Section 3.2.2.
4

In stochastic MDPs, this assumption is usually valid.

64

3.4.1

Sampling-based Trajectory Prediction

Sampling directly from the learned simulator has been dealt with by a series of researchers for maneuvering helicopters [7, 43, 45] and for controlling blimps [29]. All
approaches use the PEGASUS algorithm [44] to generate trajectories from the learned
stochastic models.
Ng et al. [45, 43] learn models for hovering a helicopter based on locally weighted linear regression. To account for noise and model inaccuracies, this originally deterministic
model was made stochastic by adding i.i.d. Gaussian (system) noise to the transition
dynamics.
Unlike [45, 43], Bagnell et al. [7] explicitly describe uncertainty about the learned
model by means of a posterior distribution over a finite set of locally affine models.
Trajectories are sampled from this mixture of models for learning the policy
Ko et al. [29] combine idealized parametric models with nonparametric Gaussian processes for modeling the dynamics of an autonomous blimp. The GPs are used to model
the discrepancy between the nonlinear parametric blimp model and the data. Trajectories are sampled from this hybrid model when learning the policy. In the following, we
discuss these policy search algorithms.
Locally Weighted Regression Forward Models and Sampling-based Trajectory Prediction
In [7], locally weighted Bayesian regression was used for learning forward models for
hovering an autonomous helicopter. To account for model uncertainty, a posterior probability distribution over the model parameters ψ and, hence, the model itself was considered instead of a point estimate of the model parameters. Trajectories were sampled
from this mixture of models for learning the policy.
Trajectories τ i were generated using the PEGASUS approach [44]. At each time step,
a model parameter set ψ i was sampled from the posterior distribution p(ψ|X, U , y, xt , ut ).
After every transition, the dynamics model was updated with the observed (simulated)
transition. After each generated trajectory τ i , the model was reset by deleting the
simulated trajectory τ i from the model [7]. For Bayes-optimal model estimators, this
procedure is equivalent to sampling the model and sampling a full trajectory from it.
Algorithm 16 summarizes how to sample trajectories from the learned model while incorporating the posterior uncertainty about the model itself. The model uncertainty
is implicitly integrated out by averaging over the expected long-term rewards for all
generated trajectories τ i .
In [7], a gradient-free optimization, the Nelder-Mead method, was used to update
the policy parameters θ, which outperformed naive gradient-based optimization. The
resulting approach learned a neural-network controller with 10 parameters to hover a
helicopter about a fixed point [7], see Figure 3.7(a). Extrapolation outside the range
of collected data was discouraged by large penalties on the corresponding states. The
learned controller that was based on the probability distribution over models, was substantially less oscillatory than a controller learned by using the maximum likelihood
65

Algorithm 16 Policy evaluation and T -step predictions [7]
1: Input:
transition model f , posterior distribution over model parameters
p(ψ|X, U , y), policy parameters θ
2: for i = 1, . . . , m do
3:
for t = 0, . . . , T − 1 do
Sample trajectory τ i
4:
Sample local model parameters ψ i ∼ p(ψ|X, U , y, xt , ut )
5:
Compute control ut = πθ (xt )
6:
Generate a sample state transition xt+1 ∼ p(xt+1 |xt , ut , ψ i )
7:
Update X, U with simulated transition (xt , ut , xt+1 )
8:
end for
9:
Compute Jiπθ
10:
Reset the learned model to the original model f
11: end for
πθ
m
1
12: J˜πθ = m
i=1 Ji

With permission by J.A. Bagnell

With permission by A. Ng

(a) Helicopter hovering [7].

(b) Inverted helicopter hovering [45].

Figure 3.7: Model-based policy search methods with stochastic inference were used for
learning to hover helicopters.
model, i.e., a point estimate of the model parameters.
Ng et al. [45, 43] learn models for helicopter hovering based on locally-weighted
linear regression, see Figure 3.7(b). Unlike in [7], a point estimate of the parameters
ψ in Equation (3.3) was determined, for instance by maximum likelihood or maximuma-posteriori estimation. To account for noise and model inaccuracies, this originally
deterministic model was made stochastic by adding i.i.d. Gaussian (system) noise to
the transition dynamics. Angular velocities expressed in helicopter coordinates were
integrated and subsequently transformed into angles in world coordinates, which made
the model necessarily nonlinear. With this approach, models for helicopter hovering in
a standard [45] or inverse [43] pose were determined using data collected from human
pilots’ trajectories.
For learning a controller with these stochastic nonlinear transition dynamics, the
PEGASUS [44] sampling method was used to sample trajectories from the model. With
these sampled trajectories, a Monte-Carlo estimate of the expected long-term reward
was computed. A greedy hill-climbing method was used to learn the parameters θ of
66

With permission by D. Fox

Figure 3.8: Combination of a parametric prior and GPs for modeling and learning to
control an autonomous blimp [29].
the policy πθ , which was represented by a simplified neural network [43].
In the case of inverted helicopter hovering, a human pilot flipped the helicopter
upside down. Then, the learned controller took over and stabilized the helicopter in the
inverted position [43], an example of which is shown in Figure 3.7(b).
Gaussian Process Forward Models and Sampling-based Trajectory Prediction
In [29], GP forward models were learned to model the yaw-dynamics of an autonomous
blimp, see Figure 3.8. The GP models were combined with an idealized parametric model
of the blimp’s dynamics, i.e., the GP modeled the discrepancy between the parametric
nonlinear model and the observed data. The model was trained on blimp trajectory
data generated by a human flying the blimp using a remote control.
The PEGASUS approach [44] was used to sample long-term trajectories. Each new
sample was incorporated into the model by updating the kernel matrix. The controller
was learned using the gradient-free Nelder-Mead [40] optimization method. The four
controller parameters θ were the drag coefficient, the right/left motor gains, and the
slope of a policy-smoothing function. The learned controller was an open-loop controller,
i.e., the controls were pre-computed offline and, subsequently, applied to the blimp. The
controller based on the learned GP dynamics outperformed the optimal controller solely
based on the underlying idealized parametric blimp dynamics model [29].

3.4.2

Deterministic Trajectory Predictions

In the following, we summarize policy search methods that perform deterministic trajectory predictions for policy evaluation.
Gaussian Process Forward Models and Deterministic Trajectory Prediction
The pilco (probabilistic inference for learning control) policy search framework [18, 19]
uses a GP forward model of the robot’s dynamics to consistently account for model errors.
67

Algorithm 17 pilco policy search framework [18, 19]
Init: Sample controller parameters θ ∼ N (0, I). Apply random control signals and
record data.
repeat
Learn probabilistic (GP) dynamics model using all data.
repeat
Compute p(x1 ), . . . , p(xT ) using moment matching and J˜πθ
Analytically compute policy gradients dJ˜πθ / dθ
Update parameters θ (linesearch in BFGS or CG).
until convergence; return θ ∗
Apply πθ ∗ to system (single trial/episode) and record data.
until task learned
In combination with deterministic inference by means of moment matching for predicting
state trajectories p(τ ) = [p(x1 ), . . . , p(xT )] pilco computes an analytic approximation J˜
˜ dθ
to the expected long-term reward J πθ in Equation (3.2). Moreover, the gradients dJ/
of the expected long-term reward with respect to the policy parameters are computed
analytically. For policy learning, standard optimizers (e.g., BFGS or CG) can be used.
The pilco algorithm is outlined in Algorithm 17. The algorithm is typically initialized
uninformatively, i.e., the policy parameters are sampled randomly, and data is recorded
from a short state trajectory generated by applying random actions. In the following, we
briefly outline details about computing the long-term predictions, the policy gradients,
and application of the pilco framework to control and robotic systems.
Long-Term Predictions. For predicting a distribution p(τ |πθ ) over trajectories for
a given policy, pilco iteratively computes the state distributions p(x1 ), . . . , p(xT ). For
these predictions, the posterior uncertainty about the learned GP forward model is explicitly integrated out. Figure 3.9 illustrates this scenario: Let us assume, a joint Gaussian distribution p(xt , ut ) is given. For predicting the distribution p(xt+1 ) of the next
state, the joint distribution p(xt , ut ) in the lower-left panel has to be mapped through
the posterior GP distribution on the latent transition function, shown in the upper-left
panel. Exact inference is intractable due to the nonlinear covariance function. Extensive
Monte-Carlo sampling yields a close approximation to the predictive distribution, which
is represented by the shaded bimodal distribution in the right panel. Pilco computes
the mean and the variance of this shaded distribution exactly and approximates the
shaded distribution by a Gaussian with the correct mean and variance as shown by the
blue distribution in the upper-right panel [18, 19].
Analytic Policy Gradients. The predicted states are not point estimates but represented by Gaussian probability distributions p(xt ), t = 1, . . . , T . When computing
the policy gradients dJ πθ /θ, pilco explicitly accounts for the probabilistic formulation by analytically computing the policy gradients for probabilistic models presented

68

xt+1

xt+1
p(xt, ut)

−1

−0.5

0

0.5

1

−0.5

0
(xt, ut)

0.5

1

0

0.5

1
p(xt+1)

1.5

1

0
−1

Figure 3.9: Approximate predictions with Gaussian processes at uncertain inputs: In
order to determine the predictive distribution p(xt+1 ), it is required to map the input
distribution p(xt , ut ) (lower-left panel) through the posterior GP distribution (upper-left
panel) while explicitly averaging out the model uncertainty (shaded area). Exhaustive
Monte-Carlo sampling yields the exact distribution, represented by the red shaded distribution (right panel). The deterministic moment-matching approximation computes
the mean and variance of the exact predictive distribution and fits a Gaussian (blue,
right panel) with the exact first two moments to it. The contour lines in the upper-left
panel represent the joint distribution between inputs and prediction.
in Chapter 3.3.2.
Due to the explicit incorporation of model uncertainty into long-term predictions
and gradient computation, pilco typically does not suffer severely from model errors.
Robot Applications. As shown in Figure 3.11, the pilco algorithm achieved an unprecedented speed of learning on a standard benchmark task, the under-actuated cartpole swing-up and balancing task, see Figure 3.10. In particular, the cart-pole swing-up
was learned requiring an order of magnitude less interaction time with the robot than
any other RL method that also learns from scratch, i.e., without an informative initialization by demonstrations for instance. For the cart-pole swing-up problem, the learned
nonlinear policy was a radial-basis function network with 50 axes-aligned Gaussian basis
functions. The policy parameters θ were the weights, the locations, and the widths of
the basis functions resulting in 305 policy parameters.
Pilco was also successfully applied to efficiently learning controllers from scratch in
a block-stacking task with a low-cost five degrees of freedom robot arm [19], see also
Figure 3.10. The state x of the system was defined as the 3D coordinates of the block in
the end-effector of the manipulator. For tracking these coordinates, an RGB-D camera
was used. The learned policy πθ was an affine function of the state, i.e., u = Ax + b.
Exactly following Algorithm 17, pilco learned to stack a tower of six blocks in less

69

Required interaction time in s

Figure 3.10: Pilco learning successes. Left: Autonomous learning to stack a block of
blocks using an off-the-shelf low-cost manipulator [19]. Right: Autonomous learning to
swing up and balance a freely swinging pendulum attached to a cart [18].
C: Coulom 2002
KK: Kimura & Kobayashi 1999
D: Doya 2000
WP: Wawrzynski & Pacut 2004
RT: Raiko & Tornio 2009
pilco: Deisenroth & Rasmussen 2011

5

10

4

10

3

10

125 s

2

10

17.5 s
1

10

C

KK

D

WP

RT

pilco

Figure 3.11: Pilco achieves an unprecedented speed of learning on the cart-pole swingup task. The horizontal axis gives references to RL approaches that solve the same task,
the vertical axis shows the required interaction time in seconds on a logarithmic scale.
than 20 trials. State-space constraints for obstacle avoidance were straightforwardly
incorporated into the learning process as well [19].

3.4.3

Overview of Model-based Policy Search Algorithms

Table 3.2 summarizes the model-based policy search approaches that were presented in
this chapter. Each algorithm is listed according to their prediction method (sampling/
deterministic), their learned forward model (LWR/LWBR/GP), the policy updates (gradient free/gradient based), and the corresponding robotic applications.
Note that in [43, 45], model errors and local minima are dealt with by injecting
additional noise to the system to reduce the danger of overfitting. Generally, noise in
the system (either by artificially injecting it [43, 45] or by using probabilistic models [7,
29, 18, 19]) also smoothens out the objective function and, hence, local minima.

70

Table 3.2: Overview of model-based policy search algorithms with robotic applications.
Algorithm
[7]
[43, 45]
[29]
[18, 19]

3.5

Predictions
sampling (PEGASUS)
sampling (PEGASUS)
sampling (PEGASUS)
moment matching

Forward Model
LWBR
LWR+noise
GP
GP

Policy Update
gradient free
gradient free
gradient free
gradient based

Application
helicopter hovering
helicopter hovering
blimp control
manipulator, cart pole

Discussion

In the following, we briefly discuss three important topics related to model-based policy
search. In particular, we first discuss advantages and disadvantages of stochastic inference versus deterministic inference. Then, we discuss how uncertainty about the learned
model itself is treated in the literature. Finally, we shed some light on the requirements
when a policy is learned from scratch, i.e., learning must happen without a good initialization. The latter point is important if neither informed knowledge about the dynamics
nor “good” data sets from demonstrations are available. Instead, the robot has to learn
starting from, potentially sparse, data and uninformed prior knowledge.

3.5.1

Deterministic and Stochastic Long-Term Predictions

We discussed two general model-based approaches for computing distributions over trajectories and the corresponding long-term reward: Monte-Carlo sampling using the PEGASUS trick and deterministic predictions using linearization, unscented transformation, or moment matching. The advantage of stochastic sampling is that the sampler
will return a correct estimate of the expected long-term reward J πθ in the limit of an
infinite number of sampled trajectories. Exhaustive sampling can be computationally
inefficient, but it can be straightforwardly parallelized. A more significant issue with
sampling, even when using the PEGASUS approach [44], is that it is only practical for
several tens of policy parameters.
As an alternative to stochastic sampling, deterministic predictions only compute
an exact trajectory distribution for linear-Gaussian systems. Therefore, in nonlinear
systems, only an approximation to the expected long-term reward is returned. The
computations required for computing predictive distributions are non-trivial and can
be computationally expensive. Unlike stochastic sampling, deterministic predictions are
not straightforwardly parallelizable. On the other hand, deterministic predictions have
several advantages that can outweigh its disadvantages: First, despite the fact that deterministic predictions are computationally more expensive than generating a single sample
transition, the requirement of many samples quickly gets computationally even more expensive. A striking advantage of deterministic predictions is that gradients with respect
to the policy parameters can be computed analytically. Therefore, policy search with
deterministic prediction methods can learn policies with thousands of parameters [18].
Table 3.3 summarizes the properties of deterministic and stochastic trajectory predictions. The table lists whether the expected long-term reward J πθ and the corresponding
gradients dJ πθ / dθ can be evaluated exactly or only approximately. For stochastic trajec-

71

Table 3.3: Properties of deterministic and stochastic trajectory predictions in modelbased policy search.
Stochastic
Deterministic
J πθ
exact in the limit approximate
dJ πθ / dθ
exact in the limit
exact
Computations
simple
involved
# Policy parameters
1 ≤ |θ| ≤ 50
1 ≤ |θ| ≤ ?
tory predictions, i.e., sampling, the required computations are relatively simple whereas
the computations for deterministic predictions are mathematically more involved. Finally, we give practicable bounds on the number of policy parameters that can be learned
using either of the prediction methods. For stochastic trajectory generation, J πθ can be
evaluated exactly in the limit of infinitely many sample trajectories. The corresponding
policy gradients converge even slower. In practice, where only a finite number of samples
is available both J πθ and dJ πθ / dθ cannot be evaluated exactly.

3.5.2

Treatment of Model Uncertainty

Expressing uncertainty about the learned model is important for model-based policy
search to be robust to model errors. When predicting or generating trajectories, there
are two general ways of treating model uncertainty.
In [61, 18, 19], model uncertainty is treated as temporally uncorrelated noise, i.e.,
model errors at each time step are considered independent. This approach is computationally relatively cheap and allows for the consideration of an infinite number of models
during model averaging [18, 19].
Sampling the model parameters initially and fixing the model parameters for the
generated trajectory has the advantage that temporal correlation is automatically accounted for when the partially sampled trajectories are treated as training data until the
model parameters are resampled [7]. Here, temporal correlation means that the state at
one time step along a trajectory is correlated with the state at the previous time step.
On the other hand, only a finite number of models can be sampled.

3.5.3

Extrapolation Properties of Models

In a classical RL set-up, we used to seek a policy without too specific prior information.
Key to successful learning is the exploration strategy of the learner to discover rewarding
states and trajectories. In a robotics context, arbitrary exploration is not desired if not
discouraged since the robot can easily be damaged. Therefore, the classical RL paradigm
in a robotics context is not really applicable since exploration needs to take hardware
constraints into account. Two ways of implementing cautious exploration are to either
avoid significant changes in the policy [48] or to explicitly discourage entering undesired
regions in the state space [19].

72

In this chapter, we discussed model-based policy search methods. Typically, it is
assumed that these models are known or have been trained in a pre-processing step [7,
43, 45, 29]. Here, humans were asked to maneuver the robot (e.g., a helicopter or a blimp)
in order to collect data for model building. A crucial aspect of the collected data is that
it covers the regions of the state space that are relevant for successfully learning the task
at hand. Nevertheless, it is possible that it could be optimal (according to the reward
function) to explore regions outside the training data of the current model. In this case,
however, the learned model must be able to faithfully predict its confidence far away from
the training data. Deterministic models (e.g., LWR or neural networks) cannot faithfully
represent their confidence far away from the training data, which is why extrapolation
is often discouraged by large penalty terms in the reward function [7]. Two models
that possess credible error bars outside the training set are locally weighted Bayesian
regression and Gaussian processes. Therefore, they can even be used for learning from
scratch in a robotics context, i.e., without the need to ask a human expert to generate
good data for model learning or a reasonably innate starting policy—if the robot is
relatively robust to initial arbitrary exploration [18, 19].

73

Chapter 4

Conclusion
In this review, we have provided an overview of successful policy search methods in
the context of robot learning, where high-dimensional and continuous state-action space
challenge any RL algorithm. We distinguished between model-free and model-based
policy search methods.
Model-free policy search is very flexible as it does not impose any structure on learned
models. Instead of learning models, data from the robot is directly used to evaluate
and update the policy. When prior knowledge in form of demonstrations or low-level
policies is available, model-free policy search often yields relatively fast convergence.
In Chapter 2, we distinguished between three ways of updating the policy based on
trajectory samples from the robot: updates using policy gradients, updates based on the
EM algorithm, and updates based on information-theoretic insights. Although modelfree policy search imposes only general assumptions on the entire learning process, it
is practically limited by the number of policy parameters it can learn: Learning is
purely based on sampled trajectories from the robot. While tens of parameters are still
feasible, learning hundreds or thousands of policy parameters seems impracticable due
to an excessive need of real-robot experiments. Our recommendation as a model-free
approach is REPS [48].
The objective of model-based policy search is to increase the data efficiency of modelfree methods. For this purpose, an internal model of the robot is learned that, subsequently, is used for long-term predictions and policy optimization. The learned policy
is, therefore, inherently biased toward the quality of the model. Thus, it is crucial to account for potential model errors during policy learning by expressing uncertainty about
the learned model itself. This idea has been successfully implemented by all model-based
algorithm presented in Chapter 3. Note that model learning imposes assumptions, such
as differentiability, on the robot’s forward dynamics, effectively reducing the generality of model-free policy search. We distinguished between stochastic and deterministic
approaches for trajectory predictions. While sampling-based inference is conceptually
simple and can be easily parallelized, it is currently limited to successfully learn policies
with several tens of parameters, similarly to model-free policy search. In cases with hundreds or thousands of policy parameters, we have to resort to deterministic approximate

74

inference (e.g., linearization or moment matching), ideally in combination with an analytic computation of policy gradients. Our recommendation as a model-based approach
is pilco [18, 19].

Acknowledgments
The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007–2013) under grant agreement #270327.

75

Bibliography
[1] P. Abbeel, M. Quigley, and A. Y. Ng. Using Inaccurate Models in Reinforcement
Learning. In Proceedings of the 23rd International Conference on Machine Learning,
pages 1–8, Pittsburgh, PA, USA, June 2006.
[2] E. W. Aboaf, S. M. Drucker, and C. G. Atkeson. Task-Level Robot Learning: Juggling a Tennis Ball More Accurately. In Proceedings of the International Conference
on Robotics and Automation, 1989.
[3] S. Amari. Natural Gradient works efficiently in Learning. Neural Computation,
10:251–276, February 1998.
[4] B. D. O. Anderson and J. B. Moore. Optimal Filtering. Dover Publications, Mineola,
NY, USA, 2005.
[5] K. J. Astr¨
om and B. Wittenmark. Adaptive Control. Dover Publications, 2008.
[6] C. G. Atkeson and J. C. Santamar´ıa. A Comparison of Direct and Model-Based
Reinforcement Learning. In Proceedings of the International Conference on Robotics
and Automation, 1997.
[7] J. A. Bagnell and J. G. Schneider. Autonomous Helicopter Control using Reinforcement Learning Policy Search Methods. In Proceedings of the International
Conference on Robotics and Automation, pages 1615–1620. IEEE Press, 2001.
[8] J. A. Bagnell and J. G. Schneider. Covariant Policy Search. In Proceedings of the
International Joint Conference on Artifical Intelligence, August 2003.
[9] J. Baxter and P. Bartlett. Direct Gradient-Based Reinforcement Learning: I. Gradient Estimation Algorithms. Technical report, 1999.
[10] J. Baxter and P. L. Bartlett. Infinite-horizon policy-gradient estimation. 2001.
[11] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 1 of Optimization and Computation Series. Athena Scientific, Belmont, MA, USA, 3rd
edition, 2005.
[12] C. M. Bishop. Pattern Recognition and Machine Learning. Information Science and
Statistics. Springer-Verlag, 2006.
76

[13] J. A. Boyan. Least-Squares Temporal Difference Learning. In In Proceedings of the
Sixteenth International Conference on Machine Learning, pages 49–56, 1999.
[14] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,
2004.
[15] W. S. Cleveland and S. J. Devlin. Locally-Weighted Regression: An Approach to
Regression Analysis by Local Fitting. Journal of the American Statistical Association, 83(403):596–610, 1988.
[16] R. Coulom. Reinforcement Learning Using Neural Networks, with Applications to
Motor Control. PhD thesis, Institut National Polytechnique de Grenoble, 2002.
[17] C. Daniel, G. Neumann, and J. Peters. Hierarchical Relative Entropy Policy Search.
In N. Lawrence and M. Girolami, editors, Proceedings of the International Conference of Artificial Intelligence and Statistics, pages 273–281, 2012.
[18] M. P. Deisenroth and C. E. Rasmussen. PILCO: A Model-Based and Data-Efficient
Approach to Policy Search. In Proceedings of the International Conference on Machine Learning, pages 465–472, New York, NY, USA, June 2011. ACM.
[19] M. P. Deisenroth, C. E. Rasmussen, and D. Fox. Learning to Control a LowCost Manipulator using Data-Efficient Reinforcement Learning. In Proceedings of
the International Conference on Robotics: Science and Systems, Los Angeles, CA,
USA, June 2011.
[20] K. Doya. Reinforcement Learning in Continuous Time and Space. Neural Computation, 12(1):219–245, January 2000.
[21] S. Fabri and V. Kadirkamanathan. Dual Adaptive Control of Nonlinear Stochastic
Systems using Neural Networks. Automatica, 34(2):245–253, February 1998.
[22] A. A. Fel’dbaum. Dual Control Theory, Parts I and II. Automation and Remote
Control, 21(11):874–880, 1961.
[23] N. Hansen, S. Muller, and P. Koumoutsakos. Reducing the Time Complexity of the
Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES).
Evolutionary Computation, 11(1):1–18, 2003.
[24] V. Heidrich-Meisner and C. Igel. Hoeffding and Bernstein Races for Selecting Policies in Evolutionary Direct Policy Search. In ICML ’09: Proceedings of the 26th
Annual International Conference on Machine Learning, pages 401–408. ACM, 2009.
[25] V. Heidrich-Meisner and C. Igel. Neuroevolution Strategies for Episodic Reinforcement Learning. Journal of Algorithms, 64(4):152–168, oct 2009.
[26] A. J. Ijspeert and S. Schaal. Learning Attractor Landscapes for Learning Motor
Primitives. In Advances in Neural Information Processing Systems 15, (NIPS 2003),
pages 1523–1530. MIT Press, Cambridge, MA, 2003.
77

[27] S. J. Julier and J. K. Uhlmann. Unscented Filtering and Nonlinear Estimation.
Proceedings of the IEEE, 92(3):401–422, March 2004.
[28] H. Kimura and S. Kobayashi. Efficient Non-Linear Control by Combining Q-learning
with Local Linear Controllers. In Proceedings of the 16th International Conference
on Machine Learning, pages 210–219, 1999.
[29] J. Ko, D. J. Klein, D. Fox, and D. Haehnel. Gaussian Processes and Reinforcement
Learning for Identification and Control of an Autonomous Blimp. In Proceedings of
the International Conference on Robotics and Automation, pages 742–747, 2007.
[30] J. Kober, B. J. Mohler, and J. Peters. Learning Perceptual Coupling for Motor
Primitives. In Intelligent Robots and Systems (IROS), pages 834–839, 2008.
[31] J. Kober and J. Peters. Policy Search for Motor Primitives in Robotics. Machine
Learning, pages 1–33, 2010.
[32] J. Kober and J. Peters. Learning Elementary Movements jointly with a Higher
Level Task. In Intelligent Robots and Systems (IROS), pages 338–343, 2011.
[33] N. Kohl and P. Stone.
Policy Gradient Reinforcement Learning for Fast
Quadrupedal Locomotion. In International Conference for Robotics and Automation (ICRA), 2003.
[34] P. Kormushev, S. Calinon, and D. G. Caldwell. IEEE/RSJ International Conference
on Intelligent Robots and Systems. In Proceedings of the IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2010.
[35] M. G. Lagoudakis and R. Parr. Least-Squares Policy Iteration. Journal of Machine
Learning Research, 4:1107–1149, December 2003.
[36] D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, The Edinburgh Building, Cambridge CB2 2RU, UK, 2003.
[37] D. C. McFarlane and K. Glover. Lecture Notes in Control and Information Sciences,
volume 138, chapter Robust Controller Design using Normalised Coprime Factor
Plant Descriptions. Springer-Verlag, 1989.
[38] J. Morimoto and C. G. Atkeson. Minimax Differential Dynamic Programming: An
Application to Robust Biped Walking. In S. Becker, S. Thrun, and K. Obermayer,
editors, Advances in Neural Information Processing Systems. The MIT Press, 2003.
[39] R. Neal and G. E. Hinton. A View Of The Em Algorithm That Justifies Incremental,
Sparse, And Other Variants. In Learning in Graphical Models, pages 355–368.
Kluwer Academic Publishers, 1998.
[40] J. A. Nelder and R. Mead. A Simplex Method for Function Minimization. Computer
Journal, 7:308–313, 1965.
78

[41] G. Neumann. Variational Inference for Policy Search in Changing Situations. In Proceedings of the 28th International Conference on Machine Learning, (ICML 2011),
pages 817–824, New York, NY, USA, June 2011. ACM.
[42] A. Y. Ng. Stanford Engineering Everywhere CS229—Machine Learning, Lecture 20, 2008. http://see.stanford.edu/materials/aimlcs229/transcripts/
MachineLearning-Lecture20.html.
[43] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and
E. Liang. Autonomous Inverted Helicopter Flight via Reinforcement Learning. In
M. H. Ang Jr. and O. Khatib, editors, International Symposium on Experimental Robotics, volume 21 of Springer Tracts in Advanced Robotics, pages 363–372.
Springer, 2004.
[44] A. Y. Ng and M. Jordan. Pegasus: A Policy Search Method for Large MDPs
and POMDPs. In Proceedings of the 16th Conference on Uncertainty in Artificial
Intelligence, pages 406–415, 2000.
[45] A. Y. Ng, H. J. Kim, M. I. Jordan, and S. Sastry. Autonomous Helicopter Flight
via Reinforcement Learning. In S. Thrun, L. K. Saul, and B. Sch¨olkopf, editors,
Advances in Neural Information Processing Systems, Cambridge, MA, USA, 2004.
The MIT Press.
[46] B. Øksendal. Stochastic Differential Equations: An Introduction with Applications
(Universitext). Springer, 6th edition, Sept. 2010.
[47] J. Peters, M. Mistry, F. E. Udwadia, J. Nakanishi, and S. Schaal. A Unifying
Methodology for Robot Control with Redundant DOFs. Autonomous Robots, (1):1–
12, 2008.
[48] J. Peters, K. M¨
ulling, and Y. Altun. Relative Entropy Policy Search. In Proceedings
of the 24th National Conference on Artificial Intelligence (AAAI). AAAI Press,
2010.
[49] J. Peters and S. Schaal. Policy Gradient methods for Robotics. In Proceedings of
the IEEE International Conference on Intelligent Robotics Systems (IROS), Beijing,
China, 2006.
[50] J. Peters and S. Schaal. Applying the Episodic Natural Actor-Critic Architecture to
Motor Primitive Learning. In Proceedings of the European Symposium on Artificial
Neural Networks, 2007.
[51] J. Peters and S. Schaal. Natural Actor-Critic. Neurocomputation, 71(7-9):1180–
1190, 2008.
[52] J. Peters and S. Schaal. Reinforcement Learning of Motor Skills with Policy Gradients. Neural Networks, (4):682–97, 2008.
79

[53] J. Peters, S. Vijayakumar, and S. Schaal. Reinforcement Learning for Humanoid
Robotics. In Humanoids2003, 3rd IEEE-RAS International Conference on Humanoid Robots, Karlsruhe. IEEE, September 2003.
[54] T. Raiko and M. Tornio. Variational Bayesian Learning of Nonlinear Hidden StateSpace Models for Model Predictive Control. Neurocomputing, 72(16–18):3702–3712,
2009.
[55] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning.
Adaptive Computation and Machine Learning. The MIT Press, Cambridge, MA,
USA, 2006.
[56] T. R¨
uckstieß, M. Felder, and J. Schmidhuber. State-Dependent Exploration for
Policy Gradient Methods. In European Conference on Machine Learning (ECML),
pages 234–249, 2008.
[57] T. R¨
uckstieß, F. Sehnke, T. Schaul, D. Wierstra, Y. Sun, and J. Schmidhuber.
Exploring Parameter Space in Reinforcement Learning. Paladyn, 1(1):14–24, Mar.
2010.
[58] S. J. Russel and P. Norvig. Artificial Intelligence: A Modern Approach. Pearson
Education, 2003.
[59] S. Schaal and C. G. Atkeson. Constructive Incremental Learning from only Local
Information. Neural Computation, 10(8):2047–2084, 1998.
[60] S. Schaal, J. Peters, J. Nakanishi, and A. Ijspeert. Learning Movement Primitives.
In International Symposium on Robotics Research, (ISRR 2003), pages 561–572,
2003.
[61] J. G. Schneider. Exploiting Model Uncertainty Estimates for Safe Dynamic Control
Learning. In Advances in Neural Information Processing Systems. Morgan Kaufman
Publishers, 1997.
[62] F. Sehnke, C. Osendorfer, T. R¨
uckstieß, A. Graves, J. Peters, and J. Schmidhuber.
Policy gradients with parameter-based exploration for control. In Proceedings of the
International Conference on Artificial Neural Networks ICANN, 2008.
[63] F. Sehnke, C. Osendorfer, T. R¨
uckstieß, A. Graves, J. Peters, and J. Schmidhuber.
Parameter-Exploring Policy Gradients. Neural Networks, 23(4):551–559, 2010.
[64] F. Stulp and O. Sigaud. Path integral policy improvement with covariance matrix
adaptation. In ICML, 2012.
[65] Y. Sun, D. Wierstra, T. Schaul, and J. Schmidhuber. Efficient natural evolution
strategies. In Proceedings of the 11th Annual conference on Genetic and evolutionary
computation, GECCO ’09, pages 539–546, New York, NY, USA, 2009. ACM.

80

[66] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy Gradient Methods
for Reinforcement Learning with Function Approximation. In Neural Information
Processing Systems (NIPS), 1999.
[67] R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT Press, Boston, MA,
1998.
[68] E. Theodorou, J. Buchli, and S. Schaal. A Generalized Path Integral Control
Approach to Reinforcement Learning. Journal of Machine Learning Research,
(11):3137–3181, 2010.
[69] S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics. The MIT Press, Cambridge, MA, USA, 2005.
[70] M. Toussaint. Robot Trajectory Optimization using Approximate Inference. In Proceedings of the 26th International Conference on Machine Learning, (ICML 2009),
2009.
[71] N. Vlassis and M. Toussaint. Model-Free Reinforcement Learning as Mixture Learning. In International Conference on Machine Learning (ICML 2009), page 136,
2009.
[72] N. Vlassis, M. Toussaint, G. Kontes, and S. Piperidis. Learning Model-Free Robot
Control by a Monte Carlo EM Algorithm. Autonomous Robots, 27(2):123–130, 2009.
[73] P. Wawrzynski and A. Pacut. Model-free off-policy Reinforcement Learning in
Continuous Environment. In Proceedings of the INNS-IEEE International Joint
Conference on Neural Networks, pages 1091–1096, 2004.
[74] D. Wierstra, T. Schaul, J. Peters, and J. Schmidhuber. Natural evolution strategies.
In IEEE Congress on Evolutionary Computation, pages 3381–3387, 2008.
[75] R. J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist
Reinforcement Learning. Machine Learning, 8:229–256, 1992.
[76] B. Wittenmark. Adaptive Dual Control Methods: An Overview. In In Proceedings
of the 5th IFAC Symposium on Adaptive Systems in Control and Signal Processing,
pages 67–72, 1995.
[77] K. Xiong, H.-Y. Zhang, and C. W. Chan. Performance Evaluation of UKF-based
Nonlinear Filtering. Automatica, 42:261–270, 2006.

81

A

Gradients of Frequently Used Policies

All used policies use Gaussian distributions to generate exploration. Here we state the
most frequently used gradients for Gaussian policies w.r.t the mean and the covariance
matrix of the Gaussian. The gradients are always stated for policies in action space,
however, the policies which are defined in parameter space have of course the same
gradient.
The log-likelihood of a Gaussian policy πθ (u|x) = N (u|µ, Σ) is given by
d
1
1
log πθ (u|x) = − log 2π − log |Σ| − (u − µ)T Σ−1 (u − µ) .
2
2
2

Constant Mean. If the policy is given as πθ (u|x) = N (u|µ, Σ) and µ is part of θ,
then
∇µ log πθ (u|x) = (u − µ)T Σ−1 .
The gradient simplifies if Σ = diag(σ 2 ),

∇µd log πθ (u|x) = (ud − µd )/σd2 ,
for the dth dimension of µ.
Linear Mean.
of θ, then

If the policy is given as πθ (ut |xt ) = N (u|φt (x)T M , Σ) and M is part
∇M log πθ (u|x) = (u − φt (x)T M )T Σ−1 φt (x),

or for a diagonal covariance matrix
∇md log πθ (u|x) = (ud − φt (x)T md )φt (x)/σd2 ,
where md corresponds to the dth column of M .
Diagonal Covariance Matrix.
derivative is given by

For a diagonal covariance matrix Σ = diag(σ 2 ) the

∇σd log πθ (u|x) = −

1
+ (ud − µd )2 /σd3
σd2

Full Covariance Matrix. For representing the full covariance matrix, typically the
Cholesky decomposition of the covariance matrix Σ = AT A, where A is an uppertriangular matrix, is used as parametrization [65]. The parametrization with the Cholesky
decomposition exploits the symmetry of the covariance matrix and enforces that Σ is
positive definite. The gradient of log πθ (u|x) w.r.t A is given by
1
∂ai,j log πθ (u|x) = − ∂ai,j log |AT A| −
2
1
∂a A−T (u − µ)
2 i,j
= a−1
i,j δi,j + si,j ,
82

T

A−T (u − µ)

where δi,j is the dirac-delta function which is one if i = j and zero elsewhere and si,j is
the (i, j)th element of the matrix S,
S = (u − µ) (u − µ)T A−1 A−T A−1

B

Weighted ML Estimates of Frequently Used Policies

We will assume a data-set D given in the following form
D = x[i] , u[i] , d[i]

i=1...N

,

where d[i] denotes the weighting of the ith sample. In this Section we will state the
solution of weighted ML estimation, i.e.,
N
∗

log πθ u[i] x[i] ,

θ = argmaxθ
i=1

for the most frequently used policies.
For the episode-based formulation of Policy-Search, the states x[i] are exchanged
with the contexts s[i] and the actions u[i] are exchanged by the parameters θ [i] of the
low-level controller.
Gaussian Policy, Constant Mean. Consider a policy which is given by π(u) =
N (u|µ, Σ), i.e., we do not have a state or a context. Such a policy is for example useful
to model the upper-level policy without contexts. The weighted ML-solution for µ and
Σ is given by
µ=

N
[i] [i]
i=1 d u
,
N
[i]
i=1 d

Σ=

where
Z=

N
[i]
i=1 d

N
[i]
i=1 d

2

u[i] − µ
Z
N
i=1

−

d[i]

u[i] − µ

T

,

(4.1)

2

N
[i]
i=1 d

is used to obtain an unbiased estimate of the covariance. The elements σ of a diagonal
covariance matrix Σ = diag(σ) can be obtained by

σh =

N
[i]
i=1 d

[i]

uh − µh
Z

2

.

(4.2)

Gaussian Policy, Linear Mean. The policy is given by π(u|x) = N (u|W T φ(x), Σ).
The weighted ML-solution for W is determined by the weighted pseudo-inverse and
already given in Equation (2.43).
83

Gaussian Policy, Linear Mean, State-Dependent Variance The policy is given
by π(u|x) = N (u|wT φ(x), φ(x)T Σw φ(x)). Here we consider only the scalar case as
the multi-dimensional distribution case is more complicated and only rarely used. The
weighted ML-solution for w is determined by the weighted pseudo-inverse where the
weights are given as the product of the state-dependent variance φ(x[i] )T Σw φ(x[i] ) and
the actual weighting d[i] . The solution for w is already given in Equation (2.53).

C

Derivations of the Dual Functions for REPS

Lagrangian Function. Consider the following general constraint optimization problem with equality and inequality constraints
max
y

s.t:

f (y)
a(y) = 0
b(y) ≤ 0

(4.3)

Such optimization problem can be solved by finding the saddle-points of the Lagrangian
L = f (y) + λT1 a(y) + λT2 b(y).

(4.4)

The optimization problem has a local maximum if the direction of the gradient ∂y f (y)
is aligned with the normal of the constraints λT1 ∂y a(y) and λT2 ∂y b(y). Such a point can
be found by differentiating the Lagrangian w.r.t y and setting it to zero.
∂y f (y) = λT1 ∂y a(y) + λT2 ∂y b(y).

(4.5)

Dual Function. Optimizing the dual function of an optimization problem is, under
certain conditions equivalent to solving the original optimization problem [14]. However,
the dual function is often easier to optimize. The dual function is obtained by finding
y = c(λ1 , λ2 ) which satisfies the saddle-point condition given in Equation (4.5). This
solution is in turn set back into the Lagrangian, which results in the dual-function
g(λ1 , λ2 ).
If the original optimization problem is maximized, the dual function needs to be
minimized [14]. The dual function only depends on the Lagrangian multipliers and
is therefore often easier to optimize. Each inequality constraint used in the original
optimization problem introduces an inequality constraint for the Lagrangian multipliers.
Hence, the original optimization problem can also be solved by solving the following
program
min

λ1 ,λ2

g(λ1 , λ2 )

s.t: λ2 ≥ 0

(4.6)

The solution for y can subsequently be found by setting the Lagrangian parameters back
into c(λ1 , λ2 ).
84

Infinite Horizon REPS. We denote p(x, u) = µπ (x)π(u|x) and p(x) = u p(x, u)
for brevity of the derivations. The Lagrangian for the program in Equation (2.58) with
state features ϕ(x) is given by
L=

p(x, u)r(x, u)

+η

x,u

+ vT

ϕ(x )
x,u

x

+λ 1−

−

p(x, u) log
x,u

p(x, u)p(x |x, u) −

p(x, u)
q(x, u)

p(x , u )
u

p(x, u) ,

(4.7)

x,u

where η, v and λ denote the Lagrangian multipliers. Rearranging terms results in
L=
x,u

p(x, u) r(x, u) − η log

+v T
x

p(x |x, u)ϕ(x )

p(x, u)
− λ − v T ϕ(x)
q(x, u)

+ ηε + λ.

(4.8)

We substitute Vv (x) = v T ϕ(x). Differentiating the Lagrangian w.r.t p(x, u)
p(x, u)
+1 −λ
q(x, u)
− Vv (x) + Ep(x |x,u) Vv (x ) = 0,

∂p(x,u) L = r(x, u) − η log

(4.9)

and setting the derivative to zero yields the solution for p(x, u),
p(x, u) = q(x, u) exp

δv (x, u)
η

−η − λ
η

exp

,

(4.10)

with δv (x, u) = r(x, u)+Ep(x |x,u) [Vv (x )]−Vv (x). Given that we require
1, it is necessary that
exp

−η − λ
η

=

x,u

q(x, u) exp

x,u p(x, u)

=

−1

δv (x, u)
η

,

(4.11)

Setting Equation (4.11) into Equation (4.10) yields the closed form solution for p(x, u).
Reinserting Equation (4.10) into the Lagrangian (4.8)1
g(η, λ) = η + η + ηλ = η + η log exp

η+λ
η

.

(4.12)

As we know from Equation (4.11) that λ depends on v, we substitute (4.11) to get the
formulation of the dual function which depends on η and v.
g(η, v) = η + η log

q(x, u) exp
x,u

1

δv (x, u)
η

.

(4.13)

It is easier to just insert Equation (4.10) into the log p(x, u) term of the Lagrangian. All other terms
connected to p(x, u) cancel out.

85

Optimizing the Dual-Function. The dual function is in log-sum-exp form and therefore convex in v. As we have one inequality constraint in the original optimization problem, we also get an inequality constraint for the dual problem which requires that η > 0.
Hence, for a given set of samples (x[i] , u[i] ), we have to solve the following problem2
1
exp
N

min η + η log
η,v

x[i] ,u[i]

δv (x[i] , u[i] )
η

,

s.t: η > 0.

(4.14)

Any optimizer for constraint optimization problems can be used to solve this problem,
for example fmincon in MATLAB. This optimization can typically performed more
efficiently by providing the optimization algorithm also the derivatives of g, which are
given by
∂η g(η, v) = + log
i

∂v g(η, v) =

i Zi

1
Zi
N

−

i Zi δv (x

η

[i] , u[i] )

i Zi

Ep(x |x[i] ,u[i] ) [ϕ(x )] − ϕ(x[i] )
i Zi

,

(4.15)

,

(4.16)

with Zi = exp δv (x[i] , u[i] )/η .
Dual-Function of Episode-based REPS. The derivation of the dual-function for
parameter-based REPS follows the derivation given for the infinite horizon REPS. For
this reason, we will only state the resulting dual-function for the contextual policy search
setup and skip the derivation. The dual-function is given by
ˆ + η log
g(η, v) = η + v T ϕ

q(s, θ) exp
s,θ

δv (s, θ)
η

,

(4.17)

where δv (s, θ) = R(s, θ) − v T ϕ(s).
Dual-Function of Hierarchical REPS. The dual function of HiREPS is given by
1+ ηξ

ˆ q κξ + η log
g(η, ξ, v) = η + H

p˜(o|s, θ)
s,θ

exp

δv (s, θ)
η

,

(4.18)

where δv (x, u) is defined as for the episode-based REPS algorithm and ξ is the Lagrangian multiplier connected with the constraint which prevents an overlapping of the
options.
2
For numerical stability, we recommend to subtract the maximum δv inside the exponential and add
it again outside the log, i.e.,

g(η, v) = η + max δv + η log
x[i] ,u[i]

86

1
exp
N

δv (x[i] , u[i] ) − max δv
η

