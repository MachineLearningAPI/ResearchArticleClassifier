RESEARCHCONTRIBUTIONS

Artificial
Intelligence and
Language Processing

A Theory of the Learnable

David Waltz
Editor

L, G. VALIANT

ABSTRACT: Humans appear to be able to learn new
concepts without needing to be programmed explicitly in
any conventional sense. In this paper we regard learning as
the phenomenon of knowledge acquisition in the absence of
explicit programming. We give a precise methodology for
studying this phenomenon from a computational viewpoint.
It consists of choosing an appropriate information gathering
mechanism, the learning protocol, and exploring the class of
concepts that can be learned using it in a reasonable
(polynomial) number of steps. Although inherent algorithmic
complexity appears to set serious limits to the range of
concepts that can be learned, we show that there are some
important nontrivial classes of propositional concepts that
can be learned in a realistic sense.
1. INTRODUCTION
Computability theory became possible once precise
models became available for modeling the commonplace phenomenon of mechanical calculation. The theory that evolved has been used to explain human experience and to suggest how artificial computing devices
should be built. It is also worth studying for its own
sake.
The commonplace phenomenon of learning surely
merits similar attention. The problem is to discover
good models that are interesting to study for their own
sake and that promise to be relevant both to explaining
human experience and to building devices that can
learn. The models should also shed light on the limits
of what can be learned, just as computability does on
what can be computed.
In this paper we shall say that a program for performing a task has been acquired by learning if it has been
acquired by any means other than explicit programming. Among human skills some clearly appear to have
This research was supported in part by National Science Foundation
grant MCS-83-02385. A preliminary version of this paper appeared in
the proceedings of the 16th ACM Symposium on Theory of Computing,
Washington, D.C., 1984, 436-445.

©1984ACMO001-0782/84/1100-1134 75¢

1134

Communicationsof the ACM

a genetically preprogrammed element, whereas some
others consist of executing an explicit sequence of instructions that has been memorized. There remains a
large area of skill acquisition where no such explicit
programming is identifiable. It is this area that we describe here as learning. The recognition of familiar objects, such as tables, provides such examples. These
skills often have the additional property that, although
we have learned them, we find it difficult to articulate
what algorithm we are really using. In these cases it
would be especially significant if machines could be
made to acquire them by learning.
This paper is concerned with precise computational
models of the learning phenomenon. We shall restrict
ourselves to skills that consist of recognizing whether a
concept (or predicate) is true or not for given data. We
shall say that a concept Q has been learned if a program for recognizing it has been deduced (i.e., by some
method other than the acquisition from the outside of
the explicit program).
The main contribution of this paper is that it shows
that it is possible to design learning machines that have
all three of the following properties:
1. The machines can provably learn whole classes of
concepts. Furthermore, these classes can be characterized.
2. The classes of concepts are appropriate and nontrivial for general-purpose knowledge.
3. The computational process by which the machines
deduce the desired programs requires a feasible
(i.e., polynomial) number of steps.
A learning machine consists of a learning protocol together with a deduction procedure. The former specifies
the manner in which information is obtained from the
outside. The latter is the mechanism by which a correct
recognition algorithm for the concept to be learned is
deduced. At the broadest level, the suggested methodology for studying learning is the following: Define a plausible learning protocol, and investigate the class of con-

November 1984 Volume 27 Number 11

Research Contributions

cepts for which recognition programs can be deduced
in polynomial time using the protocol.
The specific protocols considered in this paper allow
for two kinds of information supply. First the learner
has access to a supply of typical data that positively
exemplify the concept. More precisely, it is assumed
that these positive examples have a probabilistic distribution determined arbitrarily by nature. A call of a
routine EXAMPLES produces one such positive example. The relative probability of producing different examples is determined by the distribution. The second
source of information that may be available is a routine
ORACLE. In its most basic version, when presented
with data, it will tell the learner whether or not the
data positively exemplify the concept.
The major remaining design choice that has to be
made is that of knowledge representation. Since our
declared aim is to represent general knowledge, it
seems almost unavoidable that we use some kind of
logic rather than, for example, formal grammars or geometrical constructs. In this paper we shall represent
concepts as Boolean functions of a set of propositional
variables. The recognition algorithms that we attempt
to deduce will be therefore Boolean circuits or expressions.
The adequacy of the propositional calculus for representing knowledge in practical learning systems is
clearly a crucial and controversial question. Few would
argue that this much power is not necessary. The question is whether it is enough. There are several arguments suggesting that such a system would, at least, be
a very good start. First, when one examines the most
famous examples of systems that embody preprogrammed knowledge, namely, expert systems such as
DENDRAL and MYC!N, essentially no logical notation
beyond the propositional calculus is used. Surely it
would be overambitious to try to base learning systems
on representations that are more powerful than those
that have been successfully managed in programmed
systems. Second, the results in this paper can be negatively interpreted to suggest that the class of learnable
concepts even within the propositional calculus is severely circumscribed. This suggests that the search for
extensions to the propositional calculus that have substantially larger learnable classes may be a difficult
one.
The positive conclusions of this paper are that there
are specific classes of concepts that are learnable in
polynomial time using learning protocols of the kind
described. These classes can all be characterized by
defining the class of programs that recognize them. In
each case the programs are special kinds of Boolean
expressions. The three classes are (l) conjunctive normal form expressions with a bounded number of literals in each clause, (2) monotone disjunctive normal
form expressions, and (3) arbitrary expressions in
which each variable occurs just once. In the first of
these, no calls of the oracle are necessary. In the last,
no access to typical examples is necessary but the oracles need to be more powerful than the one described
above.

November 1984 Volume 27 Number 11

The deduction procedure will in each case output an
expression that with high likelihood closely approximates the expression to be learned. Such an approximate expression never says yes when it should not but
may say no on a small fraction of the probability space
of positive examples. This fraction can be made arbitrarily small by increasing the runtime of the deduction
procedure. Perhaps the main technical discovery contained in the paper is that with this probabilistic notion
of learning highly convergent learning is possible for
whole classes of Boolean functions. This appears to distinguish this approach from more traditional ones
where learning is seen as a process of "inducing" some
general rule from information that is insufficient for a
reliable deduction to be made. The power of this probabi!istic viewpoint is illustrated, for example, by the fact
that an arbitrary set of polynomially many positive examples cannot be relied on to determine expressions
consisting even of only a single monomial in any reliable way. A more detailed description of this methodological problem can be found in [9].
There is another aspect of our formulation that is
worth emphasizing. In a learning system that is about
to learn a new concept, there may be an enormous
number of propositional variables available. These may
be primitive inputs, the values of preprogrammed concepts, or the values of concepts that have been lcarned
previously. We want the complexity of learning the
new concept to be related only to the number of variables that may be set in natural examples of it, and not
on the cardinality of the universe of available variables.
Hence, the questions asked of ORACLE and the values
given by EXAMPLES will not be truth assignments to
all the variables. In the archetypal case, they will specify an assignment to a subset of the variables that is still
sufficient to guarantee the truth of the function.
Whether the classes of learnable Boolean concepts
can be extended significantly beyond the three classes
given is an interesting question. There is circumstantial
evidence from cryptography, however, that the whole
class of functions computable by polynomial size circuits is not learnable. Consider a cryptographic scheme
that encodes messages by evaluating the function Ek
where k specifies the key. Suppose that this scheme is
immune to chosen plaintext attack in the sense that,
even if the values of Ek are known for polynomially
many dynamically chosen inputs, it is computationally
infeasible to deduce an algorithm for Ek or for an approximation to it. This is equivalent to saying, however,
that Ek is not learnable. The conjectured existence of
good cryptographic functions that are easy to compute
therefore implies that some easy-to-compute functions
are not learnable.
If the class of learnable concepts is as severely limited as suggested by our results, then it would follow
that the only way of teaching more complicated concepts is to build them up from such simple ones. Thus,
a good teacher would have to identify, name, and sequence these intermediate concepts in the manner of a
programmer. The results of learnability theory would
then indicate the maximum granularity of the single

Communications of the ACM

1135

Research Contributions
concepts that can be acquired without programming.
In summary, this paper attempts to explore the limits
of what is learnable as allowed by algorithmic complexity. The results are distinguishable from the diverse
body of previous work on learning because they attempt to reconcile the three properties ((1)-(3)) mentioned earlier. Closest in rigor to our approach is the
inductive inference literature (see Angluin and Smith
[1] for a survey) that deals with inducing such things as
recursive functions or formal grammars (but not Boolean functions) from examples. There is a large body of
work on pattern recognition and classification, using
statistical and other tools (e.g., [4]), but the question of
general knowledge representation is not addressed
there directly. Learning, in various less formal senses,
has been studied widely as a branch of artificial intelligence. A survey and bibliography can be found in
[2, 7]. In their terminology the subject of this paper is
concept learning.
In previous work on learning, much emphasis is
placed on the diversity of methods that humans appear
to be using. These include learning by example, learning by analogy, and learning by being told. This paper
emphasizes the polarity between learning by being programmed and learning in the absence of any programming element. Many of the conventionally accepted
modes of learning can be regarded as being hybrids of
these two extremes.
2. A LEARNING PROTOCOL
FOR BOOLEAN FUNCTIONS
We consider t Boolean variables pl . . . . . pt, each of
which can take the value 1 or 0 to indicate w h e t h e r the
associated proposition is true or false. There is no assumption about the independence of the variables. Indeed, they may be functions of each other.
A vector is an assignment to each of the t variables of
a value from {0, 1, *}. The symbol * denotes that a
variable is undetermined. A vector is total if every variable is determined (i.e., is assigned a value from {0, 1}).
For example, the assignment pl = p3 = 1, p4 = 0, and
p2 = * is a vector that is not total.
A Boolean function F is a mapping from the set of 2 t
total vectors to {0, 1}. A Boolean function F becomes a
concept F if its domain is extended to the set of all
vectors as follows: For a vector v, F(v) = 1 if and only if
F(w) = 1 for all total vectors w that agree with v on all
variables for which v is determined. The purpose of
this extension is that it permits us not to mention the
variables on which F does not depend.
Given a concept F, we consider an arbitrary probability distribution D over the set of all vectors v such that
F(v) = 1. In other words, for each v such that F(v) = 1, it
is the case that D(v} _> O. Also ~ D(v) = 1 w h e n summation is over this set of vectors. There are no other assumed restrictions on D, which is intended to describe
the relative frequency with which the positive examples of F occur in nature.
What are reasonable learning protocols to consider?
First we must avoid giving the teacher too much power,
namely, the power to communicate a program instruc-

1136

Communicationsof the ACM

tion by instruction. For example, if a premeditated sequence of vectors with repetitions could be communicated, then this could be used to encode the description
of the program even if just two such vectors were used
for binary notation. Second we must avoid giving the
teacher what is evidently too little power. In particular,
the protocol must provide some typical examples of
vectors for which F is true, for otherwise, if F is true for
just one vector that is total, only an exponential search
or more powerful oracles would be able to find it.
Such considerations led us to consider the following
learning protocol as a reasonable one. It gives the
learner access to the following two routines:
1.

EXAMPLES: This has no input. It gives as output a
vector v such that F(v) = 1. For each such v, the
probability that v is output on any single call is
D(v).

2.

ORACLE( ): On vector v, as input it outputs 1 or 0
according to w h e t h e r F(v) = 1 or 0.

The first of these gives randomly chosen positive examples of the concept being learned. The second provides a test for w h e t h e r a vector, which the deduction
procedure considers to be critical, is an example of the
concept or not. In a real system, the oracle m a y be a
h u m a n expert, a database of past observations, some
deduction system, or a combination of these.
Finally we observe that our particular choice of
learning protocol was strongly influenced by the earlier
decision to deal with concepts rather than raw Boolean
functions. If we were to deal with the latter, then m a n y
other alternatives are equally natural. For example,
given a vector v, instead of asking, as we do, w h e t h e r
all completions of it make the function true, we could
ask w h e t h e r there exist any completions that make it
true. This suggests natural alternative semantics for
EXAMPLES and ORACLE. In Section 7 we shall use
such more elaborate oracles.
3. LEARNABILITY
We shall consider various classes of programs having
pl . . . . . pt as inputs and show that in each case any
program in the class can be deduced, with only small
probability of error, in polynomial time using the protocol previously described. We assume that programs can
take the values 1, 0, and undetermined.
More precisely we say that a class X of programs is
learnable with respect to a given learning protocol if and
only if there exists an algorithm A {the deduction procedure} invoking the protocol with the following properties:

1.

The algorithm runs in time polynomial in an adjustable parameter h, in the various parameters that
quantify the size of the program to be learned, and
in the n u m b e r of variables t.

2.

For all programs f E X and all distributions D over
vectors v on which f outputs 1, the algorithm will
deduce with probability at least (1 - h -1} a program
g ~ X that never outputs one w h e n it should not
but outputs one almost always w h e n it should. In

November 1984 Volume 27 Number 11

Research Contributions

particular, {i) for all vectors v, g(v) = i implies
f(v) = 1, and (ii} the sum of D(v) over all v such that
f(v) = 1, but g(v) ~ 1 is at most h-1.
In our definition we have disallowed positive answers from g to be wrong, only because the particular
classes X in this paper do not require it. This we call
one-sided error learning, in other circumstances it may
be efficacious to allow two-sided errors. In that more
general case, we would need to put a probability distribution D on the set of vectors such that f(v} ~ 1. Condition (i) i n (2} is then replaced by ~ D{v} over all v such
that f(v) ~ 1, but g{v) = 1 is at most h-1.
A second aspect of our definition is that the parameter h is used in two independent probabilistic bounds.
This simplifies the statement of the results. It would be
more precise, however, to work explicitly with two independent parameters hi and h2.
Third we should note that programs that compute
concepts should be distinguished from those that compute merely the Boolean function. This distinction has
little consequence for the three classes of expressions
that we consider, since in these cases programs for the
concepts follow directly from the specification of the
expressions. For the sake of generality, our definitions
do allow the value of a program to be undefined, since
a nontotal vector will not, in general, determine the
value of an expression as 0 or 1.
It would be interesting to obtain negative results,
other than the informal cryptographic evidence mentioned in the introduction, indicating that the class of
unrestricted Boolean circuits is not learnable. A recent
result [6] in this direction does establish this, under the
hypothesis that integer factorization is intractable.
Their result applies to the learning protocol consisting
of EXAMPLES and ORACLE, when the latter is restricted to total vectors. It may also be interesting to go
in the opposite direction and determine, for example,
whether the hypothesis that P equals NP would imply
that all interesting classes are learnable.
4. A COMBINATORIAL BOUND
The probabilistic analysis needed for our later results
can be abstracted into a single lemma. The proof of the
lemma is independent of the rest of the paper.
We define the function L(h,s) for any real number h
greater than one and any positive integer S as follows:
Let L(h,S) be the smallest integer such that in L(h,S)
independent Bernoulli trials each with probability at
least h-1 of success, the probability of having fewer
than S successes is less than h -1.
The relevance of the function L(h,S) to learning can
be illustrated as follows: Consider an urn U containing
a very large number of marbles possibly of many different types. Suppose we wish to "learn" what kinds of
marbles U contains by taking a small random sample X.
If all the marbles in U are of distinct types, then it is
clearly impossible to deduce much information about
them. Suppose, however, that we know that there are
only S types of marbles and that we wish to choose X so
that it contains representatives of all but 1 percent of

November 1984 Volume 27 Number 11

the marbles in U. Whatever the respective frequencies
of the S types may be, the definition of L(h,S} implies
that if iX[ = L(100,S) then with probability greater than
99 percent we will have succeeded. To see why this
implication holds, suppose that after L(100,S} successive
choices marble types representing between them at
least 1 percent of the urn remain unchosen. In that case
we have made L[100,S) Bernoulli trials each with probability at least 1 percent of success 0.e., of choosing a
previously unchosen type} and have succeeded fewer
than S times (since some type remains unchosen}. Note
that what constitutes a success here will depend on
previous choices. The probability of success, however,
will always be at least 1 percent, independent of previous choices, and this is sufficient for our argument.
The following simple upper bound holds for the
whole range of values of S and h and shows that L(h,S)
is essentially linear both in h and in S.
PROPOSITION: For all integers S >_ 1 and all real
h>l.
L(h,S) _< 2h(S + log~h).

Proof: We use the following three well-known inequalities of which the last is due to Chernoff (see [6]
page 18).
1. For a l l x > 0, (1 + x-l} x < e .
2. For a l l x > 0 , ( 1 - x - 1 } x < e -I.
3. In m independent trials, each with probability at
least p of success, the probability that there are at
most k success, where k < mp, is at most
m

-

The first factor in the expression in (a) above can be
rewritten as
1 mp
k~ ((m-k)/(mp-k}}{mp-k}
-

-

F/
Using (2) with x = (m - k)/(mp - k}, we can upper
bound the product by
e-mp+k(mp / k ) k.

Substituting m = 2h(S + log~h), p = h -1, and k = S and
using logarithms to the base e gives the bound
e-2S-21ogh+S. [2(1 + (tog h)/S)] (s/l°g h)logh.
Rewriting this using (1} with x = (log~h}/S gives
e-S-21ogh. 2 s. elOgh _< (2/e)S. e-log h < h-1.
As an illustration of an application, suppose that we
hare access to EXAMPLES, a source of natural positive
examples of vectors for concept F. Suppose that we
want to find an approximation to the subset P of variables that are determined in at least one natural example of F. Consider the following procedure: Pick the first
L(h, IPI} vectors provided by EXAMPLES, and let P ' be
the union of the sets of variables determined by these
vectors. The definition then implies that with probability at least 1 - h -1 the set P ' will have the following

Communications of the ACM

1137

Research Contributions

property: If a vector is chosen with probability distribution D, then the probability that it determines a variable in P - P' is less than h-1. The argument used
above for the urn model establishes this as follows: If P '
does not have the claimed property, then the following
has happened: L(h, [P[) trials have been made (i.e., calls
of EXAMPLES) each with probability greater than h-1
of success (i.e., finding a vector that determines a variable in P - P', but there have been fewer than [P]
successes (i.e., discoveries of new additions to P'). The
probability of such a happening is less than h -1 by the
definition of L.
The above application shows that the set of variables
that are determined in natural examples of F can be
approximated by a procedure whose runtime is independent of t, the total number of variables. It follows
that in the definition Of learnability the dependence on
t in part (1} can be relaxed to a dependence on the
number of variables that are ever defined in positive
examples.

5. BOUNDED CNF EXPRESSIONS
A conjunctive normal form (CNF) expression is any product c~c2 • • • Cr of clauses where each clause ci is a sum
q~ + q2 + • • • + qi, of literals. A literal q is either a

variable p or the negation p of a variable. For example,
(p~ + #2)(p~ + fi2 + pa) is a CNF expression. For a
positive integer k, a k-CNF expression is a CNF expression where each clause is the sum of at most k literals.
For CNF expressions in general, we do not know of
any polynomial time deduction procedure with respect
to our learning protocol. The question of whether one
exists is tantalizing because of its apparent simplicity.
In this section we shall prove that for any fixed integer k the class of k-CNF expressions is learnable. In fact
it is learnable easily in the sense that calls of EXAMPLES suffice and no calls Of ORACLE are required.
In this and subsequent sections, we shall use the relation ~ on concepts as follows: F ~ G means that for
all vectors v whenever F(v) = 1 it is also the case that
G(v} = i. (N.B. This is equivalent to the relation F ~ G
when F, G are regarded as functions.) For brevity we
shall often denote both expressions and even vectors by
the concepts they represent. Thus the concept represented by vector w is true for exactly those vectors that
agree with w on all variables on which w is determined.
The concept represented by expression f is obtained by
considering the Boolean function computed by f and
extending its domain to become a concept. In this section we regard a clause cl to be a special kind of expression. In Sections 7 and 8, we consider monomials m,
simple products of literals, as special forms of expressions. In all cases statements of the form f ~ g, v ~ ci,
v ~ m, or m ~ f can be understood by interpreting the
two sides as concepts or functions in the obvious way.
THEOREM A: For any positive integer k, the class of
k-CNF expressions is learnable via an algorithm A that uses
L = L(h, (2t) k÷l) calls of EXAMPLES and no calls of ORACLE, where t is the number of variables.
Proof:

1138

The algorithm is initialized with formula g as

Communications of the ACM

the product of all possible clauses of up to k literals
from {pl, pl, p2,/~2 . . . . . pt, #t}. Clearly the number of
ways of choosing a clause of exactly k literals is at most
(2t}k, and hence the number of ways of choosing a
clause with up to k literals is less than 2t + (2t) 2 + • • •
+ (2t) k < {2t}k+l. This bounds the initial number of
clauses in g.
The algorithm then calls EXAMPLES L times to give
positive examples of the concept represented by k-CNF
formula f. For each vector v so obtained, it deletes all of
the remaining clauses in g that do not contain a literal
that is determined to be true in v (i.e., in the clauses
deleted, each literal is either not determined or is negated in v}. More precisely the following is repeated L
times:

begin v := EXAMPLES
for each ci in g delete ci if v ~ cl.
end
The claim is that with high probability the value of g
after the Lth execution of this block will be the desired
approximation to the formula f that is being learned.
Initially the set of vectors Iv[ v ~ g} is clearly empty.
We first claim that as the algorithm proceeds the set
[v[v ~ g } will always be a subset of {v Iv ~ f } . To
prove this it is clearly sufficient to prove the same
statement when both sets are restricted to only total
vectors. Let B be the product of all the clauses c containing up to k literals with the property that "Vv if
v ~ f then v ~ c." It is clear that in the course of the
algorithm no clause in B can ever be deleted. Hence it
is certainly true that {v [ v ~ g} C {v[v ~ B}. To establish the claim, it remains only to prove that B computes
the same Boolean function as f. (In fact B will be the
maximal k-CNF formula equivalent to f.) it is easy to
see that f ~ B, since by the definition of B, for every c
in B it is the case that "for all v if v ~ f then v ~ c." To
verify the converse, that B ~ f, we first note that, by
definition, f is a k-CNF formula. If some clause in f did
not occur in B, then this clause c' would have the
property that "3v such that v ~ f b u t v ~ c'." But this
is impossible since if c' is a clause of f and if v ~-~ c'
then v ~ f. We conclude that every k-CNF representation of the function that f represents consists of a set of
clauses that is a subset of the clauses in B. Hence B ~ f.
Let X = ~ D(v) with summation over all (not necessarily total) vectors v such that v ~ f but v ~ g. This
quantity is defined for each intermediate value of g in
the course of the algorithm and, as is easily verified, it
is monotone decreasing with time. Now clauses will be
removed from g whenever EXAMPLES outputs a vector
v such that v ~ g. The probability of this happening at
any moment is exactly the current value of X. Also, the
process of running the algorithm to completion can
have one of just two possible outcomes: (1) At some
point X becomes less than h -I, in which case the g
found will approximate to f exactly as required by the
definition of learnability. (2) The value of X never goes
below h -1, and hence g is not an acceptable approximation. The probability of the latter eventuality is, how-

November 1984 Volume 27 Number 11

Research Contributions

ever, at most h-1 since it corresponds to the situation of
performing L(h, (2t) k+l) Bernoulli experiments (i.e., calls
of EXAMPLES) each with probability greater than h -1
of success (i.e., finding a v such that v ~ g) and obtaining fewer than (2t) k+l successes (a success being manifested by the removal of at least one clause
from g).
[]
In conclusion we observe that a CNF expression g
immediately yields a program for computing the associated concept. Given a vector v, we substitute the determined truth values in g. The concept will be true for v
if and only if all the clauses are made true by the
substitution.
6. DNF EXPRESSIONS
A disjunctive normal form (DNF) expression is any sum
ml + m2 + • • • + mr of monomials where each monomial mi is a product of literals. For example, pl/~2 + plp3p4
is a DNF expression. Such expressions appear particularly easy for humans to comprehend. Hence we expect
that any practical learning system would have to allow
for them.
An expression is monotone if no variable is negated in
it. We shall show that for monotone DNF expressions
there exists a simple deduction procedure that uses
both EXAMPLES and ORACLE. For unrestricted DNF a
similar result can be proved with respect to a different
size measure. In the unrestricted case, there is the additional difficulty that we can guarantee to deduce a program only for the function and not for the concept. This
difficulty does not arise in the monotone case where
given an expression we can always compute the value
of the associated concept.for a vector v by making the
substitution and asking w h e t h e r the resulting expression is identically true.
A monomial m is a prime implicant of a function F (or
of an expression representing the function) if m ~ F
and if m ' ~,~ F for any m ' obtained by deleting one
literal from m. A DNF expression is prime if it consists
of the sum of prime implicants none of which is redundant in the sense of being implied by the sum of the
others. There is always a unique prime DNF expression
in the monotone case, but not in general. We therefore
define the degree of a DNF expression to be the largest
number of monomials that a prime DNF expression
equivalent to it can have. The unique prime DNF
expression in the monotone case is simply the sum of
all the prime implicants.
THEOREM B: The class of monotone DNF expressions is
learnable via an algorithm B that uses L = L(h,d) calls of
EXAMPLES and dt calls of ORACLE, where d is the degree
of the DNF expression f to be learned and t the number of
variables.
Proof: The algorithm is initialized with formula g
identically equal to the constant zero. The algorithm
then calls EXAMPLES L times to produce positive examples of f. Each time a vector v is produced such that
v ~ g, a new monomial m is added to g. The monomial
m is the product of those literals determined in v that
are essential to make v ~,~ f. More precisely, the loop

November 1984 Volume 27 Number 11

that is executed L times is the following:
begin v := EXAMPLES
if v ~ g then
begin for i := 1 to t do
if pi is determined in v then
begin set ~ equal to v but with pi := *;
if ORACLE(W) = 1 then v :=
end
set m equal to the product of all literals q
such that v ~ q;
g:=g+m
end
end
The test v ~ g amounts to asking w h e t h e r none of
the monomials of g are made true by the values determined to be true in v. Every time EXAMPLES produces
a value v such that v ~ g, the inner loop of the algorithm will find a prime implicant m to add to g. Each
such m is different from any previously added (since
the contrary would have implied that v ~ g). It follows
that such a v will be found at most d times, and hence
ORACLE will he called at most dt times.
Let X = ~ D(w) with summation over all (not necessarily total) vectors w such that w ~ f but w ~ g. This
quantity is defined for each intermediate value of g in
the course of the algorithm, is initially unity, and decreases monotonically with time. Now a monomial will
be added to g each time EXAMPLES outputs a vector v
such that v ~-~ g. The probability of this occurring at
any call of EXAMPLES is exactly X. The process of
running the algorithm to completion can have just two
possible outcomes: (1) At some time X has become less
than h -1, in which case the final expression g found
will approximate to f as required by the definition of
learnability; and (2) the value of X never goes below
h -1, and hence g is not an acceptable approximation.
The probability of this second eventuality is, however,
at most h -1, since it corresponds to the situation of
performing L(h,d) Bernoulli experiments (i.e., calls of
EXAMPLES} each with probability greater than h-1 of
success (i.e., of finding a v such that v ~ f but v ~ g)
and obtaining fewer than d successes (each manifested
by the addition of a new monomial).
[]
For unrestricted DNF expressions, several problems
arise. The main source of difficulty is the fact that the
problem of determining w h e t h e r a nontotal vector implies the function specified by a DNF formula is NPhard. This is simply because the problem of determining whether the nowhere determined vector implies
the function is the tautology question of Cook [3]. An
immediate consequence of this is that it is unreasonable to assume that the program being learned computes
concepts rather than functions. Another consequence
is that in any algorithm akin to Algorithm B the test
v ~ g may not be feasible if v is not total. Algorithm B
is sufficient, however, to establish the following:
THEOREM B': Suppose the notion of learnability is restricted to distributions D such that D(v) = 0 whenever v is
not total. Then the class of DNF expressions is learnable, in

Communications of the ACM

1139

Research Contributions
the sense that programs for the functions (but not necessarily the concepts) can be deduced in L(h,d) calls of EXAMPLES and dt calls of ORACLE where d is the degree of the
DNF expression to be learned.
The reader should note that here there is the additional philosophical difficulty that availability of the
expression does not in itself imply a polynomial time
algorithm for ORACLE. On the other hand, the theorem
does say that if an agent has some, maybe ad hoc, black
box for ORACLE whose workings are unknown to him
or her, it can be used to teach someone else a DNF
expression that approximates the function.
Finally, it may be worth emphasizing that the monotone case is nonproblematic and the deduction procedure for it very simpleminded. It may be interesting to
pursue more sophisticated deduction procedures for it
if contexts can be found in which they can be proved
advantageous. The question as to w h e t h e r monotone
DNF expressions can be learned from EXAMPLES alone
is open. A positive answer would be especially significant. A partial result in this direction can be obtained
by dualizing Theorem A: For any k the class of DNF
expressions having a bound k on the length of each
disjunct can be learned in polynomial time from negative examples alone (i.e., from a source of vectors v
such that/3(v) # 0).

7. a-EXPRESSIONS
We have already seen a class of expressions, namely
the k-CNF expressions, that can be learned from positive examples alone. Here we shall consider the other
extreme, a class that can be learned using oracles
alone.
Deducing expressions on which there are less severe
structural restrictions than DNF or CNF appears much
more difficult. The aim of this section is to give a paradigmatic example of how far one can go in that direction if one is willing to pay the price of oracles that are
more sophisticated than the one previously described.
A general expression over variable s e t {pl . . . . . pt} is
defined recursively as follows:
1.

For each i (1 _< i _< t) "pl" and "Pi" are expressions;

2.

iff~ . . . . . fr are expressions, then "(fl + f2 + .-" +
fr)" is an expression (called a plus expression);

3.

if fl . . . . . fi are expressions, then "(fl x f2 x . . . x
fr)" is an expression (called a times expression).

A a-expression is an expression in which each variable appears at most once. We can assume that a r expression is monotone, since we can always relabel
negated variables with new names that denote their
negation. In the recursive definition of any juexpression, there are clearly at most 2t - 1 intermediate expressions of which at most t are of type (1) and at
most t - 1 of types (2) or (3). Without loss of generality,
we shall assume that in the definition of a t - e x p r e s s i o n
rules (2) and (3) alternate. We shall regard two/zexpressions as identical if their definitions can be made
formally the same by reordering sums and products and
relabeling as necessary.

1140

Communicationsof the ACM

For learning a-expressions, we shall employ more
powerful oracles. The boundary between reasonable
and unreasonable oracles does not appear sharp. We
make no claims about the reasonableness of these new
oracles except that they may serve as vehicles for understanding learnability.
The definitions refer to the Boolean function F (not
regarded as a concept here). The oracle of Section 2 will
be renamed N-ORACLE, since it is one of necessity: NORACLE(v) = 1 if and only if for all total vectors w
such that w ~ v it is the case that F(w) = 1. The dual of
this would be a possibility oracle: P-ORACLE(v) = 1 if
and only if there exists a total vector w such that w ~ v
and F(w} = 1. For brevity we shall also define the prime
implicant oracle: Pl(v) = 1 if and only if N-ORACLE(v) =
1, but N-ORACLE(w) = 0 for any w obtained from v by
making one variable determined in v undetermined.
Finally we define two further oracles, ones of relevant
possibility RP and of relevant accompaniment RA. For convenience we define the first by how it behaves on vectors represented as monomials: RP(m) = 1 iff from some
monomial m' ram' is a prime implicant off. For sets
V, W of variables, we define RA(V, W) = 1 iff every
prime implicant o f f that contains a variable from V also
contains a variable from W.
THEOREM C: The class of in-expressions in learnable via
a deduction procedure C that uses O(t3) calls of N-ORACLE
RP and RA altogether, where t is the number of variables
(and no calls of EXAMPLES). The procedure always deduces
exactly the correct expression.
Proof: Let ~ be the monomial that is the product of no
literals. The algorithm will first compute RP{pj) for each
pj to determine which of the variables occur in the
prime implicants of the function that the expression f to
be learned represents. Let gl . . . . . gr be distinct single
variable expressions, one for each pj for which RP(p/) =
1. Note that these are exactly the variables that occur
in f, since f is monotone.
With each gi we associate two monomials mi and rhi.
The former will be the single variable pj that gi represents. The latter is defined as any monomial rh having
no variable in common with m~ such that m~rh is a
prime implicant off. The algorithm will construct each
such rhi as the final value of m in the following procedure: Set m = ~; while PI(mmi) = 0 find a p~ not in mini
such that RP(pkmmi) = 1 and set m := pkm. Hence in at
most t 2 calls of PI (i.e., t 3 calls of N-ORACLE) and t 2
calls of RP values for every rhi will be found.
Once initialized the algorithm proceeds by alternately executing a plus-phase and a times-phase. At the
start of each phase, we have a set of expressions gl . . . . .
gr where each gi is associated with two monomials mi
and rhi (having no variables in common) where mi is a
prime implicant ofgi and mirhi is a prime implicant off.
We distinguish the gs as plus or times expressions according to w h e t h e r the outermost construction rule was
addition or multiplication. A plus-phase will first compute an equivalence relation S on the subset of {gi} that
are times expressions. For each equivalence class G
such that no gi E G already occurs as a s u m m a n d in a

November 1984 Volume 27 Number 11

Research Contributions
sum, we construct a new expression that is the sum of
the members of G and call this sum gk where k is a
previously unused index. If some members of G already
occur in a sum, say gj (N.B. they are never distributed
in more than one sum), then we modify the sum
expression & to equal the sum of every expression in G.
A times-phase is exactly analogous except that it computes a different equivalence relation T, now on plus
expressions, and will form new, or extend old, times
expressions. In the above context, single variable
expressions will be regarded as both times and plus
expressions. Also, it is immaterial which of the two
kinds of phase is used to start the algorithm.
The intention of the algorithm is best expressed by
the claim to follow, which will be verified later. Suppose that the expressions occurring in the definition o f f
are f~. . . . . fq (where q _< 2t - 1}. We shall say that g _<fi
iff the set of prime implicants of g is a subset of the set
of prime implicants of fi where (1) iffi is a plus expression then fi = fi and (2) iffi is a times expression then f~
is the product of some subset of the multiplicands off~.
Claim 1: After every phase of the algorithm for every
g~ that has been constructed, there is exactly one
expression fi such that (1) gi -< fi and (2) fi is of the same
kind (i.e., plus or times) as gi.
The procedure builds up the rooted tree of the
expression as rooted subtrees starting from the leaves.
One evident difficulty is that there is no a priori knowledge available about the shape of the tree. Hence in the
grafting process a subtree may become attached to another at just about any node.
Whenever a plus or times expression gi is created or
enlarged, its associated mi and thi are updated as follows. If gi is a sum, then we let mi = mj and rhi = ~j for
any & that is a summand in g~. Ifg~ is a product, then m~
will be the product of all the mjs that correspond to
multiplicands in gi. Finally rhi will be generated as the
final value of m in the following procedure: Set m := ~;
while Pl(mmi) = 0 find a pk not in mini such that
RP(pkmmi) = 1 and set m := pkm. Since such an rhi has to
be found at most t times in the overall algorithm, the
total cost is at most t 2 calls of PI (i.e., t 3 calls of NORACLE) and t 2 calls of RP.
In order to complete the description of the algorithm,
it remains only to define the equivalence relations S
and T.
Definition:

giS& if and only if

1. PI(rnirhj) = PI(mjrhi) = 1 and
2. mi, ~j contain disjoint sets of variables, as do mj, mi.
For defining T we shall denote by Vi the set of variables that occur in the expression gi.
Definition:

giTgj if and only if RA(Vi, Vj) = RA(V# Vi)

First we shall verify that S and T are indeed equivalence relations under the assumption that Claim 1
holds at the beginning of each phase. Clearly S and T

November 1984 Volume 27 Number 11

are defined to be both reflexive and symmetric. To
verify that T is transitive, suppose that giTgj and &Tgk.
The former implies that every prime implicant of f containing some variable from gi also contains some variable from &. The latter implies that every prime implicant o f f containing some variable from & also contains
a variable from gk. Hence giTgk follows. In order to verify the transitivity of S, we shall make a more general
observation about any subexpression off.
Claim 2: If mi, mj are prime implicants of distinct
times subexpressions fi, fj o f f and if for some m, with no
variable in common with either mi or mj, both mira and
mjm are prime implicants off, then fi, fj must occur as
summands in the same plus expression off.
Proof: Most easily verified by representing expression
f as a directed graph (e.g., [8]) with edges labeled by the
Boolean variables. The sets of labels along directed
paths between a distinguished source node and a distinguished sink node correspond exactly to prime implicants in the case of # expressions where all edges
will have different labels.
Now to verify that S is transitive, suppose that g~S&
and gjSgk where, by Claim 1, gi -< fi, & -< f# and gk ~ fk
for appropriate times expressions fi, fj, fk. Then it follows that mini, mjrhj, and mkrhj are all prime implicants
o f f where mi, m# mk have no variable in common with
rhj. It follows from Claim 2 that fi, fj, and fk are addends
in the same sum expression. Hence giSgk follows.
Claim 3: Suppose that after some phase of the algorithm Claim 1 holds and times expressions g~, gj have
been formed where gi -< fi, & -< ~, and fi, fj are times
expressions. Then gi and & will be placed in the same
sum expression at the next plus-phase if and only iffi =
fi, fj = )~, and ffi, fj are addends in the same sum expression in f.
Proof: (~) If gi -< fi = fi, & <- fj -- )~j, and fi, fj are in the
same sum, then mi, mj will be prime implicants off,, ~.
Also rnithj and mjrhi will be prime implicants off, and
the variables in mi will be disjoint from those in rhj as
will be those in mj from those in rhi. Hence giSgj will
hold and gi,gj will he in the same plus expression after
the next plus-phase.
(~) Suppose that gi -< fi, gj -~ fj, and giSgj holds. Then
mi, rnj will be prime implicants of gi, &, respectively,
m~rhj, rnffhj will both be prime implicants off, and the
variables in rhj will be disjoint from those of both mi
and mj. It follows from Claim 2 that fi = 1~ and fj = )~j
must occur as summands in the same plus expression
off. [N.B. Here we are using the fact that Claim 2
remains true if we allow a "times subexpression" to be
the product of any subset of the multiplicands in a
times expression in f.]
Claim 4: Suppose that after some phase of the algorithm, Claim 1 holds and some plus expressions gi -< f~
and gj _<fj ~fi, fj plus expressions) have been found. (1) If
gi = fi, & = ~, and fi, fi are multiplicands in the same
times expression in f, then &, & will be placed in the
same times expression after the next times phase. (2} If

Communications of the ACM

1141

Research Contributions

gi, g] are placed in the same times expression at any
subsequent phase, then fi, ~ are in the same product in

f.
(1) If the conditions of (1) above hold, then giT&
will be discovered at the next times-phase and the
claim follows. (2) Iffi, J~ are not in the same product in f,
then f contains some prime implicant containing variables from one of gi, & and not from the other. Hence
giTgj will never hold.
Proof:

P r o o f o f C l a i m 1:

By induction on the n u m b e r of
phases on Claims 1, 3, and 4 simultaneously.
We define the depth of a formula fi to be the maxim u m n u m b e r of alternations of sum and product required in its definition.
Iffi is an expression of depth k in f, then after
k phases a gi identical to fi will have been constructed
by the algorithm.
C l a i m 5:

Proof:

By induction on Claims 3 and 4.

To conclude the proof of the theorem, it remains to
analyze the runtime. This is dominated by the cost of
computing mi and rhi, for which we have already accounted, plus the cost of computing the equivalence
relations S and T. We first note that fewer than 2t
expression names gi are used in the course of the algorithm. When an expression is grafted into another at a
point deep in the latter, then the semantics of all the
subexpressions above will change. By Claims 3 and 4
such grafting can occur w h e n a gi is added to a sum
expression but not when added to a times expression.
Hence the values mi and rhi do not need to be changed
for any expression w h e n such grafting occurs. It follows
that for computing S only 2t values of m~, rh~ need to be
considered. Hence 0(t 2) calls of PI or 0(t a) calls of NORACLE suffice overall. For computing T grafting may
cause a ripple effect. On each occasion the value of Vj
may have to be updated for up to t such sets, and hence
t 2 calls of RA will suffice. Hence 0(t a) calls of RA in the
overall algorithm will be enough.
[]
8. R E M A R K S

In this paper we have considered learning as the process of deducing a program for performing a task, from
information that does not provide an explicit description of such a program. We have given precise meaning
to this notion of learning and have shown that in some
restricted but nontrivial contexts it is computationally
feasible.
Consider a world containing robots and elephants.
Suppose that one of the robots has discovered a recognition algorithm for elephants that can be meaningfully
expressed in k-conjunctive normal form. Our Theorem
A implies that this robot can communicate its algorithm to the rest of the robot population by simply
exclaiming "elephant" whenever one appears.
An important aspect of our approach, if cast in its
greatest generality, is that we require the recognition
algorithms of the teacher and learner to agree on an
overwhelming fraction of only the natural inputs. Their

1142

Communications of the ACM

behavior on u n n a t u r a l inputs is irrelevant, and hence
descriptions of all possible worlds are not necessary. If
followed to its conclusion, this idea has considerable
philosophical implications: A learnable concept is nothing more than a short program that distinguishes some
natural inputs from some others. If such a concept is
passed on among a population in a distributed manner,
substantial variations in meaning may arise. More importantly, what consensus there is will only be
meaningful for natural inputs. The behavior of an individual's program for u n n a t u r a l inputs has no relevance.
Hence thought experiments and logical arguments involving u n n a t u r a l hypothetical situations may be
meaningless activities.
The second important aspect of the formulation is
that the notion of oracles makes it possible to discuss a
whole range of teacher-learner interactions beyond the
mere identification of examples. This is significant in
the context of artificial intelligence where h u m a n s may
be willing to go to great lengths to convey their skills to
machines but are frustrated by their inability to articulate the algorithms they themselves use in the practice
of the skills. We expect that some explicit programming
does become essential for transmitting skills that are
beyond certain limits of difficulty. The identification of
these limits is a major goal of the line of work proposed
in this paper.
REFERENCES

1. Angluin,D., Smith,C.H. Inductiveinference:Theory and methods.
Comput. Surv. 15, 3 (Sept. 1983),237-269.
2. Barr, A., and Feigenbaum,E.A. The Handbook of Artificial Intelligence.
Vol. 2. WilliamKaufmann,Los Altos,Calif., 1982.
3. Cook,S.A.The complexityof theorem provingprocedures. In Proceedings of 3rd Annual ACM Symposium on Theory of Computing
(Shaker Heights, Ohio, May 3-5). ACM,New York, 1971,151-158.
4. Duda,R.O., and Hart, P.F. Pattern Classification and Scene Analysis.
Wiley, New York. 1973.
5. ErdSs,P., and Spencer, J. Probabilistic Methods in Combinatorics. Academic Press, New York, 1974.
6. Goldreich,O., Goldwasser,S., and Micali,S. How to construct random functions.In Proceedings of 25th IEEE Symposium on Foundations
of Computer Science (SingerIsland,Fla., Oct. 24-26).IEEE,New York,
1984.
7. Michalski,R.S..Carbonell,J.G.,and Mitchell,T.M.Machine Learning:
An Artificial Intelligence Approach. TiogaPublishingCo., Palo Alto,
Calif., 1983.
8. Skyum,S., and Valiant,L.G.A complexitytheory basedon Boolean
algebra. In Proceedings of 22rid IEEE Symposium on Foundations of
Computer Science (Nashville,Tenn.,Oct. 28-30}.IEEE,New York,
1981, 244-253.
9. Valiant,L.G.Deductivelearning.Philosophical Transactions of the
Royal Society of London (1984).To be published.
CR Categories and Subject Descriptors: F.1.1 [Computation by Abstract Devices[: Modelsof Computation;F.2.2[Analysis of Algorithms
and Problem Complexity[: NonnumericalAlgorithmsand Problems;
L2.6 [Artificial Intelligence]: Learning--concept learning
General Terms: Algorithms
Additional Key Words and Phrases: inductiveinference,probabilis-

tic modelsof learning,propositionalexpressions
Received 1/84; revised6/84; accepted 7/84
Author's Present Address:L.G.Valiant,Center for Research in Computing Technology.AikenComputationLaboratory,Harvard University,
Cambridge, MA02138.
Permissionto copy without fee all or part of this material is granted
provided that the copiesare not made or distributedfor direct commercial advantage, the ACM copyrightnoticeand the title of the publication
and its date appear, and noticeis giventhat copyingis by permissionof
the Associationfor ComputingMachinery.To copy otherwise, or to
republish, requires a fee and/or specificpermission.

November 1984

Volume 27 Number 11

