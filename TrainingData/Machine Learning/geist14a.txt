Journal of Machine Learning Research 15 (2014) 289-333

Submitted 9/11; Revised 4/13; Published 1/14

Off-policy Learning With Eligibility Traces: A Survey
Matthieu Geist

matthieu.geist@supelec.fr

IMS-MaLIS Research Group & UMI 2958 (GeorgiaTech-CNRS)
Supélec
2 rue Edouard Belin
57070 Metz, France

Bruno Scherrer

bruno.scherrer@inria.fr

MAIA project-team
INRIA Lorraine
615 rue du Jardin Botanique
54600 Villers-lès-Nancy, France

Editor: Ronald Parr

Abstract
In the framework of Markov Decision Processes, we consider linear off-policy learning,
that is the problem of learning a linear approximation of the value function of some fixed
policy from one trajectory possibly generated by some other policy. We briefly review
on-policy learning algorithms of the literature (gradient-based and least-squares-based),
adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting
them to off-policy learning with eligibility traces. This leads to some known algorithms—
off-policy LSTD(λ), LSPE(λ), TD(λ), TDC/GQ(λ)—and suggests new extensions—offpolicy FPKF(λ), BRM(λ), gBRM(λ), GTD2(λ). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their
known convergence properties and illustrate their relative empirical behavior on Garnet
problems. Our experiments suggest that the most standard algorithms on and off-policy
LSTD(λ)/LSPE(λ)—and TD(λ) if the feature space dimension is too large for a leastsquares approach—perform the best.
Keywords:
bility traces

reinforcement learning, value function estimation, off-policy learning, eligi-

1. Introduction
We consider the problem of learning a linear approximation of the value function of some
fixed policy in a Markov Decision Process (MDP) framework. This study is performed in
the most general situation where learning must be done from a single trajectory possibly
generated by some other policy, also known as off-policy learning. Given samples, well-known
methods for estimating a value function are temporal difference (TD) learning and Monte
Carlo (Sutton and Barto, 1998). TD learning with eligibility traces (Sutton and Barto,
1998), known as TD(λ), constitutes a nice bridge between both approaches; by controlling
the bias/variance trade-off (Kearns and Singh, 2000), their use can significantly speed up
learning. When the value function is approximated through a linear architecture, the depth
c 2014 Matthieu Geist and Bruno Scherrer.

Geist and Scherrer

λ of the eligibility traces is also known to control the quality of approximation (Tsitsiklis
and Van Roy, 1997). Overall, the use of these traces often plays an important practical role.

bootstrapping
residual
projected fixed point

gradient-based
TD
(Sutton and Barto, 1998)
gBRM
(Baird, 1995)
TDC/GTD2
(Sutton et al., 2009)

least-squares-based
FPKF
(Choi and Van Roy, 2006)
BRM (Engel, 2005)
(Geist and Pietquin, 2010b)
LSTD (Bradtke and Barto, 1996)
LSPE (Nedić and Bertsekas, 2003)

Table 1: Taxonomy of linearly parameterized estimators for value function approximation
(Geist and Pietquin, 2013).

There has been a significant amount of research on parametric linear approximation of
the value function, without eligibility traces (in the on- or off-policy case). We follow the
taxonomy proposed by Geist and Pietquin (2013), briefly recalled in Table 1 and further
developed in Section 2. Value function approximators can be categorized depending on the
cost function they minimize (based on bootstrapping, on a Bellman residual minimization or
on a projected fixed point approach) and on how it is minimized (gradient descent or linear
least-squares). Most of these algorithms have been extended to take into account eligibility
traces, in the on-policy case. Works on extending these eligibility-trace approaches to offpolicy learning are scarcer. They are summarized in Table 2 (algorithms in black). The
first motivation of this article is to argue that it is conceptually simple to extend all the
algorithms of Table 1 so that they can be applied to the off-policy setting and use eligibility
traces. If this allows re-deriving existing algorithms (in black in Table 2), it also leads to
new candidate algorithms (in red in Table 2). The second motivation of this work is to
discuss the subtle differences between these intimately-related algorithms, and to provide
some comparative insights on their empirical behavior (a topic that has to our knowledge
not been considered in the literature, even in the simplest on-policy and no-trace situation).

bootstrapping
residual
projected fixed point

gradient-based
off-policy TD(λ)
(Bertsekas and Yu, 2009)
off-policy gBRM(λ)
GQ(λ) a.k.a. off-policy TDC(λ)
(Maei and Sutton, 2010)
off-policy GTD2(λ)

least-squares-based
off-policy FPKF(λ)
off-policy BRM(λ)
off-policy LSTD(λ)
off-policy LSPE(λ)
(Yu, 2010a)

Table 2: Surveyed off-policy and eligibility-traces approaches. Algorithms in black have
been published before (provided references), algorithms in red are new.

290

Off-policy Learning with Traces

The rest of this article is organized as follows. Section 2 introduces the background
of Markov Decision Processes, describes the state-of-the-art algorithms for learning without
eligibility traces, and gives the fundamental idea to extend the methods to the off-policy situation with eligibility traces. Section 3 details this extension for the least-squares approaches:
the resulting algorithms are formalized, and we derive recursive and memory-efficient formula for their implementation (this allows online learning without loss of generality, all the
more that half of these algorithms are recursive by their very definition), and we discuss
their convergence properties. Section 4 does the same job for stochastic gradient approaches,
which offers a smaller computational cost (linear per update, instead of quadratic). Last
but not least, Section 5 describes an empirical comparison and Section 6 concludes.

2. Background
We consider a Markov Decision Process (MDP), that is a tuple {S, A, P, R, γ} in which S
is a finite state space identified with {1, 2, . . . , N }, A a finite action space, P ∈ P(S)S×A
the set of transition probabilities, R ∈ RS×A the reward function and γ the discount factor.
A mapping π ∈ P(A)S is called a policy. For any policy π, let P π be the corresponding
stochastic transition matrix, and Rπ the vector of mean reward when following π, that is
of components Ea|π,s [R(s, a)]. The value V π (s) of state s for a policy π is the expected
discounted cumulative reward starting in state s and then following the policy π:
∞

V π (s) = Eπ

γ i ri |s0 = s ,
i=0

where Eπ denotes the expectation over trajectories induced by policy π. The value function
satisfies the (linear) Bellman equation:
∀s, V π (s) = Es ,a|s,π [R(s, a) + γV π (s )].
It can be rewritten as the fixed-point of the Bellman evaluation operator: V π = T π V π where
for all V, T π V = Rπ + γP π V .
In this article, we are interested in learning an approximation of this value function V π
under some constraints. First, we assume our approximation to be linearly parameterized:
∀s, Vˆθ (s) = θT φ(s)
with θ ∈ Rp being the parameter vector and φ(s) ∈ Rp the feature vector in state s. This
encompasses notably the tabular case (exact representation of the value function). Also, we
want to estimate the value function V π (or equivalently the associated parameter θ) from a
single finite trajectory generated using a possibly different behavioral policy π0 .1 Let µ0 be
the stationary distribution of the stochastic matrix P0 = P π0 of the behavior policy π0 (we
assume it exists and is unique). Let D0 be the diagonal matrix of which the elements are
(µ0 (si ))1≤i≤N . Let Φ be the matrix of feature vectors:
Φ = [φ(1) . . . φ(N )]T .
1. This can be easily extended to multiple finite trajectories.

291

Geist and Scherrer

As we consider a linear approximation, the considered value functions belong to the space
spanned by Φ. The projection Π0 onto this hypothesis space with respect to the µ0 -quadratic
norm, which will be central for the understanding of the algorithms, has the following closedform:
Π0 = Φ(ΦT D0 Φ)−1 ΦT D0 .
If π0 is different from π, it is called an off-policy setting. Notice that all algorithms considered in this paper use this Π0 projection operator, that is the projection according to the
observed data.2 It would certainly be interesting to consider the projection according to
the stationary distribution of π, the (unobserved) target policy: this would reduce off-policy
learning to on-policy learning. However, this would require re-weighting samples according
to the stationary distribution of the target policy π, which is unknown and probably as
difficult to estimate as the value function itself.
2.1 Standard Algorithms For On-policy Learning Without Traces
We now review existing on-policy linearly parameterized temporal difference learning algorithms (see Table 1). In this case, the behavior and target policies are the same, so we
omit the subscript 0 for the policy (π) and the projection (Π). We assume that a trajectory
(s1 , a1 , r1 , s2 , . . . , si , ai , ri , si+1 , . . . , sn , an , rn , sn+1 ) sampled according to the policy π is
available, and will explain how to compute the ith iterate for several algorithms. For all
j ≤ i, let us introduce the empirical Bellman operator at step j:
Tˆj : RS → R
V → rj + γV (sj+1 )
so that Tˆj V is an unbiased estimate of T V (sj ).
Projected fixed point approaches aim at finding the fixed-point of the operator being
the composition of the projection onto the hypothesis space and the Bellman operator.
In other words, they search for the fixed-point Vˆθ = ΠT Vˆθ , Π being the just introduced
projection operator. Solving the following fixed-point problem,
i

Tˆj Vˆθi − Vˆω (sj )

θi = argmin
ω∈Rp

2

,

j=1

with a least-squares approach corresponds to the Least-Squares Temporal Differences (LSTD)
algorithm of Bradtke and Barto (1996). Recently, Sutton et al. (2009) proposed two algorithms reaching the same objective, Temporal Difference with gradient Correction (TDC)
and Gradient Temporal Difference 2 (GTD2), by performing a stochastic gradient descent
of the function θ → Vˆθ − ΠT Vˆθ 2 which is minimal (and equal to 0) when Vˆθ = ΠT Vˆθ .
2. As far as we know, there are two notable exceptions. Precup et al. (2001) propose an algorithm that
updates parameters according to full trajectories (not according to transitions, as all approaches to be
reviewed next). Therefore, the distribution weighting the projection operator is the one of the starting
states of these trajectories instead of the one involved by the behavioral policy. Another work to move
in a different direction is the off-policy approach of Kolter (2011): samples are weighted such that the
projection operator composed with the Bellman operator is non-expansive: this is weaker than finding
the projection of the stationary distribution, but offers some guarantees. In this article, we consider only
the Π0 projection.

292

Off-policy Learning with Traces

A related approach consists in building a recursive algorithm that repeatedly mimics the
iteration Vˆθi ΠT Vˆθi−1 . In practice, we aim at minimizing
i

2

Tˆj Vˆθi−1 − Vˆω (sj )

ω→

.

j=1

Performing the minimization exactly through a least-squares method leads to the LeastSquares Policy Evaluation (LSPE) algorithm of Bertsekas and Ioffe (1996). If this minimization is approximated by a stochastic gradient descent, this leads to the classical Temporal
Difference (TD) algorithm (Sutton and Barto, 1998).
Bootstrapping approaches consist in treating value function approximation after seeing the ith transition as a supervised learning problem, by replacing the unobserved values
V π (sj ) at states sj by some estimate computed from the trajectory until the transition
(sj , sj+1 ), the best such estimate being Tˆj Vˆθj−1 . This amounts to minimizing the following
function:
i

2

Tˆj Vˆθj−1 − Vˆω (sj )

ω→

.

(1)

j=1

Choi and Van Roy (2006) proposed the Fixed-Point Kalman Filter (FPKF), a least-squares
variation of TD that minimizes exactly the function of Equation (1). If the minimization
is approximated by a stochastic gradient descent, this gives—again—the classical TD algorithm (Sutton and Barto, 1998).
Finally, residual approaches aim at minimizing the distance between the value function
and its image through the Bellman operator, V − T V 2µ0 . Based on a trajectory, this
suggests the following function to minimize
i

Tˆj Vˆω − Vˆω (sj )

ω→

2

,

j=1

which is a surrogate of the objective V −T V 2µ0 that is biased (Antos et al., 2006). This cost
function has originally been proposed by Baird (1995) who minimized it using a stochastic
gradient approach (this algorithm being referred here as gBRM for gradient-based Bellman Residual Minimization). Both the parametric Gaussian Process Temporal Differences
(GPTD) algorithm of Engel (2005) and the linear Kalman Temporal Differences (KTD)
algorithm of Geist and Pietquin (2010b) can be shown to minimize the above cost using a
least-squares approach, and are thus the very same algorithm, that we will refer to as BRM
(for Bellman Residual Minimization) in the remaining of this paper.3
To sum up, it thus appears that after the ith transition has been observed, the above
mentioned algorithms behave according to the following pattern:
i

Tˆj Vˆξ − Vˆω (sj )

move from θi−1 to θi towards the minimum of ω →

2

,

(2)

j=1

either through a least-squares approach or a stochastic gradient descent. Each of the algorithms mentioned above is obtained by substituting θi , θi−1 , θj−1 or ω for ξ.
3. Note that this is only true in the linear case. GPTD and KTD were both introduced in a more general
setting: GPTD is non-parametric and KTD is motivated by the goal of handling nonlinearities.

293

Geist and Scherrer

2.2 Towards Off-policy Learning With Traces
It is now easy to preview, at least at a high level, how one may extend the previously
described algorithms so that they can deal with eligibility traces and off-policy learning.
Eligibility Traces. The idea of eligibility traces amounts to looking for the fixed-point of
the following variation of the Bellman operator (Bertsekas and Tsitsiklis, 1996)
∞

∀V ∈ RS , T λ V = (1 − λ)

λk T k+1 V
k=0

that makes a geometric average with parameter λ ∈ (0, 1) of the powers of the original
Bellman operator T . Clearly, any fixed-point of T is a fixed-point of T λ and vice-versa.
After some simple algebra, one can see that:
T λ V = (I − λγP )−1 (R + (1 − λ)γP V )
−1

= V + (I − λγP )

(3)

(R + γP V − V ).

This leads to the following well-known temporal difference expression in some state s
∞

T λ V (s) = V (s) + Eπ

(γλ)k−i rk + γV (sk+1 ) − V (sk ) si = s
k=i

∞

(γλ)k−i δik (s)

= V (s) +
k=i

where we recall that Eπ means that the expectation is done according to the target policy π, and where δik (s) = Eπ rk + γV (sk+1 ) − V (sk ) si = s is the expected
temporal-difference (Sutton and Barto, 1998). With λ = 0, we recover the Bellman
evaluation equation. With λ = 1, this is the definition of the value function as the
k−i r |s = s].
expected and discounted cumulative reward: T 1 V (s) = Eπ [ ∞
k i
k=i γ
Off-policy Learning. As before, we assume that we are given a trajectory (s1 , a1 , r1 , s2 , . . . ,
sj , aj , rj , sj+1 . . . , sn , an , rn , sn+1 ), except now that it may be generated from some behavior policy possibly different from the target policy π of which we want to estimate
the value. We are going to describe how to compute the ith iterate for several algorithms. For any i ≤ k, unbiased estimates of the temporal difference terms δik (sk ) can
be computed through importance sampling (Ripley, 1987). Indeed, for all s, a, let us
introduce the following weight:
ρ(s, a) =

π(a|s)
.
π0 (a|s)

In our trajectory context, for any j and k, write
k

ρkj

=

ρl with ρl = ρ(sl , al )
l=j

294

Off-policy Learning with Traces

with the convention that if k < j, ρkj = 1. With these notations,
δˆik = ρki Tˆk V − ρk−1
V (sk )
i
λ V of
is an unbiased estimate of δik (si ), from which we may build an estimate Tˆj,i
T λ V (sj ) (we will describe this very construction separately for the least-squares and
the stochastic gradient as they slightly differ).
λ , we get the general
Then, by replacing the empirical operator Tˆj in Equation (2) by Tˆj,i
pattern for off-policy trace-based algorithms:
i
λ ˆ
Tˆj,i
Vξ − Vˆω (sj )

move from θi−1 to θi towards the minimum of ω →

2

,

(4)

j=1

either through a least-squares approach or a stochastic gradient descent after having instantiated ξ = θi , θi−1 , θj−1 or ω. This process, including in particular the precise definition
λ , will be further developed in the next two sections.4 Since
of the empirical operator Tˆj,i
they are easier to derive, we begin by focusing on least-squares algorithms (right column of
Table 2) in Section 3. Then, Section 4 focuses on stochastic gradient-based algorithms (left
column of Table 2).

3. Least-squares Extensions To Eligibility Traces And Off-policy Learning
First, we consider the least-squares solution to the problem described in Equation (4). At
their ith step, the algorithms that we are about to describe will compute the parameter θi
by exactly solving the following problem:
i
λ ˆ
Tˆj,i
Vξ − Vˆω (sj )

θi = argmin
ω∈Rp

2

j=1

where we define the following empirical truncated approximation of Tλ :
λ
Tˆj,i
: RS → R
i

i

(γλ)k−j δˆjk = V (sj ) +

V → V (sj ) +
k=j

(γλ)k−j ρkj Tˆk V − ρk−1
V (sk ) .
j
k=j

Though different definitions of this operator may lead to practical implementations, note
λ only uses samples seen before time i: this very feature—considered by all existing
that Tˆj,i
works in the literature—will enable us to derive recursive and low-memory algorithms.
Recall that a linear parameterization is chosen here, Vˆξ (si ) = ξ T φ(si ). We adopt the
following notations:
φi = φ(si ), ∆φi = φi − γρi φi+1 and ρ˜k−1
= (γλ)k−j ρk−1
.
j
j
λ
4. Note that we let the empirical operator Tˆj,i
depends on the index j of the sample (as before) but also
on the step i of the algorithm. This will be particularly useful for the derivation of the recursive and
memory-efficient least-squares algorithms that we present in the next section.

295

Geist and Scherrer

The generic cost function to be solved is therefore:
i

i

(φTj ξ

θi = argmin J(ω; ξ) with J(ω; ξ) =
ω∈Rp

j=1

ρ˜jk−1 (ρk rk − ∆φTk ξ) − φTj ω)2 .

+

(5)

k=j

Before deriving existing and new least-squares algorithms, as announced, some technical
lemmas are required.
The first lemma allows computing directly the inverse of a rank-one perturbed matrix.
Lemma 1 (Sherman-Morrison) Assume that A is an invertible n × n matrix and that
u, v ∈ Rn are two vectors satisfying 1 + v T A−1 u = 0. Then:
(A + uv T )−1 = A−1 −

A−1 uv T A−1
.
1 + v T A−1 u

The next lemma is simply a rewriting of imbricated sums. However, it is quite important here
λ (operator which depends on future of s , so
as it will allow stepping from the operator Tˆj,i
j
acasual)—forward view of eligibility traces—to the recursion over parameters using eligibility
traces (dependence on only past samples)—backward view of eligibility traces. In other
words, the forward view is a theoretical way of mixing backups that shifts parametrically
(through the choice of λ) from the standard Bellman operator to the Monte Carlo one.
However, it cannot be implemented easily, as it requires knowing the future states. On the
other hand, the backward view, which is equivalent (see notably Lemma 2 and Proposition 6),
is a more mechanistic and convenient viewpoint that allows performing the same updates
using solely information gathered in the states encountered in the past. See Sutton and
Barto (1998, Chapter 7) for further discussion on backward/forward views.
Lemma 2 Let f ∈ RN×N and n ∈ N. We have:
n

n

n

i

f (i, j) =
i=1 j=i

f (j, i)
i=1 j=1

We are now ready to mechanically derive the off-policy algorithms LSTD(λ), LSPE(λ),
FPKF(λ) and BRM(λ). This is what we do in the following subsections.
3.1 Off-policy LSTD(λ)
The Least-Squares Temporal Difference algorithm, that computes directly a fixed-point of
the projected Bellman operator, has originally been introduced in the no-trace and on-policy
case by Bradtke and Barto (1996). It has been extended to eligibility traces by Boyan (1999),
to off-policy (through state-action value function approximation) learning (without traces)
by Lagoudakis and Parr (2003), and to off-policy learning with traces by Yu (2010a).
The off-policy LSTD(λ) algorithm actually corresponds to instantiating Problem (5)
with ξ = θi :
i

θi = argmin
ω∈Rp

i

(φTj θi
j=1

ρ˜jk−1 (ρk rk − ∆φTk θi ) − φTj ω)2 .

+
k=j

296

Off-policy Learning with Traces

This can be solved by zeroing the gradient respectively to ω:
i

i

φj φTj )−1

θi = (
j=1
i

⇔

i

ρ˜k−1
(ρk rk − ∆φTk θi ))
j

φj (φTj θi +
j=1

k=j

i

φj ρ˜k−1
(ρk rk − ∆φTk θi ),
j

0=
j=1 k=j

which, through Lemma 2, is equivalent to:
j

i

0=

T
φk ρ˜j−1
k )(ρj rj − ∆φj θi ).

(
j=1 k=1

Introducing the (importance-based) eligibility vector zj :
j

j

φk ρ˜kj−1

zj =

j−1
j−k

=

k=1

φk (γλ)
k=1

ρm = γλρj−1 zj−1 + φj ,

(6)

m=k

one obtains the following batch estimate:
i

i

zj ∆φTj )−1

θi = (
j=1

where

zj ρj rj = (Ai )−1 bi

i

i

zj ∆φTj

Ai =

(7)

j=1

and bi =

j=1

zj ρj rj .

(8)

j=1

Thanks to Lemma 1, the inverse Mi = (Ai )−1 can be computed recursively:
i

zj ∆φTj )−1 = Mi−1 −

Mi = (
j=1

Mi−1 zi ∆φTi Mi−1
.
1 + ∆φTi Mi−1 zi

This can be used to derive a recursive estimate:
i

i

zj ∆φTj )−1

θi = (
j=1

= θi−1 +

i−1

Mi−1 zi ∆φTi Mi−1
)(
zj rj ρj + zi ρi ri )
zj ρj rj = (Mi−1 −
1 + ∆φTi Mi−1 zi j=1
j=1

Mi−1 zi
(ρi ri − ∆φTi θi−1 ).
1 + ∆φTi Mi−1 zi

Writing the gain Ki =

Mi−1 zi
,
1+∆φT
i Mi−1 zi

this gives Algorithm 1.

This algorithm has been proposed and analyzed by Yu (2010a). The author proves the
following result: if the behavior policy π0 induces an irreducible Markov chain and chooses
with positive probability any action that may be chosen by the target policy π, and if
the compound (linear) operator Π0 T λ has a unique fixed-point, then off-policy LSTD(λ)
297

Geist and Scherrer

Algorithm 1: Off-policy LSTD(λ)
Initialization;
Initialize vector θ0 and matrix M0 ;
Set z0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = γλρi−1 zi−1 + φi ;
Update parameters ;
Mi−1 zi
Ki = 1+∆φ
;
TM
z
i

i−1 i

θi = θi−1 + Ki (ρi ri − ∆φTi θi−1 ) ;
T ∆φ )T ;
Mi = Mi−1 − Ki (Mi−1
i

converges to it almost surely.5 Formally, it converges to the solution θ∗ of the so-called
projected fixed-point equation:
Vθ∗ = Π0 T λ Vθ∗ .
(9)
Using the expression of the projection Π0 and the form of the Bellman operator in Equation (3), it can be seen that θ∗ satisfies (see Yu, 2010a for details)
θ∗ = A−1 b
where
A = ΦT D0 (I − γP )(I − λγP )−1 Φ and b = ΦT D0 (I − λγP )−1 R.

(10)

The core of the analysis of Yu (2010a) consists in showing that 1i Ai and 1i bi defined in
Equation (8) respectively converge to A and b almost surely. Through Equation (7), this
implies the convergence of θi to θ∗ .
3.2 Off-policy LSPE(λ)
The Least-Squares Policy Evaluation algorithm, that computes iteratively the fixed point
of the projected Bellman operator, was originally introduced by Bertsekas and Ioffe (1996)
and first analyzed in an on-policy context by Nedić and Bertsekas (2003). Its extension to
off-policy learning with traces was briefly mentioned by Yu (2010a).
The off-policy LSPE(λ) algorithm corresponds to instantiate ξ = θi−1 in Problem (5):
i
ω∈Rp

i

ρ˜jk−1 (ρk rk − ∆φTk θi−1 ) − φTj ω)2 .

(φTj θi−1 +

θi = argmin
j=1

k=j

5. It is not always the case that Π0 T λ has a unique fixed-point, see Tsitsiklis and Van Roy (1997) for a
counter-example.

298

Off-policy Learning with Traces

This can be solved by zeroing the gradient respectively to ω:
i

i

i

φj φTj )−1

θi = (
j=1

φj (φTj θi−1
j=1

k=j

i

i

i

φj φTj )−1

= θi−1 + (
j=1

ρ˜k−1
(ρk rk − ∆φTk θi−1 ))
j

+

φj ρ˜k−1
(ρk rk − ∆φTk θi−1 ).
j
j=1 k=j

Using Lemma 2 and the definition of the eligibility vector zj in Equation (6), we get:
i

j

i

T
φk ρ˜j−1
k (ρj rj − ∆φj θi−1 )

φj φTj )−1

θi = θi−1 + (
j=1

j=1 k=1

i

i

φj φTj )−1

= θi−1 + (
j=1

zj (ρj rj − ∆φTj θi−1 ).
j=1

Define the matrix Ni as follows:
i

φj φTj )−1 = Ni−1 −

Ni = (
j=1

Ni−1 φi φTi Ni−1
,
1 + φTi Ni−1 φi

(11)

where the second equality follows from Lemma 1. Let Ai and bi be defined as in the LSTD
description in Equation (8). For clarity, we restate their definition along with their recursive
writing:
i

zj ∆φTj = Ai−1 + zi ∆φTi+1

Ai =
j=1
i

bi =

zj ρj rj = bi−1 + zi ρi ri .
j=1

Then, it can be seen that the LSPE(λ) update is:
θi = θi−1 + Ni (bi − Ai θi−1 ).
The overall computation is provided in Algorithm 2.
This algorithm, (briefly) mentioned by Yu (2010a), generalizes the LSPE(λ) algorithm of
Bertsekas and Ioffe (1996) to off-policy learning. With respect to LSTD(λ), which computes
θi = (Ai )−1 bi at each iteration as stated in Equation (7), LSPE(λ) is fundamentally recursive
(as it is based on an iterated fixed-point relation). Along with the almost sure convergence
of 1i Ai and 1i bi to A and b defined in Equation (10), it can be shown that iNi converges
to N = (ΦT D0 Φ)−1 —see for instance Nedić and Bertsekas (2003)—so that, asymptotically,
LSPE(λ) behaves as:
θi = θi−1 + N (b − Aθi−1 ) = N b + (I − N A)θi−1
299

Geist and Scherrer

Algorithm 2: Off-policy LSPE(λ)
Initialization;
Initialize vector θ0 and matrix N0 ;
Set z0 = 0, A0 = 0 and b0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = γλρi−1 zi−1 + φi ;
Update parameters ;
Ni = Ni−1 −

Ni−1 φi φT
i Ni−1
1+φT
i Ni−1 φi
zi ∆φTi ;

;

Ai = Ai−1 +
bi = bi−1 + ρi zi ri ;
θi = θi−1 + Ni (bi − Ai θi−1 ) ;

or using the definition of Π0 , A, b from Equation (10) and T λ from Equation (3):
Vθi = Φθi = ΦN b + Φ(I − N A)θi−1 = Π0 T λ Vθi−1 .

(12)

The behavior of this sequence depends on whether the spectral radius of Π0 T λ is smaller
than 1 or not. Thus, the analyses of Yu (2010a) and Nedić and Bertsekas (2003) (for the
convergence of Ni ) imply the following convergence result: under the assumptions required
for the convergence of off-policy LSTD(λ), and the additional assumption that the operator
Π0 T λ has a spectral radius smaller than 1 (so that it is contracting), LSPE(λ) also converges
almost surely to the fixed-point of the compound Π0 T λ operator.
There are two sufficient conditions that can ensure such a desired contraction property.
The first one is when one considers on-policy learning, as Nedić and Bertsekas (2003) did
when they derived the first convergence proof of (on-policy) LSPE(λ). When the behavior
policy π0 is different from the target policy π, a sufficient condition for contraction is that λ
be close enough to 1; indeed, when λ tends to 1, the spectral radius of T λ tends to zero and
can potentially balance an expansion of the projection Π0 . In the off-policy case, when γ
is sufficiently big, a small value of λ can make Π0 T λ expansive (see Tsitsiklis and Van Roy
1997 for an example in the case λ = 0) and off-policy LSPE(λ) will then diverge. Eventually,
Equations (9) and (12) show that when λ = 1, both LSTD(λ) and LSPE(λ) asymptotically
coincide (as T 1 V does not depend on V ).
3.3 Off-policy FPKF(λ)
The Fixed Point Kalman Filter algorithm is a bootstrapped recursive least-squares approach
to value function approximation originally introduced by Choi and Van Roy (2006). Its
extensions to eligibility traces and to off-policy learning are new.
300

Off-policy Learning with Traces

The off-policy FPKF(λ) algorithm corresponds to instantiate ξ = θj−1 in Problem (5):
i

i

(φTj θj−1

θi = argmin
ω∈Rp

ρ˜k−1
(ρk rk − ∆φTk θj−1 ) − φTj ω)2 .
j

+

j=1

k=j

This can be solved by zeroing the gradient respectively to ω:
i

i

φj (φTj θj−1

θi = Ni

ρ˜k−1
(ρk rk − ∆φTk θj−1 )),
j

+

j=1

k=j

where Ni is the matrix introduced for LSPE(λ) in Equation (11). For clarity, we restate its
definition here and its recursive writing:
i

φj φTj )−1 = Ni−1 −

Ni = (
j=1

Ni−1 φi φTi Ni−1
.
1 + φTi Ni−1 φi

(13)

Using Lemma 2, one obtains:
i

i

φj φTj θj−1

θi = N i (
j=1

j
T
φk ρ˜j−1
k (ρj rj − ∆φj θk−1 )).

+
j=1 k=1

With respect to the previously described algorithms, the difficulty here is that on the right
side there is a dependence with all the previous terms θk−1 for 1 ≤ k ≤ i. Using the symmetry
T ∆φ , it is possible to write a recursive algorithm by
of the dot product ∆φTj θk−1 = θk−1
j
introducing the trace matrix Zj that integrates the subsequent values of θk as follows:
j
T
T
ρ˜j−1
k φk θk−1 = Zj−1 + γλρj−1 φj θj−1 .

Zj =
k=1

With this notation we obtain:
i

i

φj φTj θj−1 +

θi = N i (
j=1

(zj ρj rj − Zj ∆φj )).
j=1

Using Equation (13) and a few algebraic manipulations, we end up with:
θi = θi−1 + Ni (zi ρi ri − Zi ∆φi ).
This is the parameter update as provided in Algorithm 3.
As LSPE(λ), this algorithm is fundamentally recursive. However, its overall behavior is
quite different. As we discussed for LSPE(λ), iNi can be shown to tend asymptotically to
N = (ΦT D0 Φ)−1 and FPKF(λ) iterates eventually resemble:
1
θi = θi−1 + N (zi ρi ri − Zi ∆φi ).
i
301

Geist and Scherrer

Algorithm 3: Off-policy FPKF(λ)
Initialization;
Initialize vector θ0 and matrix N0 ;
Set z0 = 0 and Z0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = γλρi−1 zi−1 + φi ;
T ;
Zi = γλρi−1 Zi−1 + φi θi−1
Update parameters ;
Ni = Ni−1 −

Ni−1 φi φT
i Ni−1
1+φT
i Ni−1 φi

;

θi = θi−1 + Ni (zi ρi ri − Zi ∆φi ) ;

The term in brackets is a random component (that only depends on the previous transitions)
and 1i acts as a learning coefficient that asymptotically tends to 0. In other words, FPKF(λ)
has a stochastic approximation flavor. In particular, one can see FPKF(0) as a stochastic
approximation of LSPE(0). Indeed, asymptotically, FPKF(0) does the following update
1
θi = θi−1 + N (ρi φi ri − φi ∆φTi θi−1 ),
i
and one can notice that ρi φi ri and φi ∆φTi are samples of A and b to which Ai and bi converge
through LSPE(0). When λ > 0, the situation is less clear—up to the fact that since T 1 V
does not depend on V , we expect FPKF to asymptotically behave like LSTD and LSPE
when λ tends to 1.
Due to its much more involved form (notably the matrix trace Zj integrating the values
of all the values θk from the start), it does not seem easy to provide a guarantee for FPKF(λ),
even in the on-policy case. To our knowledge, there does not exist any proof of convergence
for stochastic approximation algorithms in the off-policy case with traces, and a related
result for FPKF(λ) thus seems difficult.6 Based on the above-mentioned relation between
FPKF(0) and LSPE(0) and the experiments we have run (see Section 5), we conjecture that
off-policy FPKF(λ) has the same asymptotic behavior as LSPE(λ). We leave the formal
study of this algorithm for future work.
3.4 Off-policy BRM(λ)
The Bellman Residual Minimization algorithm is a least-squares approach that minimizes
directly the Bellman residual. The off-policy BRM(λ) algorithm corresponds to instantiate
ξ = ω in Problem (5):
6. An analysis of TD(λ), with a simplifying assumption that forces the algorithm to stay bounded is given
by Yu (2010a). An analysis of GQ(λ) is provided by Maei and Sutton (2010), with an assumption on
the second moment of the traces, which—as explained in Proposition 2 of Yu (2010a)—does not hold in
general. A full analysis of these algorithms thus remains to be done. See also Sections 4.1 and 4.2.

302

Off-policy Learning with Traces

i

i

ω∈Rp

j=1

k=j

i

i

= argmin
ω∈Rp

ρ˜jk−1 (ρk rk − ∆φTk ω) − φTj ω)2

(φTj ω +

θi = argmin

ρ˜k−1
(ρk rk − ∆φTk ω))2 .
j

(
j=1 k=j

Define

i

i

ρ˜k−1
∆φk and zj→i =
j

ψj→i =
k=j

ρ˜k−1
ρk rk .
j
k=j

This yields the following batch estimate:
i
T
(zj→i − ψj→i
ω)2 = (A˜i )−1˜bi

θi = argmin
ω∈Rp

where

j=1

i

A˜i =

(14)

i
T
ψj→i ψj→i

and ˜bi =

j=1

ψj→i zj→i .
j=1

The transformation of this batch estimate into a recursive update rule is somewhat tedious
(it involves three “trace” variables), and the details are deferred to Appendix A for clarity.
The resulting BRM(λ) method is provided in Algorithm 4. Note that at each step, this
algorithm involves the inversion of a 2 × 2 matrix (involving the 2 × 2 identity matrix I2 ),
inversion that admits a straightforward analytical solution. The computational complexity
of an iteration of BRM(λ) is thus O(p2 ) (as for the preceding least-squares algorithms).
GPTD and KTD, which are close to BRM, have also been extended with some trace
mechanism; however, GPTD(λ) (Engel, 2005), KTD(λ) (Geist and Pietquin, 2010a) and
the just described BRM(λ) are different algorithms.7 Briefly, GPTD(λ) is very close to
LSTD(λ) and KTD(λ) uses a different Bellman operator.8 As BRM(λ) builds a linear
system whose solution is updated recursively, it resembles LSTD(λ). However, the system
it builds is different. The following theorem, proved in Appendix B, partially characterizes
the behavior of BRM(λ) and its potential limit.9
Theorem 3 Assume that the stochastic matrix P0 of the behavior policy is irreducible and
has stationary distribution µ0 . Further assume that there exists a coefficient β < 1 such that
∀(s, a), λγρ(s, a) ≤ β.

(15)

7. GPTD(λ) is not exactly a generalization of GPTD as it does not reduce to it when λ = 0. It is rather a
technical variation that bridges a gap with the Monte Carlo approach.
0 ˆ
1
0
8. The corresponding loss is (Tˆj,i
V (ω) − Vˆω (sj ) + γλ(Tˆj+1,i
Vˆ (ω) − Vˆω (sj+1 )))2 . With λ = 0 it gives Tˆj,i
1
ˆ
and with λ = 1 it provides Tj,i .
9. Our proof is similar to that of Proposition 4 of Bertsekas and Yu (2009). The overall arguments are the
following: Equation (15) implies that the traces can be truncated at some depth l, whose influence on
the potential limit of the algorithm vanishes when l tends to ∞. For all l, the l-truncated version of the
algorithm can easily be analyzed through the ergodic theorem for Markov chains. Making l tend to ∞
allows tying the convergence of the original arguments to that of the truncated version. Eventually, the
formula for the limit of the truncated algorithm is computed and one derives the limit.

303

Geist and Scherrer

Algorithm 4: Off-policy BRM(λ)
Initialization;
Initialize vector θ0 and matrix C0 ;
Set y0 = 0, D0 = 0 and z0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Pre-update traces ;
yi = (γλρi−1 )2 yi−1 + 1 ;
Compute ;
T
√
γλρi−1
√ i−1 Di−1
√
yi ∆φi + γλρ
Ui =
;
yi
yi Di−1
T
√
γλρ
√ i−1 Di−1 − √ i−1 Di−1
yi ∆φi + γλρ
;
Vi =
yi
yi
T
√
γλρ
√ i−1 zi−1 − √ i−1 zi−1
yi ρri + γλρ
Wi =
;
yi
yi
Update parameters ;
θi = θi−1 + Ci−1 Ui (I2 + Vi Ci−1 Ui )−1 (Wi − Vi θi−1 ) ;
Ci = Ci−1 − Ci−1 Ui (I2 + Vi Ci−1 Ui )−1 Vi Ci−1 ;
Post-update traces ;
Di = (γλρi−1 )Di−1 + ∆φi yi ;
zi = (γλρi−1 )zi−1 + ri ρi yi ;

Then 1i A˜i and 1i ˜bi respectively converge almost surely to
A˜ = ΦT D − γDP − γP T D + γ 2 D + S(I − γP ) + (I − γP T )S T Φ
˜b = ΦT (I − γP T )QT D + S Rπ
where we wrote:
D = diag (I − (λγ)2 P˜ T )−1 µ0 ,
D = diag P˜ T (I − (λγ)2 P˜ T )−1 µ0 ,

Q = (I − λγP )−1 ,
S = λγ(DP − γD )Q,

and where P˜ is the matrix whose coordinates are p˜ss = a π(a|s)ρ(s, a)P (s |s, a). Then,
the BRM(λ) algorithm converges with probability 1 to A˜−1˜b.
The assumption given by Equation (15) trivially holds in the on-policy case (in which
ρ(s, a) = 1 for all (s, a)) and in the off-policy case when λγ is sufficiently small with respect
to the mismatch between policies. Note in particular that this result implies the almost sure
convergence of the GPTD/KTD algorithms in the on-policy and no-trace case, a question
that was still open in the literature.10 The matrix P˜ , which is in general not a stochastic
10. See for instance the conclusion of Engel (2005).

304

Off-policy Learning with Traces

matrix, can have a spectral radius bigger than 1; Equation (15) ensures that (λγ)2 P˜ has a
spectral radius smaller than β so that D and D are well defined. Removing assumption
of Equation (15) does not seem easy, since by tuning λγ maliciously, one may force the
spectral radius of (λγ)2 P˜ to be as close to 1 as one may want, which would make A˜ and
˜b diverge. Though the quantity A˜−1˜b may compensate for these divergences, our current
proof technique cannot account for this situation and a related analysis constitutes possible
future work.
The fundamental idea behind the Bellman Residual approach is to address the computation of the fixed-point of T λ differently from the previous methods. Instead of computing
the projected fixed-point as in Equation (9), one considers the following over-determined
system
Φθ

T λ Φθ

⇔ Φθ

(I − λγP )−1 (R + (1 − λ)γP Φθ)

⇔ Φθ

QR + (1 − λ)γP QΦθ

⇔ Ψθ

QR

by Equation (3)

with Ψ = Φ − (1 − λ)γP QΦ, and solves it in a least-squares sense, that is by computing
θ∗ = A¯−1¯b with A¯ = ΨT Ψ and ¯b = ΨT QR. One of the motivations for this approach is that,
as opposed to the matrix A of LSTD/LSPE/FPKF, A¯ is invertible for all values of λ, and one
can always guarantee a finite error bound with respect to the best projection (Schoknecht,
2002; Yu and Bertsekas, 2008; Scherrer, 2010). If the goal of BRM(λ) is to compute A¯ and
¯b from samples, what it actually computes (A˜ and ˜b as characterized in Theorem 3) will in
general be biased because the estimation is based on a single trajectory.11 Such a bias adds
an uncontrolled variance term (Antos et al., 2006) to A¯ and ¯b; an interesting consequence
is that A˜ is always non-singular.12 More precisely, there are two sources of bias in the
estimation: one results from the non Monte-Carlo evaluation (the fact that λ < 1) and the
other from the use of the correlated importance sampling factors (as soon as one considers
off-policy learning). The interested reader may check that in the on-policy case, and when
λ tends to 1, A˜ and ˜b coincide with A¯ and ¯b. However, in the strictly off-policy case, taking
λ = 1 does not prevent the bias due to the correlated importance sampling factors. If we
have argued that LSTD/LSPE/FPKF should asymptotically coincide when λ = 1, we see
here that BRM should generally differ in an off-policy situation.

4. Stochastic Gradient Extensions To Eligibility Traces And Off-policy
Learning
We have just provided a systematic derivation of all least-squares algorithms for learning
with eligibility traces in an off-policy manner. When the number of features p is very large,
the O(p2 ) complexity involved by a least-squares approach may be prohibitive. In such a
situation, a natural alternative is to consider an approach based on a stochastic gradient
11. It is possible to remove the bias when λ = 0 by using double samples. However, in the case where λ > 0,
the possibility to remove the bias seems much more difficult.
12. A¯ is by construction positive definite, and A˜ equals A¯ plus a positive term (the variance term), and is
thus also positive definite.

305

Geist and Scherrer

descent of the objective function of interest (Bottou and Bousquet, 2011; Sutton et al., 2009;
Maei and Sutton, 2010).
In this section, we will describe a systematic derivation of stochastic gradient algorithms
for learning in an off-policy manner with eligibility traces. The principle followed is the
same as for the least-squares approaches: we shall instantiate the algorithmic pattern of
Equation (4) by choosing the value of ξ and update the parameter so as move towards the
minimum of J(θi , ξ) using a stochastic gradient descent. To make the pattern of Equation (4)
precise, we need to define the empirical approximate operator we use. We will consider the
λ operators (written in the followings T
ˆλ , with a slight abuse of notation):
untruncated Tˆi,n
i
n

V (sj )
(γλ)j−i ρji Tˆj V − ρj−1
i

Tˆiλ V = V (si ) +

(16)

j=i

where n is the total length of the trajectory.
It should be noted that algorithmic derivations in this section will be a little bit more
involved than in the least-squares case. First, by instantiating ξ = θi , the pattern given
in Equation (4) is actually a fixed-point problem onto which one cannot directly perform a
stochastic gradient descent; this issue will be addressed in Section 4.2 through the introduction of an auxiliary objective function, following the approach originally proposed by Sutton
et al. (2009). A second difficulty is the following: the just introduced empirical operator
Tˆiλ depends on the full trajectory after step i (on the future of the process), and is for this
reason usually coined a forward view estimate. Though it would be possible, in principle,
to implement a gradient descent based on this forward view, it would not be very memory
nor time efficient. Thus, we will follow a usual trick of the literature by deriving recursive
algorithms based on a backward view estimate that is equivalent to the forward view in
expectation. To do so, we will repeatedly use the following identity that highlights the fact
that the estimate Tˆiλ V can be written as a forward recursion:
Lemma 4 Let Tˆiλ be the operator defined in Equation (16) and let V ∈ RS . We have
λ
Tˆiλ V = ρi ri + γρi (1 − λ)V (si+1 ) + γλρi Tˆi+1
V.

Proof Using notably the identity ρji = ρi ρji+1 , we have:
n

(γλ)j−i ρji Tˆj V − ρj−1
V (sj )
i

Tˆiλ V = V (si ) +
j=i

n

(ρji Tˆj V − ρj−1
V (sj ))
i

= V (si ) + ρi Tˆi V − V (si ) + γλρi

j=i+1
λ
= ρi Tˆi V + γλρi Tˆi+1
V − V (si+1 ) .

To sum up, the “recipe” that we are about to use to derive off-policy gradient learning
algorithms based on eligibility traces will consist of the following steps:
1. write the empirical generic cost function of Equation (4) with the untruncated Bellman
operator of Equation (16) ;
306

Off-policy Learning with Traces

2. instantiate ξ and derive the gradient-based update rule (with some additional work
for ξ = θi , see Section 4.2);
3. turn the forward view into an equivalent (in expectation) backward view.
The next subsection details the precise derivation of the algorithms.
4.1 Off-policy TD(λ)
The Temporal-Difference algorithm (Sutton and Barto, 1998) is a gradient-based bootstrap
approach for value function approximation. Because it is the simplest, we begin by considering this bootstrap approach, that is by instantiating ξ = θj−1 . The cost function to be
minimized is therefore:
i

Tˆjλ Vˆθj−1 − Vˆω (sj )

2

.

j=1

Minimized with a stochastic gradient descent, the related update rule is (αi being a standard
learning rate and recalling that Vˆω (si ) = ω T φ(si ) = ω T φi ):
2
αi
∇ω Tˆiλ Vˆθi−1 − Vˆω (si )
2
ω=θi−1
λ
= θi−1 + αi φi Tˆi Vˆθ − Vˆθ (si ) .

θi = θi−1 −

i−1

i−1

(17)

At this point, one could notice that the exact same update rule would have been obtained
by instantiating ξ = θi−1 . This was to be expected: as only the last term of the sum is
considered for the update, we have j = i, and therefore ξ = θi−1 = θj−1 .
Equation (17) makes use of a λ-TD error defined as
δiλ (ω) = Tˆiλ Vˆω − Vˆω (si ).
For convenience, let also δi be the standard (off-policy) TD error defined as
δi (ω) = δiλ=0 (ω) = ρi Tˆi Vˆω − Vˆω (si ) = ρi ri + γ Vˆω (si+1 ) − Vˆω (si ).
The λ-TD error can be expressed as a forward recursion:
Lemma 5 Let δiλ be the λ-TD error and δi be the standard TD error. Then for all ω,
λ
δiλ (ω) = δi (ω) + γλρi δi+1
(ω).

Proof This is a corollary of Lemma 4:

⇔

λ
Tˆiλ Vω = ρi ri + γρi (1 − λ)Vω (si+1 ) + γλρi Tˆi+1
Vω
λ
λ
Tˆi Vω − Vω (si ) = ρi ri + γρi Vω (si+1 ) − Vω (si ) + γλρi (Tˆi+1
Vω − Vω (si+1 ))

⇔

λ
δiλ (ω) = δi (ω) + γλρi δi+1
(ω).

307

Geist and Scherrer

Therefore, we get the following update rule
θi = θi−1 + αi φi δiλ (θi−1 )
λ (θ
with δiλ (θi−1 ) = δi (θi−1 )+γλδi+1
i−1 ). The key idea here is to find some backward recursion
such that in expectation, when the Markov chain has reached its steady state distribution
µ0 , it provides the same result as the forward recursion. Such a backward recursion is given
by the following lemma.

Proposition 6 Let zi be the eligibility vector, defined by the following recursion:
zi = φi + γλρi−1 zi−1 .
For all ω, we have
Eµ0 [φi δiλ (ω)] = Eµ0 [zi δi (ω)].
Proof For clarity, we omit the dependence with respect to ω and write below δi (resp. δiλ )
for δi (ω) (resp. δiλ (ω)). The result relies on successive applications of Lemma 5. We have:
λ
Eµ0 [φi δiλ ] = Eµ0 [φi (δi + γλρi δi+1
)]
λ
= Eµ0 [φi δi ] + Eµ0 [φi γλρi δi+1
].
λ ] = E [φ
λ
Moreover, we have that Eµ0 [φi ρi δi+1
µ0 i−1 ρi−1 δi ], as expectation is done according
to the stationary distribution, therefore:

Eµ0 [φi δiλ ] = Eµ0 [φi δi ] + γλEµ0 [φi−1 ρi−1 δiλ ]
λ
= Eµ0 [φi δi ] + γλEµ0 [φi−1 ρi−1 (δi + γλρi δi+1
)]

= Eµ0 [δi (φi + γλρi−1 φi−1 + (γλ)2 ρi−1 ρi−2 φi−2 + . . . )]
= Eµ0 [δi zi ].
This suggests to replace Equation (17) by the following update rule,
θi = θi−1 + αi zi δi (θi−1 ),
which is equivalent in expectation when the Markov chain has reached its steady state. This
is summarized in Algorithm 5.
This algorithm was first proposed in the tabular case by Precup et al. (2000) (who
call it per-decision importance sampling). An off-policy TD(λ) algorithm (with function
approximation) was proposed by Precup et al. (2001), but it differs significantly from the
algorithm just described, since it updates parameters based on full episodic trajectories
rather than based on the current transition. Algorithm 5 was actually first proposed much
more recently by Bertsekas and Yu (2009).
An important issue for the analysis of this algorithm is the fact that the trace zi may have
an infinite variance, due to importance sampling (Yu, 2010b, Section 3.1). As far as we know,
the only existing analysis of off-policy TD(λ) (as provided in Algorithm 5) uses an additional
constraint which forces the parameters to be bounded: after each parameter update, the
308

Off-policy Learning with Traces

Algorithm 5: Off-policy TD(λ)
Initialization;
Initialize vector θ0 ;
Set z0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = γλρi−1 zi−1 + φi ;
Update parameters ;
θi = θi−1 + αi zi (ρi ri − ∆φTi θi−1 ) ;

resulting parameter vector is projected onto some predefined compact set. This analysis
is performed by Yu (2010b, Section 4.1). Under the standard assumptions of stochastic
approximations and most of the assumptions required for the on-policy TD(λ) algorithm,
assuming moreover that Π0 T λ is a contraction (which we recall to hold for a big enough λ)
and that the predefined compact set used to project the parameter vector is a large enough
ball containing the fixed point of Π0 T λ , the constrained version of off-policy TD(λ) converges
to this fixed-point —therefore, the same solution as off-policy LSTD(λ)—, LSPE(λ) and
FPKF(λ). We refer to Yu (2010b, Section 4.1) for further details. An analysis of the
unconstrained version of off-policy TD(λ) described in Algorithm 5 is an interesting topic
for future research.
4.2 Off-policy TDC(λ) And Off-policy GTD2(λ)
The Temporal Difference with gradient Correction and Gradient Temporal Differences 2
algorithms have been introduced by Sutton et al. (2009) as gradient descent approaches
to minimize the norm of the difference between the value function and its image through
the projected Bellman operator (they are therefore projected fixed-point approaches). Maei
and Sutton (2010) extended TDC to off-policy learning with traces, calling the resulting
algorithm GQ(λ).
This corresponds (for all algorithms and extensions) to the case ξ = θi , considered in
this section. Following the general pattern, at step i, we would like to come up with a new
parameter θi that moves (from θi−1 ) closer to the minimum of the function
ω → J(ω, θi ) = Tˆjλ Vˆθi − Vˆω (sj )

2

.

This problem is tricky since the function to minimize contains what we want to compute—θi —as a parameter. For this reason we cannot directly perform a stochastic gradient descent
of the right hand side. Instead, we will consider an alternative (but equivalent) formulation
of the projected fixed-point minimization θ = arg minω Vω −Π0 T λ Vω 2 , and will move from
θi−1 to θi by making one step of gradient descent of an estimate of the function
θ → V θ − Π0 T λ V θ
309

2

.

Geist and Scherrer

With the following vectorial notations:
ˆ ω = Vˆω (s1 ) . . .
V
ˆ λV
ˆ ω = Tˆλ Vˆω . . .
T
1

T

Vˆω (si )
T

Tˆiλ Vˆω

˜ = φ(s1 ) . . . φ(si )
Φ
˜T ,
˜ −1 Φ
˜ 0 = Φ(
˜ Φ
˜ T Φ)
Π

T

,

,

,

we consider the following objective function:
˜ 0T
ˆ λV
ˆθ
ˆθ − Π
J(θ) = V

2

T

˜0 V
ˆθ − T
ˆ λV
ˆθ
ˆθ − T
ˆ λV
ˆθ Π
= V

T 
−1 
i

i

δjλ (θ)φj  

=
j=1

φj φTj 



i

δjλ (θ)φj  .



j=1

j=1

This is the derivation followed by Sutton et al. (2009) in the case λ = 0 and by Maei and
Sutton (2010) in the case λ > 0 (and off-policy learning). Let us introduce the following
notation:
gjλ = ∇Tˆjλ Vˆθ .

(18)

Note that since we consider a linear approximation this quantity does not depend on θ.
Noticing that ∇δjλ (θ) = φj − gjλ , we can compute ∇J(θ):


T 

i

−1 

i



i

1
1
− ∇J(θ) = − ∇ 
δjλ (θ)φj  
φj φTj  
δjλ (θ)φj 
2
2
j=1
j=1
j=1

T 
−1 

i

i

δjλ (θ)φj  

= − ∇
j=1



i
j=1



i

φj φTj 





δjλ (θ)φj  − 
j=1

δjλ (θ)φj 

(19)

j=1



i



i



j=1

i

=

j=1

−1 

(φj − gjλ )φTj  

δjλ (θ)φj 



j=1



=

i

φj φTj 

gjλ φTj  
j=1

−1 

i

φj φTj 
j=1



i

δjλ (θ)φj  .


j=1

Let wi (θ) be a quasi-stationary estimate of the last part, that can be recognized as the
solution of a least-squares problem (regression of λ-TD errors δjλ on features φj ):


i

−1 
φj φTj 

wi (θ) ≈ 
j=1



i

i

δjλ (θ)φj  = argmin



ω

j=1

310

φTj ω − δjλ (θ)
j=1

2

.

Off-policy Learning with Traces

The identification with the above least-squares solution suggests to use the following stochastic gradient descent to form the quasi-stationary estimate:
wi = wi−1 + βi φi δiλ (θi−1 ) − φTi wi−1 .
This update rule makes use of the λ-TD error, defined through a forward view. As for the
previous algorithm, we can use Proposition 6 to obtain the following backward view update
rule that is equivalent (in expectation when the Markov chain reaches its steady state):
wi = wi−1 + βi zi δi (θi−1 ) − φi (φTi wi−1 ) .

(20)

Using this quasi-stationary estimate, the gradient can be approximated as:

 

i
i
1
− ∇J(θ) ≈ 
δjλ (θ)φj  − 
gjλ φTj  wi .
2
j=1

j=1

Therefore, a stochastic gradient descent gives the following update rule for the parameter
vector θ:
θi = θi−1 + αi δiλ (θi−1 )φi − giλ φTi wi .
(21)
Once again the forward view term δiλ (θi−1 )φi can be turned into a backward view by using
Proposition 6. There remains to work on the term giλ φTi .
First, one can notice that the term giλ satisfies a forward recursion.
Lemma 7 We have
λ
giλ = γρi (1 − λ)φi+1 + γλρi gi+1
.

Proof This result is simply obtained by applying the gradient to the forward recursion of
Tˆiλ Vθ provided in Lemma 4 (according to θ).
Using this, the term giλ φTi can be worked out similarly to the term δiλ (θi−1 )φi .
Proposition 8 Let zi be the eligibility vector defined in Proposition 6. We have
Eµ0 [giλ φTi ] = Eµ0 [γρi (1 − λ)φi+1 ziT ].
Proof The proof is similar to that of Proposition 6. Writing bi = γρi (1 − λ)φi+1 and
ηi = γλρi , we have
λ
Eµ0 [giλ φTi ] = Eµ0 [(bi + ηi gi+1
)φTi ]
λ
= Eµ0 [bi φTi ] + Eµ0 [ηi−1 (bi + ηi gi+1
)φTi−1 ]

= Eµ0 [bi ziT ].
Using this result and Proposition 6, it is natural to replace Equation (21) by an update
based on a backward recursion:
θi = θi−1 + αi zi δi − γρi (1 − λ)φi+1 (ziT wi−1 ) .
311

(22)

Geist and Scherrer

Last but not least, for the estimate wi to be indeed quasi-stationary, the learning rates
should satisfy the following condition (in addition to the classical conditions):
lim

i→∞

αi
= 0.
βi

Equations. (22) and (20) define the off-policy TDC(λ) algorithm, summarized in Algorithm 6. It was originally proposed by Maei and Sutton (2010) under the name GQ(λ). We
call it off-policy TDC(λ) to highlight the fact that it is the extension of the original TDC
algorithm of Sutton et al. (2009) to off-policy learning with traces. One can observe—to our
knowledge, this was never mentioned in the literature before—that when λ = 1, the learning
rule of TDC(1) reduces to that of TD(1).
Maei and Sutton (2010) show that the algorithm converges with probability 1 to the
same solution as the LSTD(λ) algorithm (that is, to θ∗ = A−1 b) under some technical
assumptions. Contrary to off-policy TD(λ), this algorithm does not requires Π0 T λ to be a
contraction in order to be convergent. Unfortunately, one of the assumptions made in the
analysis, requiring that the traces zi have uniformly bounded second moments, is restrictive
since in an off-policy setting the traces zi may easily have an infinite variance (unless the
behavior policy is really close to the target policy), as noted by Yu (2010a).13 A full proof
of convergence thus still remains to be done.
Algorithm 6: Off-policy TDC(λ), also known as GQ(λ)
Initialization;
Initialize vector θ0 and w0 ;
Set z0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = γλρi−1 zi−1 + φi ;
Update parameters ;
θi = θi−1 + αi zi (ρi ri − ∆φTi θi−1 ) − γρi (1 − λ)φi+1 (ziT wi−1 ) ;
wi = wi−1 + βi zi (ρi ri − ∆φTi θi ) − φi (φTi wi−1 ) ;

Using the same principle—performing a stochastic gradient descent to minimize J(θ))—
, an alternative to TDC, the GTD2 algorithm, was derived by Sutton et al. (2009) in the
λ = 0 case. As far as we know, it has never been extended to off-policy learning with traces;
we do it now. Notice that, given the derivation of GQ(λ), obtaining this algorithm is pretty
straightforward.
13. See also Randhawa and Juneja (2004).

312

Off-policy Learning with Traces

To do so, we can start back from Equation (19):


−1 

i
i
i
1
− ∇J(θ) =  (φj − gjλ )φTj  
φj φTj  
δjλ (θ)φj 
2
j=1
j=1
j=1


i

(φj − gjλ )φTj  wi .

≈
j=1

This suggests the following alternative update rule (based on forward recursion):
θi = θi−1 + αi (φi − giλ )φTi wi .
Using Proposition 8, it is natural to use the following alternative update rule, based on a
backward recursion:
θi = θi−1 + αi φi (φTi wi−1 ) − γρi (1 − λ)φi+1 (ziT wi−1 ) .
The update of wi remains the same, and put together it gives off-policy GTD2(λ), summarized in Algorithm 7. The analysis of this new algorithm constitutes a potential topic for
future research.
Algorithm 7: Off-policy GTD2(λ)
Initialization;
Initialize vector θ0 and w0 ;
Set z0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = γλρi−1 zi−1 + φi ;
Update parameters ;
θi = θi−1 + αi φi (φTi wi−1 ) − γρi (1 − λ)φi+1 (ziT wi−1 ) ;
wi = wi−1 + βi zi (ρi ri − ∆φTi θi ) − φi (φTi wi−1 ) ;

4.3 Off-policy gBRM(λ)
The algorithm proposed by Baird (1995) minimizes the Bellman residual using a gradientbased approach, in the no-trace and on-policy case. We extend it to eligibility traces and
to off-policy learning, which corresponds to instantiate ξ = ω. The cost function to be
minimized is then:
i

Tˆjλ Vˆω − Vˆω (sj )
j=1

313

2

.

Geist and Scherrer

Following the negative of the gradient of the last term leads to the following update rule:
θi = θi−1 − αi ∇ω Tˆiλ Vˆω − Vˆω (si )
= θi−1 − αi ∇ω Tˆiλ Vˆω − Vˆω (si )

2
ω=θi−1
ω=θi−1

Tˆiλ Vˆθi−1 − Vˆθi−1 (si )

= θi−1 + αi φi − giλ δiλ (θi−1 ),
recalling the notation giλ = ∇Tˆiλ Vˆω first defined in Equation (18).
As usual, this update involves a forward view, which we are going to turn into a backward
view. The term φi δiλ can be worked thanks to Proposition 6. The term giλ δiλ is more difficult
to handle, as it is the product of two forward views (until now, we only considered the
product of a forward view with a non-recursive term). This can be done thanks to the
following original relation (the proof being somewhat tedious, it is deferred to Appendix C):
Proposition 9 Write giλ = ∇ω Tˆiλ and define
ci = 1 + (γλρi−1 )2 ci−1 ,
ζi = γρi (1 − λ)φi+1 ci + γλρi−1 ζi−1
and di = δi ci + γλρi−1 di−1 .
We have that
Eµ0 [δiλ giλ ] = Eµ0 [δi ζi + di γρi (1 − λ)φi+1 − δi γρi (1 − λ)φi+1 ci ].
This result (together with Proposition 6) suggests to update parameters as follows:
θi = θi−1 + αi (δi (zi + γρi (1 − λ)φi+1 ci − ζi ) − di γρi (1 − λ)φi+1 ) .
This gives the off-policy gBRM(λ) algorithm, depicted in Algorithm 8. One can observe
that gBRM(1) is equivalent to TD(1) (and thus also TDC(1), cf. the comment before the
description of Algorithm 6). The analysis of this new algorithm is left for future research.

5. Empirical Study
This section aims at empirically comparing the surveyed algorithms. As they only address
the policy evaluation problem, we compare the algorithms in their ability to perform policy
evaluation (no control, no policy optimization); however, they may straightforwardly be
used in an approximate policy iteration approach (Bertsekas and Tsitsiklis, 1996; Munos,
2003). In order to assess their quality, we consider finite problems where the exact value
function can be computed.
More precisely, we consider Garnet problems (Archibald et al., 1995), which are a class
of randomly constructed finite MDPs. They do not correspond to any specific application,
but are totally abstract while remaining representative of the kind of MDP that might be
encountered in practice. In our experiments, a Garnet is parameterized by 4 parameters
and is written G(nS , nA , b, p): nS is the number of states, nA is the number of actions, b
314

Off-policy Learning with Traces

Algorithm 8: Off-policy gBRM(λ)
Initialization;
Initialize vector θ0 ;
Set z0 = 0, d0 = 0, c0 = 0, ζ0 = 0;
for i = 1, 2, . . . do
Observe φi , ri , φi+1 ;
Update traces ;
zi = φi + γλρi−1 zi−1 ;
ci = 1 + (γλρi−1 )2 ci−1 ;
ζi = γρi (1 − λ)φi+1 ci + γλρi−1 ζi−1 ;
di = (ρi ri − ∆φTi θi−1 )ci + γλρi−1 di−1 ;
Update parameters ;
θi = θi−1 + αi (ρi ri − ∆φTi θi−1 )(zi + γρi (1 − λ)φi+1 ci − ζi ) − di γρi (1 − λ)φi+1 ;

is a branching factor specifying how many possible next states are possible for each stateaction pair (b states are chosen uniformly at random and transition probabilities are set
by sampling uniform random b − 1 cut points between 0 and 1) and p is the number of
features (for function approximation). The reward is state-dependent: for a given randomly
generated Garnet problem, the reward for each state is uniformly sampled between 0 and
1. Features are chosen randomly: Φ is a nS × p feature matrix of which each component is
randomly and uniformly sampled between 0 and 1. The discount factor γ is set to 0.95 in
all experiments.
We consider two types of problems, “small” and “big”, respectively corresponding to
instances G(30, 2, 2, 8) and G(100, 4, 3, 20). We also consider two types of learning: onpolicy and off-policy. In the on-policy setting, for each Garnet a policy π to be evaluated
is randomly generated (by sampling randomly nA − 1 cut points between 0 and 1 for each
state), and trajectories (to be used for learning) are sampled according to this same policy.
In the off-policy setting, the policy π to be evaluated is randomly generated the same way,
but trajectories are sampled according to a different (similarly randomly generated) behavior
policy π0 .
For all algorithms, we choose θ0 = 0. For least-squares algorithms (LSTD, LSPE, FPKF
and BRM), we set the initial matrices (M0 , N0 , C0 ) to 103 I (the higher this value, the more
negligible its effect on estimates; we observed that this parameter did not play a crucial role
in practice). We run a first set of experiments in order to set all other parameters (eligibility
factor and learning rates). We use the following schedule for the learning rates:
αi = α0

αc
βc
and βi = β0
2 .
αc + i
βc + i 3

More precisely, we generate 30 problems (MDPs and policies) for each possible combination
small/big on-policy/off-policy (leading to four cases). For each problem, we generate one
trajectory of length 104 using the behavioral policy (which is the randomly generated target
315

Geist and Scherrer

policy in the on-policy case and the behavior policy in the off-policy case), to be used by
all algorithms. For each meta-parameter, we consider the following ranges of values: λ ∈
{0, 0.4, 0.7, 0.9, 1}, α0 ∈ {10−2 , 10−1 , 100 }, αc ∈ {101 , 102 , 103 }, β0 ∈ {10−2 , 10−1 , 100 } and
βc ∈ {101 , 102 , 103 }. Then, we compute the parameter estimates considering all algorithms
instantiated with each possible combination of the meta-parameters. This gives for each
combination a family θi,d with i the number of transitions encountered in the trajectory of
the dth problem. Finally, for each case, for all problems and each algorithm, we choose the
combination of meta-parameters which minimizes the average error on the last one-tenth
of the averaged (over all problems) learning curves (we do this to reduce the sensitivity to
the initialization and the transient behavior). Formally, we pick the set of parameters that
minimizes the following quantity:
1
err =
30

30
d=1

1
103

104

Φθi,d − V πd

2.

i=9.103

We provide the empirical results of this first set of experiments in Tables 3 to 6. As a
complement, we detail in Figure 1 the sensitivity of all algorithms with respect to the main
parameter λ that controls the eligibility traces (averaged over the 30 problems, with the
best global meta-parameters for each choice of λ). We comment these results below.

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2

λ
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

α0

αc

β0

βc

10−2
10−2
10−2
10−2

103
103
103
103

10−2
10−1

101
102

err
2.07
2.07
2.07
2.07
2.06
2.06
2.06
2.05

Table 3: Small problem (G(30, 2, 2, 8)), on-policy learning (π = π0 ).

Table 3 shows the best global meta-parameters over the 30 considered instances (one
trajectory per instance) of a small Garnet problem in an on-policy setting, as well as related
efficiency. Numerically, all methods provide equivalent performance (the slight difference of
error is not statistically significant, provided the variance of the estimates). All methods use
the same eligibility factor (λ = 1), leading to a Monte Carlo estimate, to reach their best
performance. Figure 1 (top, left) shows that this choice of λ does matter and that BRM,
gBRM and FPKF are more sensitive to a good choice of the eligibility factor.
Table 4 shows the best global meta-parameters over the 30 considered instances (one
trajectory per instance) of a big Garnet problem in an on-policy setting, as well as related
performance. These results are consistent with those of the small problem, in the on-policy
setting (with rather different meta-parameters, apart from the eligibility factor). Here again,
the algorithms need the highest value of λ to perform the best, except TDC and GTD2 that
316

Off-policy Learning with Traces

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2

λ
1.0
1.0
1.0
1.0
1.0
1.0
0.9
0.9

α0

αc

β0

βc

10−1
10−1
10−1
10−1

101
101
102
102

10−1
10−2

102
103

err
1.20
1.20
1.20
1.20
1.25
1.25
1.21
1.22

Table 4: Big problem (G(100, 4, 3, 20)), on-policy learning (π = π0 ).

take nevertheless a high value of λ. Figure 1 (top, right) suggests that as the problem’s size
grows, the role of the eligibility factor gets more prominent (but with a similar behavior).

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2

λ
0.4
0.4
0.7
0.0
0.4
0.0
0.4
0.4

α0

αc

β0

βc

10−1
10−2
10−1
10−1

102
101
101
103

10−2
10−2

101
101

err
3.69
3.69
4.74
4.42
3.85
10.42
7.81
4.53

Table 5: Small problem (G(30, 2, 2, 8)), off-policy learning (π = π0 ).
Table 5 reports the best meta-parameters in an off-policy setting for a small problem
(still for 30 instances). Regarding the least-squares methods, LSTD and LSPE get the
best results, whereas FPKF and BRM suffer more from the off-policy aspect. Regarding
gradient methods, TD’s performance is good (it is close to that of LSTD/LSPE and better
than BRM/FPKF), followed closely by GTD2. TDC and gBRM lead to the worse results.
All algorithms use a small or intermediate value of the eligibility factor. Increasing λ would
reduce the bias, but the performance would suffer from the variance due to importance
sampling, as shown also in Figure 1 (bottom, left).
Eventually, Table 6 shows the meta-parameters and performance in the most difficult
situation: the off-policy setting of the big problem. These results are consistent with the
off-policy results of the small problem, summarized in Table 5. LSTD and LSPE are the
most efficient least-squares algorithms and choose the smallest possible value λ = 0. FPKF
and BRM’s performance deteriorate (significantly for the latter). TD behaves very well and
GTD2 follows closely. The performance of TDC and gBRM are the worse. Figure 1 (bottom,
right) is similar to that of the small problem. It shows that TD (with a good learning rate)
is quite stable, in particular more than LSTD/LSPE.
317

Geist and Scherrer

on-policy, small problem

on-policy, big problem
LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2

10-10.0

0.2

0.4

Lambda

0.6

0.8

100
Average error

Average error

100

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2

10-10.0

1.0

0.2

off-policy, small problem

Lambda

0.6

0.8

1.0

off-policy, big problem

Average error

100

Average error

100

10-10.0

0.4

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2
0.2

0.4

Lambda

0.6

0.8

10-10.0

1.0

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2
0.2

0.4

Lambda

0.6

0.8

1.0

Figure 1: Sensitivity of performance of the algorithms (y-axis, in logarithmic scale) with
respect to the eligibility trace parameter λ (x-axis). Left: Small problem
(G(30, 2, 2, 8)), right: Big problem (G(100, 4, 3, 20)). Top: on-policy learning
(π = π0 ), bottom: off-policy learning (π = π0 ).

The main goal of the series of experiments we have just described was to choose reasonable values for the meta-parameters. We have also used these experiments to quickly
comment the relative performance of the algorithms, but this is not statistically significant
as this was based on a few (random) problems, onto which meta-parameters have been optimized. Though we will see that the general behavior of the algorithm is globally consistent
with what we have seen so far, the series of experiments that we are about to describe aims
at providing such a statistically significant performance comparison. For each situation
(small and big problems, on- and off-policy), we fix the meta-parameters to the previously
reported values and we compare the algorithms on several new instances of the problems.
These results are reported on Figures 2 to 5. For each of the 4 problems, we randomly generate 100 instances (MDP and policy to be evaluated). For each such problem, we generate
a trajectory of length 105 . Then, all algorithms learn using this very trajectory. On each
figure, we report the average performance (left), measured as the difference between the true
318

Off-policy Learning with Traces

LSTD
LSPE
FPKF
BRM
TD
gBRM
TDC
GTD2

λ
0
0
0.7
1.0
0.4
0.0
0.0
0.0

α0

αc

β0

βc

10−1
10−2
10−1
10−1

101
101
101
103

10−2
10−2

101
101

err
3.76
3.86
4.80
10.05
2.96
10.50
8.65
4.41

Table 6: Big problem (G(100, 4, 3, 20)), off-policy learning (π = π0 ).

value function (computed from the model) and the currently estimated one, V π − Φθ 2 , as
well as the associated standard deviation (right).
on-policy, small problem

10-1 2
10

103

Iterations

104

102

Standard Deviation of the error

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM

Average error

100

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM

101
100
10-1
10-2
10-3 2
10

105

on-policy, small problem

103

Iterations

104

105

Figure 2: Performance for small problems (G(30, 2, 2, 8)), on-policy learning (π = π0 ) (left:
average error, right: standard deviation).

We begin by discussing the results in the on-policy setting. Figure 2 compares all algorithms for 100 randomly generated small problems (that is, each run corresponds to different
dynamics, reward function, features and evaluated policy), the meta-parameters being those
provided in Table 3. All least-squares approaches provide the best results and are bunched
together; this was to be expected, as all algorithms use λ equal to 1. The gBRM, TD and
TDC algorithms provide the same results (being equivalent with the choice λ = 1), they are
slower than GTD2, which is slower than the least-squares algorithms. Figure 3 compares
the algorithms for 100 randomly generated big problems, the meta-parameters being those
provided in Table 4. These result are similar to those of the small problem in an off-policy
setting, except that TDC has now a different (and slower) behavior, due to the different
choice of the eligibility factor (λ = 0.9). GTD2 is still the better gradient-based algorithm.
319

Geist and Scherrer

on-policy, big problem

Average error

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM

10-1 2
10

103

Iterations

104

102

Standard Deviation of the error

100

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM

101
100
10-1
10-2
10-3 2
10

105

on-policy, big problem

103

Iterations

104

105

Figure 3: Performance for big problems (G(100, 4, 3, 20)), on-policy learning (π = π0 ) (left:
average error, right: standard deviation).

off-policy, small problem

10-1 2
10

102

Standard Deviation of the error

Average error

100

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM
103

Iterations

104

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM

101
100
10-1
10-2
10-3 2
10

105

off-policy, small problem

103

Iterations

104

105

Figure 4: Performance for small problems (G(30, 2, 2, 8)), off-policy learning (π = π0 ) (left:
average error, right: standard deviation).

We now consider the off-policy setting. Figure 4 provides the average performance and
standard deviation of the algorithms (meta-parameters being those of Table 5) on 100 small
problems. Once again, we can see that LSTD/LSPE provide the best results. The two
other least-squares methods (FPKF and BRM) are overtaken by the gradient-based TD
algorithm, that follows closely LSTD/LSPE. GTD2 is a little bit slower and TDC is the
slowest algorithm. Figure 5 provides the same data for the big problems (with the metaparameters of Table 6). These results are similar to those of the small problems in an
off-policy setting, except that TD is even closer to LSTD/LSPE (but requires the choice of
a learning rate).
Summary. Overall, our experiments suggest that the two best algorithms are LSTD and
LSPE, since they converge much faster in all situations with less parameter tuning. The
gradient-based TD algorithm globally displays a good behavior and constitutes a good alter320

Off-policy Learning with Traces

off-policy, big problem

10-1 2
10

102

Standard Deviation of the error

Average error

100

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM
103

Iterations

104

LSTD
LSPE
FPKF
BRM
TD
TDC
GTD2
gBRM

101
100
10-1
10-2
10-3 2
10

105

off-policy, big problem

103

Iterations

104

105

Figure 5: Performance for big problems (G(100, 4, 3, 20)), on-policy learning (π = π0 ) (left:
average error, right: standard deviation).

native when the number p of features is too big for least-squares methods to be implemented.
Though some new algorithms/extensions show interesting results (FPKF(λ) is consistently
better that the state-of-the-art FPKF by Choi and Van Roy 2006, gBRM works well in the
on-policy setting) most of the other algorithms do not seem to be empirically competitive
with the trio LSTD/LSPE/TD, especially in off-policy situations. In particular, the algorithm introduced specifically for the off-policy setting (TDC/GTD2) are much slower than
TD in the off-policy case (but GTD2 is faster in the on-policy experiments, yet with more
parameter tuning). Moreover, the condition required for the good behavior of LSPE, FPKF
and TD—the contraction of Π0 T λ —does not seem to be very restrictive in practice (at least
for the Garnet problems we considered): though it is possible to build specific pathological
examples where these algorithms diverge, this never happened in our experiments.14

6. Conclusion And Future Work
We have considered least-squares and gradient-based algorithms for value estimation in an
MDP context. Starting from the on-policy case with no trace, we have recalled that several
algorithms (LSTD, LSPE, FPKF and BRM for least-squares approaches, TD, gBRM and
TDC/GTD2 for gradient-based approaches) fall in a common algorithmic pattern: Equation (2). Substituting the original Bellman operator by an operator that deals with traces
and off-policy samples naturally leads to the state-of-the-art off-policy trace-based versions
of LSTD, LSPE, TD and TDC, and suggests natural extensions of FPKF, BRM, gBRM
and GTD2. This way, we surveyed many known and new off-policy eligibility trace-based
algorithms for policy evaluation.
We have explained how to derive recursive (memory and time-efficient) implementations
of all these algorithms and discussed their known convergence properties, including an original analysis of BRM(λ) for sufficiently small λ, that implies the so far not known convergence
14. A preliminary version of this article (Scherrer and Geist, 2011) contains such examples, and also an
example where an adversarial choice of λ leads to the divergence of LSTD(λ).

321

Geist and Scherrer

of GPTD/KTD. Interestingly, it appears that the analysis of off-policy trace-based stochastic gradient algorithms under mild assumptions is still an open problem: the only currently
known analysis of TD (Yu, 2010a) only applies to a constrained version of the algorithm,
and that of TDC (Maei and Sutton, 2010) relies on an assumption on the boundedness of
the second moment traces that is restrictive (Yu, 2010a). Filling this theoretical gap, as well
as providing complete analyses for the other gradient algorithms and FPFK(λ) and BRM(λ)
constitute important future work.
Finally, we have illustrated and compared the behavior of these algorithms; this constitutes the first exhaustive empirical comparison of linear methods.15 Overall, our study
suggests that even if the use of eligibility traces generally improves the efficiency of all algorithms, LSTD and LSPE consistently provide the best estimates; and in situations where
the computational cost is prohibitive for a least-squares approach (when the number p of
features is large), TD probably constitutes the best alternative.

Appendix A. Derivation Of The Recursive Formulas For BRM(λ)
We here detail the derivation of off-policy BRM(λ). We will need two technical lemmas. The
first one is the Woodbury matrix identity which generalizes the Sherman-Morrison formula
(given in Lemma 1).
Lemma 10 (Woodbury) Let A, U , C and V be matrices of correct sizes, then:
(A + U CV )−1 = A−1 − A−1 U (C −1 + V A−1 U )−1 V A−1 .
The second lemma is a rewriting of imbricated sums:
Lemma 11 Let f ∈ RN×N×N and n ∈ N. We have:
n

n

n

n

i

j

f (i, j, k) =
i=1 j=i k=i

n

i−1

j

f (k, i, j) +
i=1 j=1 k=1

f (k, j, i).
i=2 j=1 k=1

As stated in Equation (14), we have the following batch estimate for BRM(λ):
i
T
(zj→i − ψj→i
ω)2 = (A˜i )−1˜bi ,

θi = argmin
ω∈Rp

j=1

where
i

i

ρ˜k−1
∆φk
j

ψj→i =

ρ˜jk−1 ρk rk

and zj→i =

k=j

k=j

and
i

A˜i =

i
T
ψj→i ψj→i

and ˜bi =

j=1

ψj→i zj→i .
j=1

15. To our knowledge, there does not even exist any work reporting and comparing empirical results of
LSTD(0) and FPKF(0).

322

Off-policy Learning with Traces

To obtain a recursive formula, these two sums have to be reworked through Lemma 11.
Let us first focus on the latter:
i

i

i

i

ρ˜jk−1 ∆φk ρ˜m−1
ρm rm
j

ψj→i zj→i =
j=1

j=1 k=j m=j
i

j

k

i

ρ˜j−1
˜k−1
m ∆φj ρ
m ρk rk

=

j−1

k

ρ˜k−1
˜j−1
m ∆φk ρ
m ρj rj .

+

j=1 k=1 m=1

j=2 k=1 m=1

Writing
k
2
2
(˜
ρk−1
m ) = 1 + (γλρk−1 ) yk−1 ,

yk =

m=1

we have that:
k

ρ˜j−1
˜k−1
˜j−1
m ρ
m =ρ
k yk .
m=1

Therefore:
i

j

i

i

ρ˜j−1
k yk ∆φj ρk rk

ψj→i zj→i =
j=1

j−1

ρ˜j−1
k yk ∆φk ρj rj .

+
j=2 k=1

j=1 k=1

With the following notations:
j

ρ˜j−1
k yk ρk rk = γλρj−1 zj−1 + ρj rj yj

zj =
k=1
j

ρ˜j−1
k yk ∆φk = γλρj−1 Dj−1 + yj ∆φj ,

and Dj =
k=1

and with the convention that z0 = 0 and D0 = 0, one can write:
i

i

ψj→i zj→i =
j=1

(∆φj ρj rj yj + γλρj−1 (∆φj zj−1 + ρj rj Dj−1 )).
j=1

Similarly, on can show that:
i

i
T
ψj→i ψj→i
=

j=1

(∆φj ∆φTj yj + γλρj−1 (∆φj DTj−1 + Dj−1 ∆φTj )).
j=1

Denoting
√

yj ∆φj ,
γλρj−1
vj = √
Dj−1 ,
yj

uj =

323

Geist and Scherrer

and I2 the 2 × 2 identity matrix, we have:
i

i
T
ψj→i ψj→i

((uj + vj )(uj + vj )T − vj vjT )

=

j=1

j=1
i−1
T
ψj→i ψj→i
+ ui + vi vi I2

=
j=1

(ui + vi )T
−viT

=Ui

=Vi

We can apply the Woodbury identity given in Lemma 10:

−1 
i

−1

i−1

T 
ψj→i ψj→i

Ci = 

=

j=1

.

ψj→i zj→i + Ui I2 Vi 
j=1

= Ci−1 − Ci−1 Ui (I2 + Vi Ci−1 Ui )−1 Vi Ci−1 .
The other sum can also be reworked:
i

˜bi =

i

ψj→i zj→i =
j=1

∆φj rj yj + γλ (Dj−1 rj + ∆φj zj−1 )
j=1

√
= ˜bi−1 + ∆φi ri yi + γλ (Di−1 ri + ∆φi zi−1 ) = ˜bi−1 + Ui

yi ri +

γλ
√
yi zi−1

− √γλyi zi−1

.

=Wi

Finally, the recursive BRM(λ) estimate can be computed as follows:
θi = Ci˜bi = θi−1 + Ci−1 Ui (I2 + Vi Ci−1 Ui )−1 (Wi − Vi θi−1 ) .
This gives BRM(λ) as provided in Algorithm 4.

Appendix B. Proof Of Theorem 3: Convergence Of BRM(λ)
The proof of Theorem 3 follows the general idea of that of Proposition 4 of Bertsekas and
Yu (2009). It is done in 2 steps. First we argue that the limit of the sequence is linked to
that of an alternative algorithm for which one cuts the traces at a certain depth l. Then, we
show that for all depth l, this alternative algorithm converges almost surely, we explicitly
compute its limit and make l tend to infinity to obtain the limit of BRM(λ).
˜ The argument is similar for 1 bi → ˜b. Consider
We will only show that 1i A˜i tends to A.
i
the following l-truncated version of the algorithm based on the following alternative traces
(we here limit the “memory” of the traces to a size l):
k
k−1 2
(˜
ρm
) ,

yk,l =

m=max(1,k−l+1)
j

ρ˜j−1
k yk,l ∆φk ,

Dj,l =
k=max(1,j−l+1)

324

Off-policy Learning with Traces

and update the following matrix:
A˜i,l = A˜i−1,l + ∆φi ∆φTi yi,l + ρ˜i−1 (∆φi DTi−1,l + Di−1,l ∆φTi ).
The assumption in Equation (15) implies that ρ˜j−1
≤ β j−i , therefore it can be seen that for
i
all k,
max(0,k−l)

max(0,k−l)

2
(˜
ρk−1
m )

|yk,l − yk | =

β 2(k−m) ≤

≤

m=1

m=1

β 2l
=
1 − β2

1 (l)

where 1 (l) tends to 0 when l tends to infinity. Similarly, using the fact that yk ≤
writing K = maxs,s φ(s) − γφ(s ) ∞ , one has for all j,
max(0,j−l)

Dj,l − Dj

∞

≤
where

2 (l)

A˜i,l − A˜i

∞

k=max(1,j−l+1)

max(0,j−l)

k=1

ρ˜j−1
k |yk,l − yk | ∆φk

∞+

k=1

≤

and

j

ρ˜kj−1 yk ∆φk

≤

1
1−β 2

ρ˜kj−1
1

1
K+
− β2

j

ρ˜j−1
k
k=max(1,j−l+1)

1
1
β 2l
βl
K
+
K=
1 − β 1 − β2
1 − β 1 − β2

β 2l
K
1 − β2

2 (l)

also tends to 0. Then, it can be seen that:
∞

= A˜i−1,l − A˜i−1 + ∆φi ∆φTi (yi,l − yi )
+ ρ˜i−1 (∆φi (DTi−1,l − DTi−1 ) + (Di−1,l − Di−1 )∆φTi ) ∞
≤ A˜i−1,l − A˜i−1 ∞ + ∆φi ∆φTi ∞ |yk,l − yk | + 2β ∆φi ∞ Di−1,l − Di
≤ A˜i−1,l − A˜i−1 ∞ + K 2 1 (l) + 2βK 2 (l)

∞

and, by a recurrence on i, one obtains
A˜i,l A˜i
−
i
i

≤ (l)
∞

where (l) tends to 0 when l tends to infinity. This implies that:
lim inf
l→∞

A˜i,l
A˜i,l
A˜i
A˜i
− (l) ≤ lim inf
≤ lim sup
≤ lim sup
+ (l).
l→∞
i
i
i
i
l→∞
l→∞
˜
A

˜

In other words, one can see that limi→∞ Aii and liml→∞ limi→∞ ii,l are equal if the latter
exists. In the remaining of the proof, we show that the latter limit indeed exists and we
compute it explicitly.
˜
A

Let us fix some l and let us consider the sequence ( ii,l ). At some index i, yi,l depends
only on the last l samples, while Di,l depends on the same samples and the last l values of
yj,l , thus on the last 2l samples. It is then natural to view the computation of A˜i,l , which is
325

Geist and Scherrer

based on yi,l , Di−1,l and ∆φi = φi − γρi φi+1 , as being related to a Markov chain of which
the states are the 2l + 1 consecutive states of the original chain (si−2l , . . . , si , si+1 ). Write
E0 the expectation with respect to its stationary distribution. By the Markov chain Ergodic
Theorem, we have with probability 1:
A˜i,l
= E0 ∆φ2l ∆φT2l y2l,l + λγρ2l−1 (∆φ2l DT2l−1,l + D2l−1,l ∆φT2l ) .
i→∞ i
lim

(23)

Let us now explicitly compute this expectation. Write xi the indicator vector (of which the
k th coordinate equals 1 when the state at time i is k and 0 otherwise). One has the following
relations: φi = ΦT xi . Let us first look at the left part of the above limit:
E0 ∆φ2l ∆φT2l y2l,l
= E0 (φ2l − γρ2l φ2l+1 )(φ2l − γρ2l φ2l+1 )T y2l,l
2l
T

T

= E0 Φ (x2l − γρ2l x2l+1 )(x2l − γρ2l x2l+1 ) Φ

2
(λγ)2(2l−m) (ρ2l−1
m )
m=l+1

2l

= ΦT

2l−1 2
(λγ)2(2l−m) E0 (ρm
) (x2l − γρ2l x2l+1 )(x2l − γρ2l x2l+1 )T

Φ

m=l+1
2l

= ΦT

(λγ)2(2l−m) E0 (Xm,2l,2l − γXm,2l,2l+1 − γXm,2l+1,2l + γ 2 Xm,2l+1,2l+1 )

Φ

m=l+1
i−1 ρj−1 x xT .
where we used the definition ρ˜k−1
= (λγ)k−j ρk−1
and the notation Xm,i,j = ρm
m
i j
j
j
To finish the computation, we will mainly rely on the following Lemma:

Lemma 12 (Some Identities) Let P˜ be the matrix of which the coordinates are p˜ss =
a π(s, a)ρ(s, a)T (s, a, s ), which is in general not a stochastic matrix. Let µ0 be the sta˜ i = diag (P˜ T )i µ0 . Then
tionary distribution of the behavior policy π0 . Write D
˜ i−m ,
∀m ≤ i, E0 [Xm,i,i ] = D
˜ i−m P j−i ,
∀m ≤ i ≤ j, E0 [Xm,i,j ] = D
˜ i−m .
∀m ≤ j ≤ i, E0 [Xm,i,j ] = (P T )j−i D
Proof We first observe that:
2
T
E0 [Xm,i,i ] = E0 [(ρi−1
m ) xi xi ]
2
= E0 [(ρi−1
m ) diag(xi )]
i−1 2
= diag E0 [(ρm
) xi .
i−1 )2 x ] =
To provide the identity, we will thus simply provide a proof by recurrence that E0 [(ρm
i
T
m−i
˜
(P )
µ0 . For i = m, we have E0 [xm ] = µ0 . Now suppose the relation holds for i and let
us prove it for i + 1.

E0 [(ρim )2 xi+1 ] = E0 E0 [(ρim )2 xi+1 |Fi ]
2
2
= E0 E0 [(ρi−1
m ) (ρi ) xi+1 |Fi ]
i−1 2
) E0 [(ρi )2 xi+1 |Fi ] .
= E0 (ρm

326

Off-policy Learning with Traces

Write Fi the realization of the process until time i. Recalling that si is the state at time i
and xi is the indicator vector corresponding to si , one has for all s :
E0 [(ρi )2 xi+1 (s )|Fi ] =

π0 (si , a)ρ(si , a)2 T (si , a, s )
a

=

π(si , a)ρ(si , a)T (si , a, s )
a

= p˜si ,s
= [P˜ T xi ](s ).
As this is true for all s , we deduce that E0 [(ρi )2 xi+1 |Fi ] = P˜ T xi and
i−1 2 ˜ T
E0 [(ρim )2 xi+1 ] = E0 [(ρm
) P xi ]
T
˜
= P E0 [(ρi−1 )2 P˜ T xi ]
m

= P˜ T (P˜ T )i µ0
= (P˜ T )i+1 µ0
which concludes the proof by recurrence.
Let us consider the next identity. For i ≤ j,
j−1
T
i−1 j−1
T
E0 [ρi−1
m ρm xi xj ] = E0 [E0 [ρm ρm xi xj |Fi ]]
i−1 2
= E0 [(ρm
) xi E0 [ρj−1
xTj |Fi ]]
i
i−1 2
= E0 [(ρm
) xi xTi P j−i ]

= diag (P˜ T )m−i µ0 P j−i .

T
Eventually, the last identity is obtained by considering Ym,i,j = Xm,j,i
.

Thus, coming back to our calculus,
2l

˜ 2l−m − γ D
˜ 2l−m P − γP T D
˜ 2l−m + γ 2 D
˜ 2l+1−m
(λγ)2(2l−m) D

E0 ∆φ2l ∆φT2l y2l,l = ΦT

Φ

m=l+1

= ΦT (Dl − γDl P − γP T Dl + γ 2 Dl )Φ

l−1

l−1

˜j,
(λγ) D
2j

with Dl =

(24)

˜ j+1 .
(λγ)2j D

and Dl =

j=0

j=0

327

Geist and Scherrer

Similarly, the second term on the right side of Equation (23) satisfies:
E0 ρ2l−1 D2l−1,l ∆φT2l
2l−1

ρ˜k2l−2 yk,l ∆φk ∆φT2l

= E0 ρ2l−1
k=l
2l−1

k

(λγ)2l−1−k ρ2l−1
k

= E0
k=l

2
(˜
ρk−1
m )

ΦT (xk − γρk xk+1 )(x2l − γρ2l x2l+1 )T Φ∆φT2l

m=k−l+1

2l−1

= ΦT

(λγ)2l−1−k
k=l
k
k−1
T
(λγ)2(k−m) E0 ρ2l−1
m ρm (xk − γρk xk+1 )(x2l − γρ2l x2l+1 )

Φ

m=k−l+1
2l−1
T

k
2l−1−k

=Φ

(λγ)2(k−m)

(λγ)
k=l

m=k−l+1

E0 Xm,k,2l − γXm,k+1,2l − γXm,k,2l+1 + γ 2 Xm,k+1,2l+1

2l−1
T

k
2l−1−k

=Φ

Φ

(λγ)2(k−m)

(λγ)
k=l

m=k−l+1

˜ k+1−m P 2l−k
˜ k−m P 2l−k − γ D
˜ k+1−m P 2l−k−1 − γ D
˜ k−m P 2l+1−k + γ 2 D
D

2l−1

= ΦT

k

(λγ)2l−1−k
k=l

(λγ)2(k−m)
m=k−l+1

˜ k−m P 2l−k (I − γP ) − γ D
˜ k+1−m P 2l−1−k (I − γP )
D
2l−1
T

k

˜ k−m P − γ D
˜ k+1−m P 2l−1−k (I − γP ) Φ
(λγ)2(k−m) D

2l−1−k

=Φ

(λγ)
k=l

m=k−l+1

2l−1

= ΦT

(λγ)2l−1−k Dl P − γDl P 2l−1−k (I − γP ) Φ
k=l

T

=Φ

Dl P − γDl Ql (I − γP )Φ

with Ql =

Φ

l−1
j
j=0 (λγP ) .

328

Φ

Off-policy Learning with Traces

Gathering this and Equation (24), we see that the limit of
equals:

Ai,l
i

expressed in Equation (23)

ΦT Dl − γDl P − γP T Dl + γ 2 Dl
+λγ (Dl P − γDl )Ql (I − γP ) + (I − γP T )QTl (P T Dl − γDl )

Φ.

When l tends to infinity, Ql tends to Q = (I − λγP )−1 . The assumption of Equation (15)
ensures that (λγ)P˜ has spectral radius smaller than 1, and thus when l tends to infinity, Dl
tends to D = diag (I − (λγ)2 P˜ T )−1 µ0 and Dl to D = diag P˜ T (I − (λγ)2 P˜ T )−1 µ0 . In
other words, liml→∞ limi→∞

˜i,l
A
i

exists with probability 1 and equals:

ΦT D − γDP − γP T D + γ 2 D
+λγ (DP − γD )Q(I − γP ) + (I − γP T )QT (P T D − γD )
Eventually, this shows that limi→∞

˜i
A
i

Φ.

exists with probability 1 and shares the same value.

A similar reasoning allows to show that limi→∞

˜bi
i

exists and equals

ΦT (I − γP T )QT D + λγ(DP − γD )Q Rπ .

Appendix C. Proof Of Proposition 9
To prove Proposition 9, we need the following technical lemma.
Lemma 13 Let αi and βi be two forward recursions defined as
αi = ai + ηi αi+1
and βi = bi + ηi βi+1 .
Assume that for any function f we have that (this is typically true if the index i refers to a
state sampled according to some stationary distribution, which is the case we are interested
in)
E[f (ai , bi , ηi )] = E[f (ai−1 , bi−1 , ηi−1 )].
Let also ui , vi and wi be the backward recursions defined as
2
wi = 1 + ηi−1
wi−1 ,

ui = ai wi + ηi−1 ui−1 ,
vi = bi wi + ηi−1 vi−1 .
Then, we have
E[αi βi ] = E[ai vi + bi ui − ai bi wi ].
Proof The proof looks like the one of Proposition 6, but is a little bit more complicated.
A key equality, to be applied repeatedly, is:
αi βi = (ai + ηi αi+1 )(bi + ηi βi+1 )
= ai βi + bi αi + ηi2 αi+1 βi+1 − ai bi .
329

Geist and Scherrer

Another equality to be used repeatedly makes use of the “stationarity” assumption. For any
k ≥ 0 we have:
k

k+1
2
ηi−j
)αi+1 βi+1 ] = E[(

E[(
j=0

2
ηi−j
)αi βi ].
j=1

These two identities can be used to work the term of interest:
E[αi βi ] = E[(ai + ηi αi+1 )(bi + ηi βi+1 )]
= E[ai βi ] + E[bi αi ] + E[ηi2 αi+1 βi+1 ] − E[ai bi ]
2
= E[ai βi ] + E[bi αi ] − E[ai bi ] + E[ηi−1
(ai + ηi αi+1 )(bi + ηi βi+1 )]
2
2
2
= E[ai (1 + ηi−1
)βi ] + E[bi (1 + ηi−1
)αi ] − E[ai bi (1 + ηi−1
)] + E[(ηi−1 ηi )2 αi+1 βi+1 ].

This process can be repeated, giving
2
E[αi βi ] = E[(ai βi + bi αi − ai bi )(1 + ηi−1
+ (ηi−1 ηi−2 )2 + . . . )].

We have that
2
2
wi = 1 + ηi−1
wi−1 = 1 + ηi−1
+ (ηi−1 ηi−2 )2 + . . . ,

therefore
E[αi βi ] = E[ai wi βi ] + E[bi wi αi ] − E[ai bi wi ].
We can work on the first term:
E[ai wi βi ] = E[ai wi (bi + ηi βi+1 )]
= E[ai wi bi ] + E[ai−1 wi−1 ηi−1 (bi + ηi βi+1 )]
= E[bi (ai wi + ηi−1 (ai−1 wi−1 ) + ηi−1 ηi−2 (ai−2 wi−2 ) + . . . )]
= E[bi ui ].
The work on the second term is symmetric:
E[bi wi αi ] = E[ai vi ].
This finishes proving the result.
The proof of Proposition 9 is a simple application of the preceding technical lemma. By
lemma 5, we have that
λ
δiλ = δi + γλρi δi+1
.
.=a
.=η .
.
=αi

i

i

=αi+1

By lemma 7, we have that
λ
giλ = γρi (1 − λ)φi+1 + γλρi gi+1
.
.=η .
.=β
.=b
i =β
i
i
i+1

The result is then a direct application of Lemma 13.
330

Off-policy Learning with Traces

References
A. Antos, Cs. Szepesvári, and R. Munos. Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. In Conference
on Learning Theory (COLT), 2006.
T. Archibald, K. McKinnon, and L. Thomas. On the generation of Markov decision processes. Journal of the Operational Research Society, 46:354–361, 1995.
L.C. Baird. Residual algorithms: Reinforcement learning with function approximation. In
International Conference on Machine Learning (ICML), 1995.
D.P. Bertsekas and S. Ioffe. Temporal differences-based policy iteration and applications in
neuro-dynamic programming. Technical report, MIT, 1996.
D.P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific,
1996.
D.P. Bertsekas and H. Yu. Projected equation methods for approximate solution of large
linear systems. Journal of Computational and Applied Mathematics, 227:27–50, 2009.
L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In S. Sra, S. Nowozin,
and S.J. Wright, editors, Optimization for Machine Learning, pages 351–368. MIT Press,
2011.
J.A. Boyan. Technical update: Least-squares temporal difference learning. Machine Learning, 49(2-3):233–246, 1999.
S.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22(1-3):33–57, 1996.
D. Choi and B. Van Roy. A generalized Kalman filter for fixed point approximation and
efficient temporal-difference learning. Discrete Event Dynamic Systems, 16:207–239, 2006.
Y. Engel. Algorithms and Representations for Reinforcement Learning. PhD thesis, Hebrew
University, 2005.
M. Geist and O. Pietquin. Eligibility traces through colored noises. In IEEE International
Conference on Ultra Modern Control Systems (ICUMT), 2010a.
M. Geist and O. Pietquin. Kalman temporal differences. Journal of Artifical Intelligence
Research (JAIR), 39:483–532, 2010b.
M. Geist and O. Pietquin. Algorithmic survey of parametric value function approximation.
IEEE Transactions on Neural Networks and Learning Systems, 24(6):845 – 867, 2013.
M. Kearns and S. Singh. Bias-variance error bounds for temporal difference updates. In
Conference on Learning Theory (COLT), 2000.
J.Z. Kolter. The fixed points of off-policy TD. In Advances in Neural Information Processing
Systems (NIPS), 2011.
331

Geist and Scherrer

M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning
Research, 4:1107–1149, 2003. ISSN 1533-7928.
H.R. Maei and R.S. Sutton. GQ(λ): A general gradient algorithm for temporal-difference
prediction learning with eligibility traces. In Conference on Artificial General Intelligence
(AGI), 2010.
R. Munos. Error bounds for approximate policy iteration. In International Conference on
Machine Learning (ICML), 2003.
A. Nedić and D.P. Bertsekas. Least squares policy evaluation algorithms with linear function
approximation. Discrete Event Dynamic Systems, 13:79–110, 2003.
D. Precup, R.S. Sutton, and S.P. Singh. Eligibility traces for off-policy policy evaluation.
In International Conference on Machine Learning (ICML), 2000.
D. Precup, R.S. Sutton, and S. Dasgupta. Off-policy temporal-difference learning with
function approximation. In International Conference on Machine Learning (ICML), 2001.
R.S. Randhawa and S. Juneja. Combining importance sampling and temporal difference
control variates to simulate Markov chains. ACM Transactions on Modeling and Computer
Simulation, 14(1):1–30, 2004.
B.D. Ripley. Stochastic Simulation. Wiley & Sons, 1987.
B. Scherrer. Should one compute the temporal difference fix point or minimize the Bellman
residual? The unified oblique projection view. In International Conference on Machine
Learning (ICML), 2010.
B. Scherrer and M. Geist. Recursive least-squares learning with eligibility traces. In European
Workshop on Reinforcement Learning (EWRL), 2011.
R. Schoknecht. Optimality of reinforcement learning algorithms with linear function approximation. In Advances in Neural Information Processing Systems (NIPS), 2002.
R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). MIT Press, 3rd edition, 1998.
R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvári, and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approximation. In International Conference on Machine Learning (ICML), 2009.
J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.
H. Yu. Convergence of least-squares temporal difference methods under general conditions.
In International Conference on Machine Learning (ICML), 2010a.
H. Yu. Least squares temporal difference methods: An analysis under general condtions.
Technical Report C-2010-39, University of Helsinki, September 2010b.
332

Off-policy Learning with Traces

H. Yu and D.P. Bertsekas. New error bounds for approximations from projected linear
equations. Technical Report C-2008-43, Dept. Computer Science, Univ. of Helsinki, July
2008.

333

