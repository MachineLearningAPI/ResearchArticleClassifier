WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

Context-aware Citation Recommendation
Qi He

Jian Pei

Daniel Kifer

Penn State University

Simon Fraser University

Penn State University

qhe@ist.psu.edu
jpei@cs.sfu.ca
dan+www10@cse.psu.edu
C. Lee Giles
Prasenjit Mitra
Penn State University

Penn State University

pmitra@ist.psu.edu

giles@ist.psu.edu

ABSTRACT
When you write papers, how many times do you want to
make some citations at a place but you are not sure which
papers to cite? Do you wish to have a recommendation
system which can recommend a small number of good candidates for every place that you want to make some citations? In this paper, we present our initiative of building a
context-aware citation recommendation system. High quality citation recommendation is challenging: not only should
the citations recommended be relevant to the paper under
composition, but also should match the local contexts of the
places citations are made. Moreover, it is far from trivial to
model how the topic of the whole paper and the contexts of
the citation places should aﬀect the selection and ranking of
citations. To tackle the problem, we develop a context-aware
approach. The core idea is to design a novel non-parametric
probabilistic model which can measure the context-based
relevance between a citation context and a document. Our
approach can recommend citations for a context eﬀectively.
Moreover, it can recommend a set of citations for a paper
with high quality. We implement a prototype system in
CiteSeerX. An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates
the eﬀectiveness and the scalability of our approach.

Figure 1: A manuscript with citation placeholders
and recommended citations. The text is from Section 2.2.

citations should be added. In order to ﬁll in those citation placeholders, one needs to search the relevant literature
and ﬁnd a small number of proper citations. Searching for
proper citations is often a labor-intensive task in research paper composition, particularly for junior researchers who are
not familiar with the very extensive literature. Moreover,
the volume of research undertaken and information available make citation search hard even for senior researchers.
Do you wish to have a recommendation system which can
recommend a small number of good candidates for every
place that you want to make some citations? High quality
citation recommendation is a challenging problem for many
reasons.
For each citation placeholder, we can collect the words surrounding as the context of the placeholder. One may think
we can use some keywords in the context of a placeholder
to search a literature search engine like Google Scholar or
CiteSeerX to obtain a list of documents as the candidates
for citations. However, such a method, based on keyword
matching, is often far from satisfactory. For example, using query “frequent itemset mining” one may want to search
for the ﬁrst paper proposing the concept of frequent itemset
mining, e.g. [1]. However, Google Scholar returns a paper
about frequent closed itemset mining published in 2000 as
the ﬁrst result, and a paper on privacy preserving frequent
itemset mining as the second choice. [1] does not appear in
the ﬁrst page of the results. CiteSeerX also lists a paper
on privacy preserving frequent itemset mining as the ﬁrst
result. CiteSeerX fails to return [1] on the ﬁrst page, either.
One may wonder, as we can model citations as links from
citing documents to cited ones, can we use graph-based link
prediction techniques to recommend citations? Graph-based
link prediction techniques often require a user to provide
sample citations for each placeholder, and thus shifts much
of the burden to the user. And, graph-based link prediction

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Design, Experimentation

Keywords
Bibliometrics, Context, Gleason’s Theorem, Recommender
Systems

1.

INTRODUCTION

When you write papers, how many times do you want to
make some citations at a place but you are not sure which
papers to cite? For example, the left part of Figure 1 shows
a segment of a query manuscript containing some citation
placeholders (placeholders for short) marked as “[?]”, where
Copyright is held by the International World Wide Web Conference Committee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

421

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

Figure 2: A demo of our context-aware citation recommendation system.

methods may encounter diﬃculties to make proper citations
across multiple communities because a community may not
be aware of the related work in some other community.
A detailed natural language processing analysis of the fulltext for each document may help to make good citation recommendations, but unfortunately has to incur serious scalability issues. There may be hundreds of thousands of papers
that need to be compared with the given manuscript. Thus,
the natural language processing methods cannot be straightforwardly scaled up to large digital libraries and electronic
document archives.
The recommended citations for placeholders should satisfy
two requirements. First, a citation recommendation needs
to be explainable. Our problem is diﬀerent from generating a bibliography list for a paper where a recommendation
should discuss some ideas related to the query manuscript.
In our problem, a recommended citation for a placeholder
in a query manuscript should be relevant and authoritative
to the particular idea that is being discussed at that point
in the query manuscript. Diﬀerent placeholders in the same
query manuscript may need diﬀerent citations.
Second, the recommendations for a manuscript need to
take into account the various ideas discussed in the manuscript.
For example, suppose we are asked to recommend citations
for a query manuscript in which one section discusses mixture models and another section discusses nonparametric
distributions. Citations to nonparametric mixture models
may be ranked high since they are related to both sections
in the same manuscript.
In summary, citation recommendation for placeholders
(and for the overall bibliography) is a challenging task. The
recommendations need to consider many factors: the query
manuscript in whole, the contexts of the citation placeholders individually and collectively, and the literature articles
individually. We need to construct an elegant mathematical
framework for relevance and develop an eﬃcient and scalable
implementation.
In this paper, we present an eﬀective solution to the problem of citation recommendations for placeholders in query
manuscripts. Our approach is context-aware, where a con-

422

text is a snippet of the text around a citation or a placeholder. The core idea is to design a novel non-parametric
probabilistic model which can measure the context-based
relevance between a citation context and a document. Our
approach can recommend citations for a context eﬀectively,
which is the innovative part of the paper. Moreover, it can
recommend a set of citations for a paper with high quality.
In addition to the theoretical contribution, we also implement a prototype system in CiteSeerX. Figure 2 shows a real
example in our demo system, where a user submits an citation context with a placeholder and the title/atract. Among
the top 6 results, the 1st, 4th, 5th and 6th ones are proper
recommendations relevant and new to the manuscript; the
2nd one is the original citation; the 3rd one is a similar but
irrelevant recommendation.
We also present an extensive empirical evaluation in the
CiteSeerX digital library against many baselines. Our empirical study demonstrates the eﬀectiveness and the scalability of our approach.
The rest of this paper is organized as follows. We discuss
related work in Section 2. We formalize the problem in Section 3. We discuss candidate retrieval methods in Section 4.
We present our context-aware relevance model for ranking in
Section 5. We report our empirical evaluation in Section 6.
Section 7 concludes the paper.

2. RELATED WORK
In this section, we discuss the related work on document
recommendation, topic-based link prediction, and analysis
of citation contexts.

2.1 Document Recommendation Systems
There are some previous eﬀorts on recommending a bibliography list for a manuscript, or recommending papers to
reviewers. The existing techniques generally rely on a user
proﬁle or a partial list of citations.
Basu et al. [4] focused on recommending conference paper submissions to reviewers based on paper abstracts and
reviewer proﬁles. Reviewer proﬁles are extracted from the
Web. This is a speciﬁc step in a more general problem known
as the Reviewer Assignment Problem, surveyed by Wang et
al. [26]. Chandrasekaran et al. [6] presented a technique to
recommend technical papers to readers whose proﬁle information is stored in CiteSeer. A user’s publication records
are used to model her proﬁle. User proﬁles and documents
are presented as hierarchial concept trees with predeﬁned
concepts from the ACM Computing Classiﬁcation System.
The similarity between a user proﬁle and a document is measured by the weighed tree edit distance. Our work can also
be seen as a proﬁle-based system, where a query manuscript
is a proﬁle. However, our system uses richer information
than just predeﬁned concepts or paper abstracts.
Shaparenko and Joachims [22] proposed a technique based
on language modeling and convex optimization to recommend documents. For a large corpus, the k-most similar
documents based on cosine similarity are retrieved. However, similarity based on full-text is too slow for large digital
libraries. Furthermore, according to our experiments, similarity based on document abstract results in poor recall (cf.
Table 1).
Some previous studies recommend citations for a manuscript
already containing a partial list of citations. Speciﬁcally,
given a document d and its partial citation list r , those

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA
automatically learn the motivation functions (e.g., compare,
contrast, use of the citation, etc.) of citations by using citation context based features. The second group tries to
enhance topical similarity between citations. For example,
Huang et al. [11] observed that citation contexts can eﬀectively help to avoid “topic drifting” in clustering citations
into topics. Ritchie [21] extensively examined the impact of
various citation context extraction methods on the performance of information retrieval.
The previous studies clearly indicate that citation contexts are a good summary of the contributions of a cited
paper and clearly reﬂect the information needs (i.e., motivations) of citations. Our work moves one step ahead to
recommend citations according to contexts.

studies try to recover the complete citation list denoted by
r ⊃ r . Collaborative ﬁltering techniques have been widely
applied. For example, McNee et al. [16] built various rating matrices including a author-citation matrix, a papercitation matrix, and a co-citation matrix. Papers which are
co-cited often with citations in r are potential candidates.
Zhou et al. [27] propagated the positive labels (i.e., the existing citations in r ) in multiple graphs such as the paperpaper citation graph, the author-paper bipartite graph, and
the paper-venue bipartite graph, and learned the labels of
the rest documents for a given testing document in a semisupervised manner. Torres et al. [25] used a combination of
context-based and collaborative ﬁltering algorithms to build
a recommendation system, and reported that the hybrid algorithms performed better than individual ones. Strohman
et al. [23] experimented with a citation recommendation system where the relevance between two documents is measured by a linear combination of text features and citation
graph features. They concluded that similarity between bibliographies and Katz distance [15] are the most important
features. Tang and Zhang [24] explored recommending citations for placeholders in a very limited extent. In particular,
a user must provide a bibliography with papers relevant to
each citation context, as this information is used to compute
features in the hidden layer of a Restricted Boltzmann Machine before predictions can be made. We feel this requirement negates the need for requesting recommendations.
In our study, we do not require a partial list of citations,
since creating such a list for each placeholder shifts most of
the burden on the user. Thus, we can recommend citations
both globally (i.e., for the bibliography) and also locally (i.e.,
for each placeholder, the innovative part of this paper).

3. PROBLEM DEFINITION AND ARCHITECTURE
In this section, we formulate the problem of context-based
citation recommendation, and discuss the preprocessing step.

3.1 Problem Definition
Let d be a document, and D be a document corpus.
Definition 3.1. In a document d, a context c is a bag
of words. The global context is the title and abstract of
d. The local context is the text surrounding a citation or
placeholder. If document d1 cites document d2 , the local
context of this citation is called an out-link context with
respect to d1 and an in-link context with respect to d2 .
A user can submit either a manuscript (i.e., a global context and a set of out-link local contexts) or a few sentences
(i.e., an out-link local context) as the query to our system.
There are two types of citation recommendation tasks, which
happen in diﬀerent application scenarios.

2.2 Topic-based Citation Link Prediction
Topic models are unsupervised techniques that analyze
the text of a large document corpus and provide a lowdimensional representation of documents in terms of automatically discovered and comprehensible “topics”. Topic
models have been extended to handle citations as well as
text, and thus can be used to predict citations for bibliographies. The aforementioned work of Tang and Zhang [24]
ﬁts in this framework. Nallapati et al. [18] introduced a
model called Pairwise-Link-LDA which models the presence
or absence of a link between every pair of documents and
thus does not scale to large digital libraries. Nallapati et
al. [18] also introduced a simpler model that is similar to
the work of Cohn and Hofmann [7] and Erosheva et al. [9].
Here citations are modeled as a sample from a probability
distribution associated with a topic. Thus, a paper can be
associated with topics when it is viewed as a citation. It
can also be associated with topics from the analysis of its
text. However, there is no mechanism to enforce consistency
between the topics assigned in those two ways.
In general, topic models require a long training process
because they are typically trained using iterative techniques
such as Gibbs Sampling or variational inference. In addition
to this, they must be retrained as new documents are added.

Definition 3.2 (Global Recommendation). Given a
query manuscript d without a bibliography, a global recommendation is a ranked list of citations in a corpus D that
are recommended as candidates for the bibliography of d.
Note that diﬀerent citation contexts in d may express different information needs. The bibliography candidates provided by a global recommendation should collectively satisfy
the citation information needs of all out-link local contexts
in the query manuscript d.
Definition 3.3 (Local Recommendation). Given an
out-link local context c∗ with respect to d, a local recommendation is a ranked list of citations in a corpus D that
are recommended as candidates for the placeholder associated with c∗ .
For local recommendations, the query manuscript d is an
optional input and it is not required to already contain a
representative bibliography.
To the best of our knowledge, global recommendations
are only tackled by document-citation graph methods (e.g.,
[23]) and topic models (e.g., [24, 18]). However, the contextaware approaches have not been considered for global or
local recommendations (except in a limited case where a
bibliography with papers relevant to each citation context is
required as the input).

2.3 Citation Context Analysis
The prior work on analyzing citation contexts mainly belongs to two groups. The ﬁrst group tries to understand
the motivation functions of an existing citation. For example, Aya et al. [2] built a machine learning algorithm to

423

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

Figure 3: Context-oblivious methods for global recommendation.

Figure 4: Context-aware methods.
The singlecontext-based method is for local recommendation
with the absence of query manuscript d1 . The hybrid method is for global recommendation and local
recommendation with the presence of d1 , where the
ﬁnal candidate set is the union of the candidate sets
derived from each local out-link context in d1 .

3.2 Preprocessing
Our proposed context-based citation recommendation system can take two types of inputs, a query manuscript d1 or
just a single out-link local context c∗ . We preprocess the
query manuscript d1 by extracting its global context (i.e.
the title and abstract) and all of its out-link local contexts.
Extracting the local contexts from d1 is not a trivial task.
Ritchie [21] conducted extensive experiments to study the
impact of diﬀerent lengths of local citation contexts on information retrieval performance, and concluded that ﬁxed
window contexts (e.g. size of 100 words) are simple and reasonably eﬀective. Thus, before removing all stop words, for
each placeholder we extract the citation context by taking
50 words before and 50 words after the placeholder. This
preprocessing is eﬃcient. For a PDF document of 10 pages
and 20 citations, preprocessing takes on average less than
0.1 seconds.
The critical steps of the system are: (1) quickly retrieving
a large candidate set which has good coverage over the possible citations, and (2) for each placeholder associated with
an out-link local context and for the bibliography, ranking
the citations by relevance and returning the top K. The
next two sections focus on these critical steps.

3. The papers cited by documents already in a candidate
set, generated by some other method (e.g., GN or Author). We call this method CitHop.
4. The documents written by the authors whose papers
are already in a candidate set generated by some other
method. We call this method AuthHop.
Note that retrieval based on the similarity between the
entire text content of documents would be too slow. Thus,
these methods appear to provide a reasonable alternative.
However, the body of an academic paper covers many ideas
that would not ﬁt in its abstract and so many relevant documents will not be retrieved. Retrieval by author similarity
will add many irrelevant papers, especially if the authors
have a broad range of research interests and if CitHop
or AuthHop is used to further expand the candidate set.
Context-aware methods can avoid such problems.

4.2 Context-aware Methods
4.

THE CANDIDATE SET

The context-aware methods help improve coverage for recommendations by considering local contexts in the query
manuscript. In a query manuscript d1 , for each context c∗ ,
we can retrieve:

Citation recommendation systems, including our system
and especially those that compute intricate graph-based features [23, 27], ﬁrst quickly retrieve a large candidate set of
papers. Then, features are computed for papers in this set
and ranking is performed. This is done solely for scalability.
Techniques for retrieving this candidate set are illustrated
in Figures 3 and 4. Here, the query manuscript is represented by a partially shaded circle. Retrieved documents
are represented as circles with numbers corresponding to the
numbered items in the lists of retrieval methods discussed
below. In this section, we discuss two kinds of methods for
retrieving a candidate set. Those methods will be evaluated
in Section 6.2.

5. The top N papers whose in-link contexts are most similar to c∗ . We call this method LN (e.g., L100).
6. The papers containing the top-N out-link contexts most
similar to c∗ (these papers cite papers retrieved by
LN). When used in conjunction with LN, we call this
method LCN (e.g., LC100).
We found that method LCN is needed because frequently
a document from a digital library may have an out-link context that describes how it diﬀers from related work (e.g.,
“prior work required full bibliographies but we do not”).
Thus, while an out-link context usually describes a cited
paper, sometimes it may also describe the citing paper, and
this description may be what best matches an out-link context c∗ from a query manuscript.
The above 6 methods can be combined in some obvious
ways. For example, (L100+CitHop)+G1000 is the candidate set formed by the following process: for each context
c∗ in a query manuscript d1 , add the top 100 documents
with in-link local context most similar to c∗ (i.e. L100) and
all of their citations (i.e. CitHop) and then add the top

4.1 Context-oblivious Methods
The context-oblivious methods do not consider local contexts in a query manuscript that has no bibliography. For a
query manuscript d1 , we can retrieve
1. The top N documents with abstract and title most
similar to d1 . We call this method GN (e.g., G100,
G1000).
2. The documents that share authors with d1 . We call
this method Author.

424

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

1000 documents with abstract and title most similar to d1
(i.e. G1000).

5.

MODELING CONTEXT-BASED CITATION
RELEVANCE

In this section, we propose a non-parametric probabilistic model to measure context-based (and overall) relevance
between a manuscript and a candidate citation, for ranking retrieved candidates. To improve scalability, we use an
approximate inference technique which yields a closed form
solution. Our model is general and simple so that it can be
used to eﬃciently and eﬀectively measure the similarity between any two documents with respect to certain contexts
or concepts in information retrieval.

k

5.1 Context-Based Relevance Model

k

log cTi Td ci .

log pgen (ci ) +

Recall that a query manuscript d1 can have a global context c1 (e.g., a title and abstract, which describes the problem to be addressed) and the out-link local contexts c2 , . . . , ck1
(e.g., text surrounding citations) which compactly express
ideas that may be present in related work. A document d2
in an existing corpus D has a global context b1 (e.g. title and abstract), and local in-link contexts b1 , . . . , bk2 (e.g.,
text that is used by papers citing d2 ) which compactly express ideas covered in d2 .
In this section, we describe a principled and consistent way
of measuring sim(d1 , d2 ), deﬁned as the overall relevance of
d2 to d1 and sim(d1 , d2 ; c∗ ), deﬁned as the relevance of d2
to d1 with respect to a speciﬁc context c∗ (in particular, c∗
could be any of the out-link contexts ci ). Our techniques
are based on Gleason’s Theorem specialized to ﬁnite dimensional real vector spaces.

i=1

i=1

Now, if there is only one concept (i.e. k = 1), then the
maximum likelihood estimator is easy to compute: it is Td =
c1 ct1 . In the general case, however, numerical methods are
needed [3]. The computational cost of estimating Td can be
seen from the following proposition:
Proposition 5.2. The density matrix Td can be represented as ri=1 ti tTi where the ti are a set of at most r orthogonal column vectors with
(ti · ti ) = 1, and r is the
dimension of the space spanned by the ci (the number of linearly independent contexts). There are O(r 2 ) parameters to
determine and numerical (iterative) techniques will scale as
a polynomial in r 2 .
The detailed proof is in Appendix A.
Furthermore, the addition of new documents to the corpus
will cause the addition of new in-link contexts, requiring a
recomputation of all the Td . Thus the likelihood approach
will not scale for our system.

Theorem 5.1 (Gleason [10]). For an n-dimensional
real vector space V (with n ≥ 3), let p be a function that
assigns a number in [0, 1] to each subspace of V such that
p(v1 ⊕ v2 ) = p(v1 ) + p(v2 ) whenever v1 and v2 are orthogonal subspaces 1 and p(V ) = 1. Then p(v) = T race(T Pv )
where Pv is the projection matrix for subspace v (e.g. Pv w
is the projection of vector w onto the subspace v), and T is
a density matrix – a symmetric positive semidefinite matrix
whose trace is 1.

5.2 Scalable Closed Form Solutions
Since hundreds of thousands of density matrices need to
be estimated (one for each document) and recomputed as
new documents are added to the corpus, we opt to replace
the exact but slow maximum likelihood computation with an
approximate but fast closed form solution which we derive
in this section.
We begin with the following observation. For each concept ci , the maximum likelihood estimate of Td given that
concept ci is relevant is ci cTi . It stands to reason that our
overall estimate of Td should be similar to each of the ci cTi .
We will measure similarity by the Frobenius norm (squareroot of the sum of the squared matrix entries) and thus set
up the following optimization problem:

Note that van Rijsbergen [20] proposed using Gleason’s Theorem as a model for information retrieval, and this was also
extensively studied by Melucci [17]. However, our framework is substantially diﬀerent from their proposals since it
relies on comparisons between density matrices.
Let W be the set of all words in our corpus and let |W | be
the number of words. The vector space V is |W |-dimensional
with one dimension for each word.
In this framework, atomic concepts will be represented as
one-dimensional vector spaces and we will treat each context
(global, in-link, out-link) as an atomic concept. Each atomic
concept c will be associated a unit column vector which we
shall also denote by c (one such representation can be derived from normalizing tf-idf scores into a unit vector). The
projection matrix for c is then ccT . Our goal is to measure
the probability that c is relevant to a document d and so,
by Gleason’s Theorem, each document d is associated with
a density matrix Td . The probability that c is relevant to
1

d is then pd (c) = T race(Td ccT ) = cT Td c. Note that similar
atomic concepts (as measured by the dot product) will have
similar relevance scores.
Now, the probability distribution characterized by Gleason’s theorem is not a generative distribution – it cannot be
used to sample concepts. Instead, it is a distribution over
yes/no answers (e.g. is concept c relevant or not?). Thus
to estimate Td for a document d we will need some (unknown) generative distribution pgen over unit vectors (concepts). Our evidence about the properties of Td come from
the following process. pgen independently generates k concepts c1 , . . . , ck and these concepts are then independently
judged to be relevant to d. This happens with probability
k
i=1 pgen (ci )pd (ci ). We seek to ﬁnd a density matrix Td
that maximizes the likelihood. The log likelihood is:

k

minimize L(Td ) =

||Td − ci cTi ||2F ,

i=1

subject to the constraint that Td is a density matrix. Taking
derivatives, we get
k

∂L
=2
(Td − ci cTi ) = 0,
∂Td
i=1
leading to the solution Td = k1 ki=1 ci cTi . Now that we have
a closed form estimate for Td , we can discuss how to measure
the relevance between documents with respect to a concept.

v1 ⊕ v2 is the linear span of v1 and v2 .

425

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA
nodes2 , each of which has 8 CPU processors of 2.67GHz and
32G main memory.

Global Recommendation
Let Td1 and Td2 be the respective density matrices of the
manuscript d1 and a document d2 from the corpus D. We
deﬁne sim(d1 , d2 ), the overall relevance of d2 to d1 to be the
probability that a random context drawn from the uniform
distribution over contexts is relevant to both d1 and d2 . After much mathematical manipulation, we get the following:
Proposition 5.3. Let Td1 =
T d2 =

1
k2

k2
T
i=1 bi bi .

1
k1

k1
T
i=1 ci ci

6.1 Performance Measures
The performance of recommendation can be measured by
a wide range of metrics and means, including user studies
and click-through monitoring. For experimental purpose, we
focus on four performance measures as follows in this paper.
Recall (R): We removed original citations from the testing documents. The recall is deﬁned as the percentage of
original citations that appear in the top K recommended citations. Furthermore, we categorize recall into global recall
and local recall for global and local recommendation respectively. The global recall is ﬁrst computed as the percentage
of original bibliography of each testing document d that appears in the top K recommended citations of d, and then
averaged over all 1, 612 testing documents. The local recall
is ﬁrst computed as, for each citation placeholder, the percentage of the original citations cited by c∗ that appear in
the top K recommended citations of c∗ , and then averaged
over all 7, 049 testing out-link citation contexts.
Co-cited probability (P): We may recommend some relevant or even better recommendations other than those original ones among the top K results, which cannot be captured
by the traditional metric like precision. The previous work
usually conducted user studies for this kind of relevance evaluation [16, 6]. In this paper, we instead use the wisdom of
the population as the ground truth to deﬁne a systematic
metric. For each pair of documents di , dj where di is an
original citation and the dj is a recommended one, we calculate the probability that these two documents have been
co-cited by the popularity in the past as

and

Then the relevance of d2 to d1 is:

sim(d1 , d2 ) =

1
k1 k2

k1

k2

(ci · bj )2 .

(1)

i=1 j=1

The detailed proof is given in Appendix B. Given query
manuscript q1 , we use Equation 1 to rank documents for
global recommendation.

Local Recommendation
If we are given a single context c∗ instead of a manuscript, we
can still compute sim(c∗ , d2 ), the relevance of a document
2
bi bTi , then
d2 ∈ D to the context c∗ . Letting Td2 = k12 ki=1
by Gleason’s Theorem, we have:
sim(c∗ , d2 ) = T race(Td2 c∗ cT∗ ) =

1
k2

k2

(bj · c∗ )2 .

(2)

j=1

We deﬁne sim(d1 , d2 ; c∗ ), the relevance of d2 to d1 with
respect to context c∗ as the probability that c∗ is relevant
to both documents. Applying Gleason’s Theorem twice:
sim(d1 , d2 ; c∗ ) =
=

T race(Td1 c∗ cT∗ )T race(Td2 c∗ cT∗ )
1
k1 k2

k1
i=1

(ci · c∗ )2

k2

P =
(bj · c∗ )2 .

(3)

The co-cited probability is then averaged over all K ·l unique
document pairs for the top K results, where l is the number
of original citations. Again, we categorize this probability
into a global version and a local version: the former is averaged over all testing documents and the latter is averaged
over all testing citation contexts.
NDCG: The eﬀectiveness of a recommendation system
is also sensitive to the positions of relevant citations, which
cannot be evaluated by recall and co-cited probability. Intuitively, it is desirable that highly relevant citations appear
earlier in the top K list. We use normalized discounted
cumulative gain (NDCG) to measure the ranked recommendation list. The NDCG value of a ranking list at position i
is calculated as

j=1

Given query manuscript d1 , we use Equation 3 to rank documents for recommendations at the placeholder associated
with context c∗ in d1 . If a citation context c∗ is given without d1 , then we use Equation 2.

6.

number of papers citing both di and dj
.
number of papers citing di or dj

EXPERIMENTS

We built a real system in the CiteSeerX staging server
to evaluate context-aware citation recommendations. We
used all research papers published and crawled before year
2008 as the document corpus D. After removing duplicate
papers and the papers missing abstract/title and citation
contexts, we obtained 456, 787 unique documents in the corpus. For each paper, we extracted its title and abstract
as the global citation context or text content. Within a
paper, we simply took 50 words before and after each citation placeholder as its local citation context. We removed
some popular stop words. In order to preserve the timesensitive past/present/future tenses of verbs and the singular/plural styles of named entities, no stemming was done.
All words were transferred to lower-cases. Finally, we obtained 1, 810, 917 unique local citation contexts and 716, 927
unique word terms. We used all 1, 612 papers published and
crawled in early 2008 as the testing data set.
We implemented all algorithms in C++ and integrated
them into the Java running environment of CiteSeerX. All
experiments were conducted on a Linux cluster with 128

i

N DCG@i = Zi
j=1

2r(j) − 1
,
log(1 + j)

where r(j) is the rating of the j-th document in the ranking list, and the normalization constant Zi is chosen so that
the perfect list gets a NDCG score of 1. Given a testing
document d1 and any other document d2 from our corpus
D, we use the average co-cited probability of d2 with all
original citations of d1 to weigh the citation relevance score
of d2 to d1 . Then, we sort all d2 w.r.t. this score (suppose
Pmax is the highest score) and deﬁne 5-scale relevance number for them as the ground truth: 4, 3, 2, 1, 0 for documents
2

426

Due to the PSU policy, we could only use 8 of them.

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

Recommendation Quality

Table 1: Compare diﬀerent candidate sets. Numbers are averaged over 1,612 test documents.
Methods
Coverage
G1000
0.44
L100
0.55
LC100
0.63
L1000
0.69
LC1000
0.78
Author
0.05
L100+CitHop
0.61
L1000+CitHop
0.72
LC100+CitHop
0.73
G1000+CitHop
0.73
LC1000+CitHop
0.83
Author+CitHop
0.15
L100+G1000
0.75
LC100+G1000
0.79
(L100+CitHop)+G1000
0.79
(LC100+CitHop)+G1000
0.85
(LC1000+G1000)+CitHop
0.92
LC100+G1000+(Author+CitHop) 0.79
(LC100+G1000)+AuthHop
0.88

We compare CRM with 7 other context-oblivious baselines:
HITs [13]: the candidates are ranked w.r.t. their authority scores in the candidate set subgraph. We choose to compare with HITs because it is interesting to see the diﬀerence
between the popularity (link-based methods) and the relevance (context or document based methods).
Katz [15]: the candidates are ranked w.r.t. the Katz
distance, i β i Ni , where Ni is the number of unique paths
of length i between the query manuscript/context and the
candidate and the path should go through the top N similar
documents/contexts of the query manuscript/context, and
β i is a decay parameter between 0 and 1. We choose to
compare Katz because this feature has been shown to be
the most eﬀective among all text/citation features in [23]
for document-based global recommendation. Note that [23]
used the ground truth to calculate the Katz distance, which
is impractical.
l-count and g-count: the candidates are ranked according to the number of citations in the candidate set subgraph
(l-count) or the whole corpus (g-count).
textsim: the candidates are ranked according to similarity with the query manuscript using only title and abstract.
This allows us to see the beneﬁt of a context-aware approach.
diﬀusion: the candidates are ranked according to their
topical similarities which are generated by the multinomial
diﬀusion kernel [14] to the query manuscript, K(θ1 , θ2 ) =
√
√
|W |
(4πt)− 2 exp − 1t arccos2 ( θ1 · θ2 ) , where θ1 and θ2 are
topic distributions of the query manuscript d1 and the candidate d2 respectively, t is the decay factor. Since we only
care about the ranking, we can ignore the ﬁrst item and t.
We choose to compare with it because topic-based citation
recommendation [24] is one of related work and the multinomial diﬀusion kernel is the state-of-the-art tool in topicbased text similarity [8]. We run LDA [5] on each candidate
set online (by setting the number of topics as 60) to get the
topic distributions for documents.
mix-features: the candidates are ranked according to
the weighted linear combination of the above 6 features. We
choose to compare it because in [23], a mixture approach
considering both text-based features and citation-based features is the most eﬀective.
Figures 5 (a), (b) and (c) illustrate their performances on
recall, co-cited probability and NDCG, respectively.
Among all methods, g-count is the worst one simply because it is measured over the whole corpus, not the candidate
set. Context-oblivious content-based methods like textsim
and diﬀusion come to heel, indicating that abstract/title
only are too sparse to portray the speciﬁc citation functions
well. Moreover, they cannot ﬁnd the proper related work
that uses diﬀerent conceptual words. The diﬀusion is better
than textsim, indicating that topic-based similarity can capture the citation relations more than raw text similarity. The
social phenomenon that the rich get richer is also common
in the citation graph, since the citation features including
l-count, HITs, and Katz work better than the abstract/titlebased text features. Interestingly, [23] claimed that citation
features did a poor job at coverage. But on our data, they
have higher recall values than text features. A combination
of these features (mix-features) can further improve the performance, especially on the recall and NDCG, which means
that if a candidate is recommended by multiple features, it

Candidate Set Size
1000
341
674
2,844
5,692
17
1,327
9,049
3,561
3,790
22,629
63
1,312
1,618
2,279
4,460
24,793
1,674
39,496

in (3Pmax /4, Pmax ], (Pmax /2, 3Pmax /4], (Pmax /4, Pmax /2],
(0, Pmax /4] and 0 respectively. Finally, the NDCG over all
testing documents (the global version) or all testing citation contexts (the local version) is averaged to yield a single
qualitative metric for each recommendation problem.
Time: We use the running time as an important metric to
measure the eﬃciency of the recommendation approaches.

6.2 Retrieving Candidate Sets
Table 1 evaluates the quality of diﬀerent candidate set retrieval techniques (see Section 4 for notation and detailed
descriptions). Here we measure coverage (the average recall
of the candidate set with respect to the true bibliography
of a testing document) and candidate set size. A good candidate set should have high recall and a small size since
large candidate sets slow down the ﬁnal ranking for all recommendation systems. For context-aware methods, we feel
LC100+G1000 achieves the best tradeoﬀ between candidate set size and coverage. For context-oblivious methods,
G1000+CitHop works reasonably well (but not as well as
LC100+G1000). However, the retrieval time of contextoblivious methods is around 0.28 seconds on an 8-node cluster. On the other hand, the retrieval time of context-aware
methods ranges from 2 to 10 seconds on the same cluster (depending on the number of out-link contexts3 of a
query manuscript). Our goal for the ﬁnal system is to use
LC100+G1000 and to speed up retrieval time using a combination of indexing tricks and more machines (since this retrieval is highly parallelizable). Note, however, that ranking
techniques are orthogonal to candidate set retrieval methods. Thus, when we compare our ranking methods for recommendations with baselines and related work, we will use
LC100+G1000 as the common candidate set, since our
eventual goal is to use this retrieval method in our system.

6.3 Global Recommendation
In this section, we compare our context-aware relevance
model (CRM for short, Section 5) with other baselines in
global recommendation performance, since the related work
only focused on recommending the bibliography.
3
Due to OCR and crawling issues, there were an average of
5 out-link contexts per testing document.

427

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

(a) recall

(b) co-cited probability

(c) NDCG

Figure 5: Compare performances for global recommendation.










	





	








should be ranked higher. An interesting ﬁnding is that the
performance of HITs (especially on NDCG) increases significantly after more candidates are returned, indicating that
some candidates with moderate authoritative scores are our
targets (people may stop citing the most well-known papers
after they become the standard techniques in their domains
and shift the attentions to other recent good papers). Katz
works the best among single features, partially because it implicitly combines the text similarity with the citation count.
It works like the collaborative ﬁltering, where candidates often cited by similar documents are recommended. Finally,
our CRM method leads the pack on all three metrics, implying that after considering all historical in-link citation
contexts of candidates (then the problem of using diﬀerent
conceptual words for the same concept would be alleviated),
CRM is eﬀective in recommending bibliography.











	

(a) scatter distribution





	
		

(b) probability distribution

Figure 7: Correlation between count/probability of
missed/hit citations and their number of historical
in-link citation contexts.

crawling and OCR issues, our testing documents have an average of 5 out-link citation contexts that point to documents
in our corpus, so “top 5 per context” corresponds to “top
25 per document”), the performance of CRM-crosscontex
is very close to CRM in global recommendation. We know
CRM-crosscontext and CRM have the same input (the same
amount of information to use). However, CRM-crosscontext
tackles a much harder problem than CRM, and has to assign citations to each placeholder. Thus, CRM-crosscontext
is more capable than CRM. CRM-crosscontext outperforms
all baselines of global recommendation and thus is also superior than their local versions. Limited by space, we omit
the details here.
CRM-crosscontext is able to eﬀectively rank candidates
for placeholders. For example, more than 42% original citations can be found in the top 5 recommendations, and
frequently co-cited papers (w.r.t. original citations) also appear early as indicated by NDCG. On the other hand, CRMsinglecontext uses much less information (without global
context and other coupling contexts) but still achieves reasonable performance. For example, if a user only inputs 100
words as the query, around 34% original citations can be
found in the top 5 list.
One may wonder what would happen to some documents
in the corpus which do not have enough in-link citation contexts in history. Given an original citation from a query
manuscript, we examine the correlation of its missing/hit
probability to its number of historical in-link citation contexts in CRM, shown in Figure 7.

Ranking Time
Time is not a major issue for most ranking algorithms except
for topic-based methods, where LDA usually took tens of
seconds (50 ∼ 100) for each new candidate set. Thus, topicbased methods including diﬀusion and mix-features are not
suitable for online recommendation systems. All other ranking algorithms need less than 0.1 seconds. Limited by space,
the detailed comparisons are omitted here.

6.4 Local Recommendation
Local recommendation is a novel task proposed by our
context-aware citation recommendation system. Here, we
do not compare with the above baselines because they are
not tailored for local recommendation. Instead, we evaluate
the impact of the absence/presence of the global context and
other local contexts, and analyze the problem of contextaware methods.
If a user only inputs a bag of words as the single context
to request recommendations, we can then only use Equation 2 to rank candidates. We name this method as CRMsinglecontext. If a user inputs a manuscript with placeholders inside, we can then rank candidates for each placeholder using Equation 3. We name this method as CRMcrosscontext. Figure 6 illustrates the performance of these
two kinds of local recommendations on recall, co-cited probability and NDCG.
CRM-crosscontext is rather eﬀective. In Figure 5 (due to

428

WWW 2010 • Full Paper

(a) recall

April 26-30 • Raleigh • NC • USA

(b) co-cited probability

(c) NDCG

Figure 6: Evaluate local recommendations.

The correlation results clearly indicate that the missing/hit
probability of an original citation declines/raises proportionally to its number of historical in-link contexts. In fact,
for those new corpus documents without any in-link contexts, our context-aware methods can still conduct contextoblivious document-based recommendation, except that we
still enhance the speciﬁc citation motivations of a query
manuscript using its out-link local contexts.

[5] D. Blei, A. Ng and M. Jordan. Latent dirichlet allocation.
J. Machine Learning Research 2003.
[6] K. Chandrasekaran, S. Gauch, P. Lakkaraju and H. Luong.
Concept-Based Document Recommendations for CiteSeer
Authors. Adaptive Hypermedia and Adaptive Web-Based
Systems, Springer, 2008.
[7] D. Cohn and T. Hofmann. The missing link - a
probabilistic model of document content and hypertext
connectivity. NIPS’01.
[8] F. Diaz. Regularizing Ad Hoc Retrieval Scores. CIKM’05.
[9] E. Erosheva, S. Fienberg and J. Laﬀerty. Mixed
7. CONCLUSIONS
membership models of scientiﬁc publications. PNAS 2004.
In this paper, we tackled the novel problem of context[10] A. Gleason. Measures on the Closed Subspaces of a Hilbert
Space. J. of Mathematics and Mechanics, 1957.
aware citation recommendation, and built a context-aware
[11] S. Huang, G. Xue, B. Zhang, Z. Chen, Y. Yu and W. Ma.
citation recommendation prototype in CiteSeerX. Our sysTSSP: A Reinforcement Algorithm to Find Related Papers.
tem is capable of recommending the bibliography to a manuscript
WI’04.
and providing a ranked set of citations to a speciﬁc cita[12] A. Jeﬀrey and H. Dai. Handbook of Mathematical
tion placeholder. We developed a mathematically sound
Formulas and Integrals. Academic Press, 2008.
context-aware relevance model. A non-parametric proba[13] J. Kleinberg. Authoritative sources in a hyperlinked
bilistic model as well as its scalable closed form solutions
environment. J. of the ACM, 1999.
are then introduced. We also conducted extensive experi[14] J. Laﬀerty and G. Lebanon. Diﬀusion Kernels on Statistical
ments to examine the performance of our approach.
Manifolds. J. of Machine Learning Research, 2005.
[15] D. Liben-Nowell and J. Kleinberg. The link prediction
In the future, we plan to make our citation recommenproblem for social networks. CIKM’03.
dation system publicly available. Moreover, we plan to de[16] S. McNee, I. Albert, D. Cosley, P. Gopalkrishnan, S. Lam,
velop scalable techniques for integrating various sources of
A. Rashid, J. Konstan and J. Riedl. On the Recommending
features, and explore semi-supervised learning on the partial
of Citations for Research Papers. CSCW’02.
list of citations in manuscripts.
[17] M. Melucci. A basis for information retrieval in context.
TOIS, 2008.
[18] R. Nallapati, A. Ahmed, E. Xing and W. Cohen. Joint
8. ACKNOWLEDGEMENT
latent topic models for text and citations. SIGKDD’08.
This work is supported in part by the National Science
[19] Z. Nie, Y. Zhang, J. Wen and W. Ma. Object-Level
Foundation Grants 0535656, 0845487, and 0454052, an NSERC
Ranking: Bringing Order to Web Objects. WWW’05.
Discovery grant and an NSERC Discovery Accelerator Sup[20] C. Rijsbergen. The Geometry of Information Retrieval.
plements grant. All opinions, ﬁndings, conclusions and recCambridge University Press, 2004.
ommendations in this paper are those of the authors and do
[21] A. Ritchie. Citation context analysis for information
retrieval. PhD thesis, University of Cambridge, 2008.
not necessarily reﬂect the views of the funding agencies.
[22] B. Shaparenko and T. Joachims. Identifying the Original
Contribution of a Document via Language Modeling.
9. REFERENCES
ECML, 2009.
[23] T. Strohman, W. Croft and D. Jensen. Recommending
Citations for Academic Papers. SIGIR’07 and Technical
[1] R. Agrawal, T. Imielinski and A. Swami. Mining
Report,
Association Rules Between Sets of Items in Large
http://ciir-publications.cs.umass.edu/getpdf.php?id=610.
Databases. SIGMOD, 1993.
[24] J. Tang and J. Zhang. A Discriminative Approach to
[2] S. Aya, C. Lagoze and T. Joachims. Citation Classiﬁcation
Topic-Based Citation Recommendations PAKDD’09.
and its Applications. ICKM’05.
[25] R. Torres, S. McNee, M. Abel, J. Konstan and J. Riedl.
[3] K. Banaszek, G. D’Ariano, M. Paris and M. Sacchi.
Enhancing Digitial Libraries with Techlens. JCDL’04.
Maximum-likelihood estimation of the density matrix.
[26] F. Wang, B. Chen and Z. Miao. A Survey on Reviewer
Physical Review A, 1999.
Assignment Problem. IEA/AIE’08.
[4] C. Basu, H. Hirsh, W. Cohen and C. Nevill-Manning.
[27] D. Zhou, S. Zhu, K. Yu, X. Song, B. Tseng, H. Zha and
Technical Paper Recommendation: A Study in Combining
L. Giles. Learning Multiple Graphs for Document
Multiple Information Sources. J. of Artificial Intelligence
Recommendations. WWW’08.
Research, 2001.

429

WWW 2010 • Full Paper

April 26-30 • Raleigh • NC • USA

APPENDIX
A.

Cartesian coordinates to hyperspherical coordinates as:
w1 = r cos θ1 , w2 = r sin θ1 cos θ2 , w3 = r sin θ1 sin θ2 cos θ3 ,

PROOF OF PROPOSITION 5.2

. . . , w|W |−1 = r sin θ1 sin θ2 . . . sin θ|W |−2 cos θ|W |−1 ,

By the spectral theorem, we can choose all the ti to be
orthogonal. Each ti can also be expressed as αi1 si + · · · +
αia sa +βi1 u1 +· · ·+βib ub where the si and uj are orthogonal
unit vectors such that the si span the same space as the ci .
The log likelihood is then
k

w|W | = r sin θ1 sin θ2 . . . sin θ|W |−2 sin θ|W |−1 ;
|W |
i=1

θ1 ∈ [0, π], θ2 ∈ [0, π], . . . , θ|W |−2 ∈ [0, π], θ|W |−1 ∈ [0, 2π);

k

log pgen (ci ) +
i=1

log pd (ci )

the absolute value of the determinant of the Jacobian is

i=1
k

k

=

log pgen (ci ) +
i=1

log
i=1

k

k

k

=

log pgen (ci ) +
i=1

r
j=1

T
cT
i tj tj ci =

log pgen (ci )+

r |W |−1

i=1

log
i=1

r
j=1 ([αi1 si

i=1

+ · · · + αia sa ] · ci )2 .

E[w14 ] =
=

1
(2π)|W |/2

1
(2π)|W |/2

=Φ
=Φ

π
0
π
0

= 2Φπ
= 2Φπ
= 2Φπ

=

Recall that we treat concepts as one-dimensional vector
spaces and thus can represent them as unit vectors. Let
p be the uniform distribution over unit vectors. It is wellknown that sampling from p is equivalent to sampling |W |
independent standard Gaussian random variables (one for
each dimension) and dividing them by the square root of
their sum of squares. The similarity between d1 and d2 is:

=
=

1
k1 k2
|W |
δ=1

E

k2

i=1 j=1

b2j,δ wδ2 + 2

=Φ

= 2Φπ
= 2Φπ

T

ω= +1 =1

|W |

|W |

+2

ci,β wβ ci,γ wγ

×

β=γ+1 γ=1

bj,ω wω bj, w

|W |

E
α=1

=
=
=
=

k1 k2
1
k1 k2
i=1 j=1

4
c2i,α b2i,α wα
+

β=γ

2 2
c2i,β b2j,γ wβ
wγ + 4

δ>

...

π
0

2
x4
∞
1
e−( xi )/2 dx1
−∞ ( x2 )2
i
4
4
∞ r cos (θ1 ) |W |−1
r
×
0
r4

. . . dx|W |

sin|W |−1−i (θi )drdθ1 . . . dθ|W |−1

π
0
π
0

∞
−∞

1
(2π)|W |/2
2π
0

π
0

...

π
0

2
2
x2
∞
1 x2 e−( xi )/2 dx . . . dx
1
|W |
−∞ ( x2 )2
i
4
2
2
2
∞ r cos (θ1 ) sin (θ1 ) cos (θ2 ) |W |−1
r
×
0
r4

...

|W |−2

sin|W |−1−i (θi )drdθ1 . . . dθ|W |−1
i=1
cos2 (θ1 ) sin2 (θ1 ) cos2 (θ2 ) sin|W |−2 (θ1 ) sin|W |−3 (θ2 )dθ1 dθ2
sin|W | (θ1 ) − sin|W |+2 (θ1 ) sin|W |−3 (θ2 ) − sin|W |−1 (θ2 )

dθ1 dθ2
(|W |−1)!!
|+1)!!
− (|W
|W |!!
(|W |+2)!!
(|W |−4)!!
(|W |−3)!!
(|W |−3)!!
(|W |−2)!!
(|W |−4)!!
(|W |−3)!!
(|W |−3)!!
(|W |−2)!!

sim(d1 ; d2 )

.

Now, if all coordinates of w other than wi (for some i)
are ﬁxed, it is easy to see that the probability of wi = x
and wi = −x are the same, and so the coordinates of w
are jointly uncorrelated. This means that all terms with an
odd power of some coordinate will become 0 in the expected
value. Thus sim(d1 ; d2 ) simpliﬁes to:
sim(d1 ; d2 ) =

|W |−2

...

(|W |−4)!!
|−2)!!
− (|W
(|W |−3)!!
(|W |−1)!!
|W |−1
(|W |+1)(|W |−1)
− (|W |+2)|W |
|W |
1
.
|W |(|W |+2)

1−

|W |−2
|W |−1

Thus we have E[wi4 ] = 3E[wi2 wj2 ]. Thus sim(d1 ; d2 ) is proportional to E[wi2 wj2 ], a universal constant. We drop this
constant to simplify calculations. Thus our similarity is:

2

|W |
2
c2i,α wα
α=1
|W | |W |

π
0
π
0

= 2Φπ

(ci · w) (bj · w) p(w)dw

i=1 j=1
k1 k2

π
0

∞
−∞

π
(1 − sin2 (θ1 ))2 sin|W |−2 (θ1 ) sin|W |−3 (θ2 )dθ1 dθ2
0
π
sin|W |−2 (θ1 ) − 2 sin|W | (θ1 ) + sin|W |+2 (θ1 ) ×
0
sin|W |−3 (θ2 )dθ1 dθ2
(|W |−3)!!
|−1)!!
|+1)!!
(|W |−4)!!
− 2 (|W
+ (|W
(|W |−2)!!
(|W |)!!
(|W |+2)!!
(|W |−3)!!
(|W |−4)!!
(|W |−3)!!
|W |−1
|+1)(|W |−1)
1 − 2 |W | + (|W
(|W |−3)!!
(|W |−2)!!
(|W |+2)|W |
(|W |−4)!!
(|W |−3)!!
3
,
(|W |−3)!!
(|W |−2)!!
(|W |+2)|W |

1
(2π)|W |/2

=Φ

T race(Td1 ww )T race(Td2 ww )p(w)dw
2

.

by Wallis’s formula [12] where n!! is the double factorial (1
if n = 0 or 1; n!! = n × (n − 2)!! otherwise). Also,

PROOF OF PROPOSITION 5.3

k1

2π
0

i=1

E[w12 w22 ] =

1
k1 k2

sin|W |−1−i (θi )drdθ3 ...dθ|W |−1
i=3
(2π)|W |/2

The moment calculations are thus as follows:

Thus we need to multiply each ti by a constant γ > 1 to
increase the trace of Td to 1. Multiplication by such a γ will
then increase each of the quantities in the log terms of the log
likelihood. Thus γTd would have a higher log likelihood than
Td . Thus in the maximum likelihood solution Td = i ti tTi ,
each ti is in the subspace spanned by the contexts ci and
representing all of the ti requires O(r 2 ) parameters. So, a
numerical solution will have polynomial complexity in r 2 .

sim(d1 ; d2 ) =

|W |−2

2π π ... π ∞ r|W |−1
0
0
0 0

Φ≡

T race(Td ) =
i ti · ti
2
2
=
since the αi and βj are all orthogonal
i (αi1 + · · · + αia )
2
2
2
2
<
i (αi1 + · · · + αia + βi1 + · · · + βib ) = T race(Td ) = 1.

T

sin|W |−1−i (θi ).

Let us also deﬁne

Since the ui are orthogonal to the ci , we can increase the
likelihood by replacing each ti = αi1 si +· · ·+αia sa +βi1 u1 +
· · · + βib ub with ti = αi1 si + · · · + αia sa to get the matrix
Td = i ti tiT without changing the likelihood. However,

B.

|W |−2

k

+ · · · + αia sa + βi1 u1 + · · · + βib ub ] · ci )2

r
j=1 ([αi1 si

log
i=1

wi2 ≥ 0;

r=

ci,δ ci, bj,δ bj, wδ2 w 2 .

k1 k2
1
k1 k2
i=1 j=1
k1 k2
1
k1 k2
i=1 j=1
k1 k2
1
k1 k2
i=1 j=1
k1 k2
1
k1 k2
i=1 j=1

|W |

3
α=1
|W |

2
α=1

c2i,α b2i,α +
c2i,α b2i,α +

2(ci · bj )2 +

430

|W | |W |
β=1 γ=1

c2i,β b2j,γ

2(ci · bj )2 + 1, since the ci and bj are unit vectors.

Finally, after removing the additive and multiplicative constants (they don’t aﬀect ranking), we can use the following
equivalent formula:
sim(d1 ; d2 ) =

To compute the necessary moments, we change basis from

c2i,β b2j,γ + 4
ci,δ ci, bj,δ bj,
β=γ
δ>
|W | |W |
c2i,β b2j,γ + 4
ci,δ ci, bj,δ bj,
β=1 γ=1
δ>

k1 k2
1
(ci
k1 k2
i=1 j=1

· bj )2 .

