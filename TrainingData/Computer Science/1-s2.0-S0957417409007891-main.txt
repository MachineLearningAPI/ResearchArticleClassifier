Expert Systems with Applications 37 (2010) 2826–2837

Contents lists available at ScienceDirect

Expert Systems with Applications
journal homepage: www.elsevier.com/locate/eswa

Self-adaptive harmony search algorithm for optimization
Chia-Ming Wang a, Yin-Fu Huang b,*
a

Graduate School of Engineering Science and Technology, National Yunlin University of Science and Technology, 123 University Road, Section 3, Touliou,
Yunlin 640, Taiwan, ROC
b
Department of Computer Science and Information Engineering, National Yunlin University of Science and Technology, 123 University Road, Section 3,
Touliou, Yunlin 640, Taiwan, ROC

a r t i c l e

i n f o

Keywords:
Harmony search
Low-discrepancy sequence
Meta-heuristic algorithm
Optimization

a b s t r a c t
Recently, a new meta-heuristic optimization algorithm – harmony search (HS) with continuous design
variables was developed. This algorithm is conceptualized using the musical improvisation process of
searching for a perfect state of harmony. Although several variants and an increasing number of applications have appeared, one of its main difﬁculties is how to select suitable parameter values. In this paper,
we used the consciousness (i.e., harmony memory) to automatically adjust parameter values. In addition,
the pseudo-random number generator is also replaced by the low-discrepancy sequences for initialization of the harmony memory. Finally, the experimental results revealed the superiority of the proposed
method to the original HS and recently developed variants.
Ó 2009 Elsevier Ltd. All rights reserved.

1. Introduction
Owing to a variety of complex optimization problems in a broad
ﬁeld having been confronted successfully with meta-heuristics approaches (Pardalos & Resende, 2002), there has been an increasing
interest in the study of brand new or hybrid algorithms during the
last few years. Another motivation that encourages researchers
might be that the meta-heuristics algorithms are general purpose
optimizers that do not require special conditions or mathematical
properties of the objective functions, such as gradient information
and differentiable properties.
In recent years, Geem, Kim, and Loganathan (2001), Lee and
Geem (2005) proposed a new meta-heuristic algorithm, harmony
search (HS), which imitates the behaviors of music improvisation
process. Similarly, the HS has been successfully applied to several
real-world optimization problems (Geem, 2008). Nevertheless, it
also suffers from a serious problem as other meta-heuristics does;
its capabilities are quite sensitive to parameter setting. Even
though a few of previous studies were concerned with parameter
setting (Geem, 2007; Omran & Mahdavi, 2008), those experiments
were not rigorous enough (e.g. no replications). Furthermore, existing conclusions and heuristic values of parameters are often
obtained from low-dimensional problems (usually less than 10
variables) and one-factor-at-a-time experiments.1 Thus, research* Corresponding author. Tel.: +886 5 5342601x4314; fax: +886 5 5312063.
E-mail addresses: cmwang@ieee.org (C.-M. Wang), huangyf@yuntech.edu.tw
(Y.-F. Huang).
1
One-factor-at-a-time experiments are varied with only one factor or variable at a
time while keeping others ﬁxed (Czitrom, 1999).
0957-4174/$ - see front matter Ó 2009 Elsevier Ltd. All rights reserved.
doi:10.1016/j.eswa.2009.09.008

ers and practitioners might get wrong impressions on the HS and
ignore possible interactions of control parameters.
In this paper, we proposed a new harmony search variant,
whose parameters are automatically adjusted according to its
self-consciousness. In addition, we carefully conducted a set of
experiments to reveal the impact of control parameters. Accordingly, there is no need to tune the control parameters, thereby
leading an almost-parameter-free harmony search algorithm.
The remainder of this paper is organized as follows: in Section
2, a general harmony search algorithm and its recently developed
variants would be reviewed. Section 3 introduced our method that
could automatically adjust parameters, and initialize the harmony
memory with the low-discrepancy sequences as well. Section 4
presented the empirical results and discussions. Finally, conclusions were given in the last section.

2. Previous work
In this section, we brief review the harmony search algorithm
and recently developed variants.
2.1. Original harmony search
The general procedures of harmony search are as follows (Geem
et al., 2001):
Step 1 Create and randomly initialize a harmony memory (HM)
with size HMS.
Step 2 Improvise a new harmony from the HM.

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

2827

Fig. 1. A new harmony improvisation process, where HMS is the memory size, HMCR is the rate of choosing from the memory, PAR is a pitch adjustment rate, and bw is the
distance bandwidth.

Step 3 Update the HM. If the new harmony is better than the
worst harmony in the HM, include the new harmony into
the HM, and exclude the worst harmony from the HM.
Step 4 Repeat Steps 2 and 3 until the maximum number of iterations is reached.
A harmony memory (HM) is a set of solution vectors, and it is
convenient to view the HM as a population of a genetic algorithm.
In fact, the harmony search is completely governed by three rules
in Step 2 – random selection, memory consideration and pitch
adjustment, as shown in Fig. 1.
The parameter HMCR, which varies between 0 and 1, controls
the balance between exploration and exploitation. For example,
an HMCR value 0 behaves like a purely random search; i.e., randomly selecting a value from the possible range of variables. On
the contrary, there is no chance to improve the harmony from outside the HM, so that it merely randomly picks one pitch from the
HM. Once a pitch has been picked from the HM, the pitch adjustment rate (PAR) determines whether further adjustment is required according to a variable distance bandwidth (bw). In other
words, the pitch adjustment step is similar to the local search
mechanism, and the variable distance bandwidth is its step size.
Consequently, it shows clearly that PAR and bw have a great inﬂuence on the quality of ﬁnal solutions.
2.2. Improved harmony search
Because PAR and bw in the HS algorithm control the convergence rate and the ability for ﬁne-tuning, Mahdavi, Fesanghary,
and Damangir (2007) proposed a variant of HS, called the improved
harmony search (IHS), to dynamically increase PAR and decrease
bw, respectively. The IHS gets rid of the weaknesses based on ﬁxed
values of PAR and bw in the HS algorithm.
Indubitably, bw has a considerable inﬂuence on the precision of
solutions and should be problem dependent. Therefore, it seems
reasonable that decreasing bw with an iteration number2 could
ﬁne-tune the ﬁnal solutions. This philosophy is the same as dynamically decreasing the learning rate of neural networks (Haykin,
2007). However, how to decide a suitable set of bwmin and bwmax
becomes another new problem.
On the other hand, increasing PAR continuously in the HS might
be a questionable strategy for some reasons. Since PAR controls the
2

bwðiterÞ ¼ bwmax exp



lnðbwmin ÞÀlnðbwmax Þ
MaxIter

It is a common power law function.


Â iter , where bw is decreased exponentially.

probability to either simply select a pitch from the harmony memory randomly or further adjust a selected pitch, we believe that a
successful search should be proceeded progressively in the beginning, and then gradually settled down. Thus, PAR should be decreased with time to prevent overshooting and oscillation.
Besides, this questionable strategy is apparently contradicted to
the previous statements about bw. It is also interesting to note that
the global-best harmony search (Omran & Mahdavi, 2008) with
relatively small constant PAR instead of a variable, proposed by
the same authors, can obtain better results!
2.3. Global-best harmony search
A recently developed variant of HS, called the global-best harmony search (GHS) (Omran & Mahdavi, 2008), which borrowed
the concepts from swarm intelligence to enhance its performance.
The GHS directly adopts the current best pitch from the harmony
memory to simplify the pitch adjustment step, thereby eliminating
the difﬁculties in selecting bw. Although this variant sounds great
due to its name, there are a number of problems with it.
Because a harmony search belongs to neighborhood meta-heuristics3, it would merely use its own past experiences. For that reason, there are neither swarms nor relative global concepts in the
HS. The term ‘‘global-best” not only seems misused, but also confuses other researchers. Additionally, although selecting the current
best pitch from the harmony memory can keep away from bw, it also
causes a serious side effect – premature convergence (to be discussed later). Moreover, there are some obvious mistakes in the
GHS, so that the reliability of the numerical results is decreased.
3. Proposed method
In this section, we introduced our method and the low-discrepancy sequences for initialization of the harmony memory.
3.1. Self-adaptive mechanism
As stated before, PAR and bw may somewhat inﬂuence the
quality of ﬁnal solutions, so we modiﬁed the pitch adjustment step
of the HS such that the new harmony can better utilize its own
3
There are two major types of meta-heuristics; one is population-based and
another is neighborhood. Neighborhood meta-heuristics such as simulated annealing
(Glover, 1989) and tabu search (Kirkpatrick, Gelatt, & Vecchi, 1983) only evaluate one
potential solution at a time. They are very different from population-based heuristics,
where a set of potential solutions simultaneously move toward goals.

2828

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Fig. 2. Conceptual diagrams of the pitch adjustment mechanism, where HMS ¼ 4; HMCR ¼ 1; PAR ¼ 1, and f ðHM3 Þ < ff ðHM1 Þ; f ðHM2 Þ; f ðHM4 Þg.

Fig. 3. Uniformly distributed random numbers and low-discrepancy sequences.

Table 1
Benchmark functions.
Function
Sphere ¼

Pn

2
i¼1 xi

PnÀ1

100ðx2i À xiþ1 Þ2 þ ð1 À xi Þ2
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ﬃ
À P
Á
P
Ackley ¼ À20 exp À0:2 1n ni¼1 x2i À exp 1n ni¼1 cosð2pxi Þ þ 20 þ expð1Þ


Q
P
À100Þ2
pﬃ
þ1
À ni¼1 cos xi À100
Griewank ¼ ni¼1 ðxi 4000
i

Rosenbrock ¼

i¼1



experiences. More precisely, we replaced the parameter bw altogether and updated the new harmony according to the maximal
and minimal values in the HM. Let minðHM i Þ and maxðHMi Þ,
respectively denote the lowest and the highest values of the ith
variable in the HM, and then the selected pitch from the HM called
a trial is further adjusted by the following equations:
i

i

trial þ ½maxðHM i Þ À trial  Â ran½0; 1Þ
i

i

i

trial À ½trial À minðHM Þ Â ran½0; 1Þ

Function range

xÃ

fmin

À100 6 xi 6 100

½0; . . . ; 0

0

À2:048 6 xi 6 2:048

½1; . . . ; 1

0

À32:768 6 xi 6 32:768

½0; . . . ; 0

0

À600 6 xi 6 600

½100; . . . ; 100

0

where ran[0, 1) is a uniform number in the [0, 1] range without 1.4
Intuitively, the minðHMi Þ and maxðHMi Þ would approach the optimum gradually, and thus this mechanism could progressively make
ﬁner adjustments to the harmony. Besides, it is clear that the pitch
adjustment through this mechanism would not violent the boundary constraint of variables.

ð1Þ
ð2Þ

4
Although we use a half-closed interval [0, 1) here, it can be exchanged by [0, 1]
seamlessly.

2829

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Fig. 4. Full-factorial designed experiments of HMCR and HMS, Dim = 100. All results have been averaged over 50 runs.

Table 2
Default parameters for all methods, where the HS50 ; IHS50 and GHS50 are initialized with the same HMS and HMCR as the proposed method.
Methods

HMS

HMCR

PAR

PARmin

PARmax

bw

bwmin

bwmax

HS
IHS
GHS
HS50
IHS50
GHS50
Our method

5
5
5
50
50
50
50

0.9
0.9
0.9
0.99
0.99
0.99
0.99

0.3
–
–
0.3
–
–
–

–
0.01
0.01
–
0.01
0.01
0

–
0.99
0.99
–
0.99
0.99
1

0.01
–
–
0.01
–
–

–
0.0001
–
–
0.0001
–
–

–
1/(20 Â (UB–LB))
–
–
1/(20 Â (UB–LB))
–
–

Table 3
Initialization ranges for experiments.
Function

Sphere
Rosenbrock
Ackley
Griewank

Symmetric

Positive symmetric

Negative symmetric

Min

Max

Min

Max

Min

Max

À100
À2.048
À32.768
À600

100
2.048
32.768
600

50
1.024
16.384
300

100
2.048
32.768
600

À100
À2.048
À32.768
À600

À50
À1.024
À16.384
À300

Comparing it with the GHS in Fig. 2 where the memory size is 4,
and the other two rules are disabled to simplify our discussion. All
the harmonies in the left ﬁgure have a tendency toward the current
best harmony HM3 , despite its actual quality, and this may lead to
premature convergence. On the contrary, all the harmonies in the
right ﬁgure have a chance to adjust their positions between the
maximal and minimal values of corresponding variables. As
searching goes on, they would eventually settle down near the

global optimum. In addition, we ’decrease’ the PAR linearly with
time, the reasons have been explained in Section 2.2.
3.2. Initialization with the low-discrepancy sequences
If the harmonies can be spread out uniformly in the search
space, it is much more likely to converge to a better solution.
Therefore, we use the low-discrepancy sequences (Lecot, 1989)

2830

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Table 4
Mean and standard deviation of the benchmark function optimization results in 30 dimensions. The results were averaged over 30 runs. The last column indicates the numbers of
signiﬁcance at a ¼ 0:01 between our method and others by two-tailed t-test.
HS

HS50

IHS

IHS50

GHS

GHS50

Ours

t-test

2.7839E+01
(8.5171E+00)

1.5080E+01
(4.2610E+00)

2.8123E+01
(7.2525E+00)

1.3910E+01
(3.3366E+00)

3.0314E+01
(1.3092E+01)

1.4226E+01
(4.2203E+00)

6.9160EÀ07
(1.1035EÀ06)

(6/6)
–

6.9036E+01
(2.5807E+01)
2.1430E+00
(2.2839EÀ01)

6.7302E+01
(2.4564E+01)
2.0563E+00
(2.9215EÀ01)

8.6569E+01
(1.8392E+01)
2.6719E+00
(1.5991EÀ01)

6.5892E+01
(2.4328E+01)
2.0589E+00
(2.4955EÀ01)

8.3760E+01
(1.7622E+01)
2.6267E+00
(2.3225EÀ01)

6.4184E+01
(3.1200E+01)
1.9627E+00
(2.3377EÀ01)

2.6468E+01
(5.6833EÀ01)
7.8081EÀ04
(4.6613EÀ04)

(6/6)
–
(6/6)
–

1.2626E+00
(6.2004EÀ02)

1.1273E+00
(3.5265EÀ02)

1.2590E+00
(8.4917EÀ02)

1.1273E+00
(3.9869EÀ02)

1.2594E+00
(7.9548EÀ02)

1.1300E+00
(3.2676EÀ02)

8.4515EÀ05
(2.3783EÀ04)

(6/6)
–

2.8659E+01
(7.8289E+00)

1.7396E+01
(7.3148E+00)

3.3924E+01
(9.2825E+00)

1.5275E+01
(4.3709E+00)

2.9558E+01
(7.2631E+00)

1.3932E+01
(5.2507E+00)

7.3453EÀ07
(1.0640EÀ06)

(6/6)
–

Rosenbrock

6.7621E+01
(2.4265E+01)

6.4767E+01
(2.3599E+01)

8.8567E+01
(1.2487E+01)

7.1195E+01
(2.2561E+01)

8.0879E+01
(3.0205E+01)

7.4409E+01
(3.2250E+01)

2.6227E+01
(7.7277EÀ01)

(6/6)
–

Ackley

2.0397E+00
(1.7113EÀ01)

2.0186E+00
(2.6183EÀ01)

2.6205E+00
(2.5072EÀ01)

1.9979E+00
(2.4852EÀ01)

2.6282E+00
(2.3894EÀ01)

1.9460E+00
(2.9917EÀ01)

7.3474EÀ04
(4.9978EÀ04)

(6/6)
–

Griewank

1.2607E+00
(7.9909EÀ02)

1.1275E+00
(4.7671EÀ02)

1.2816E+00
(9.1823EÀ02)

1.1380E+00
(3.7465EÀ02)

1.2968E+00
(1.1601EÀ01)

1.1391E+00
(4.7084EÀ02)

2.8618EÀ06
(5.6166EÀ06)

(6/6)
–

2.6991E+01
(6.9635E+00)

1.4283E+01
(4.5010E+00)

2.8015E+01
(7.7629E+00)

1.4800E+01
(3.9032E+00)

2.8738E+01
(6.4934E+00)

1.3522E+01
(4.1964E+00)

8.6273EÀ07
(9.6601EÀ07)

(6/6)
–

Rosenbrock

6.9996E+01
(1.9284E+01)

7.3252E+01
(2.7232E+01)

8.5577E+01
(1.7473E+01)

6.3739E+01
(2.8103E+01)

9.2027E+01
(3.1433E+01)

7.7417E+01
(3.4397E+01)

2.6325E+01
(6.5309EÀ01)

(6/6)
–

Ackley

2.0604E+00
(1.6119EÀ01)
1.2872E+00
(8.5598EÀ02)

2.0043E+00
(2.8361EÀ01)
1.1363E+00
(4.7167EÀ02)

2.6756E+00
(1.9324EÀ01)
1.2703E+00
(1.0100EÀ01)

1.9139E+00
(2.3872EÀ01)
1.1340E+00
(3.8907EÀ02)

2.5796E+00
(1.8703EÀ01)
1.2472E+00
(6.5667EÀ02)

2.0236E+00
(2.0861EÀ01)
1.1393E+00
(3.8543EÀ02)

8.8612EÀ04
(6.3377EÀ04)
1.1649EÀ05
(2.2671EÀ05)

(6/6)
–
(6/6)
–

Symmetric
Sphere
Rosenbrock
Ackley
Griewank

Pos-Asy.
Sphere

Neg-Asy.
Sphere

Griewank

Table 5
Mean and standard deviation of the benchmark function optimization results in 100 dimensions. The results were averaged over 30 runs. The last column indicates the numbers
of signiﬁcance at a ¼ 0:01 between our method and others by two-tailed t-test.
HS

HS50

IHS

IHS50

GHS

GHS50

Ours

t-test

1.4496E+04
(1.2675E+03)

2.1740E+01
(3.8355E+00)

1.5373E+04
(1.2652E+03)

2.4987E+01
(5.1797E+00)

1.6361E+04
(1.0244E+03)

2.0145E+01
(3.0045E+00)

1.5321EÀ02
(1.1839EÀ02)

(6/6)
–

1.2241E+03
(1.2221E+02)

2.4755E+02
(5.0723E+01)

1.1552E+03
(1.0682E+02)

2.6187E+02
(4.5400E+01)

1.1962E+03
(9.5991E+01)

2.6110E+02
(4.2179E+01)

9.5993E+01
(5.4020EÀ01)

(6/6)
–

1.2487E+01
(2.2604EÀ01)
1.4061E+02
(1.0843E+01)

1.2294E+00
(1.3223EÀ01)
1.2050E+00
(3.4919EÀ02)

1.2607E+01
(3.0371EÀ01)
1.4207E+02
(1.2511E+01)

1.1956E+00
(9.6269EÀ02)
1.2107E+00
(3.2800EÀ02)

1.2801E+01
(2.3092EÀ01)
1.4635E+02
(1.3283E+01)

1.2179E+00
(1.3273EÀ01)
1.1687E+00
(3.3162EÀ02)

2.9248EÀ02
(9.8086EÀ03)
6.6646EÀ03
(3.8819EÀ03)

(6/6)
–
(6/6)
–

1.4920E+04
(1.2797E+03)
1.1830E+03
(1.1440E+02)
1.2543E+01
(2.7652EÀ01)
1.4034E+02
(1.3586E+01)

2.0737E+01
(3.0322E+00)
2.4551E+02
(3.3002E+01)
1.2489E+00
(1.0997EÀ01)
1.2044E+00
(3.3912EÀ02)

1.5441E+04
(1.6554E+03)
1.1463E+03
(1.0379E+02)
1.2669E+01
(2.9887EÀ01)
1.4087E+02
(8.4121E+00)

2.2659E+01
(3.0208E+00)
2.2606E+02
(3.9461E+01)
1.2100E+00
(1.7145EÀ01)
1.2115E+00
(3.1230EÀ02)

1.6590E+04
(1.5075E+03)
1.2071E+03
(9.6643E+01)
1.2862E+01
(2.9501EÀ01)
1.4812E+02
(1.3518E+01)

2.0800E+01
(3.5454E+00)
2.5063E+02
(3.9527E+01)
1.2192E+00
(1.2295EÀ01)
1.1700E+00
(2.4150EÀ02)

1.1234EÀ02
(9.0235EÀ03)
9.6320E+01
(5.9583EÀ01)
2.6441EÀ02
(6.7872EÀ03)
4.0883EÀ03
(2.4742EÀ03)

(6/6)
–
(6/6)
–
(6/6)
–
(6/6)
–

1.5005E+04
(1.0968E+03)

2.1909E+01
(3.4901E+00)

1.5495E+04
(1.4779E+03)

2.1822E+01
(3.8574E+00)

1.6212E+04
(1.1895E+03)

1.9844E+01
(3.2543E+00)

1.2420EÀ02
(1.1817EÀ02)

(6/6)
–

Rosenbrock

1.2147E+03
(1.1992E+02)

2.4272E+02
(4.0825E+01)

1.1747E+03
(1.0612E+02)

2.4062E+02
(4.6921E+01)

1.1780E+03
(1.0655E+02)

2.5337E+02
(3.6112E+01)

9.6473E+01
(5.2266EÀ01)

(6/6)
–

Ackley

1.2533E+01
(2.9300EÀ01)

1.2012E+00
(1.1066EÀ01)

1.2537E+01
(3.0133EÀ01)

1.1945E+00
(9.9816EÀ02)

1.2840E+01
(2.2510EÀ01)

1.2725E+00
(1.3915EÀ01)

2.8970EÀ02
(6.1588EÀ03)

(6/6)
–

Griewank

1.4903E+02
(1.2065E+01)

1.2141E+00
(3.3635EÀ02)

1.4157E+02
(1.2213E+01)

1.2041E+00
(2.4172EÀ02)

1.4549E+02
(1.2958E+01)

1.1887E+00
(3.2901EÀ02)

9.7983EÀ03
(9.7899EÀ03)

(6/6)
–

Symmetric
Sphere
Rosenbrock
halﬂine
Ackley
Griewank

Pos-Asy.
Sphere
Rosenbrock
Ackley
Griewank

Neg-Asy.
Sphere

2831

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Sphere

Rosenbrock

5

4

4

3

3

2

2

1

1

0
0

2

4

6

8

10
4

x 10

2
1
2

4

6

8

10
4

4000

2000

15
10

1000

5
2

4

6

8

10

0
0

2

4

6

4

4

x 10

10

25
20

3

15

2

10

1

5
2

4

6

8

10
4

4

6

0
0

8

10
4

x 10

3000
2000
1000
2

4

6

8

10
4

0
0

4

6

8

10

x 10

4

15

300

2

4

x 10
400

2000

2

4000

x 10

1

0
0

x 10

4

0
0

8

4

x 10

1.5

3000

3000

25

x 10

x 10

10

200
0.5

1000
0
0

Griewank

20

0
0
5

3

0
0

x 10

x 10

5

4

Ackley

4

x 10

2

4

6

8

10
4

x 10

0
0

5

100
2

4

6

8

10

0
0

2

4

6

4

x 10

8

10
4

x 10

0
0

2

4

6

8

10
4

x 10

Fig. 5. Comparisons among seven methods with symmetric initial range on four test functions in 100 dimensions. The dash lines indicate the results of the proposed method.
The apparent red lines indicate the results of the original HS, IHS and GHS, and the green lines indicate the HS50 ; IHS50 and GHS50 . They are too close to be distinct. Three rows
respectively indicate the best result, the average result, and the diversity. The vertical axes of the ﬁrst two rows and the third row are the function values and the diversity,
respectively. The horizontal axis is the number of function evaluations. All results have been averaged over 30 runs.

Fig. 6. 2 D surfaces of test functions

2832

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Fig. 7. Full-factorial designed experiments of HMCR and HMS, Dim = 30. All results have been averaged over 50 runs.

Table 6
The p-values of the two-tailed t-test between our method and others in 30 dimension. The results were averaged over 30 runs.
HS

HS50

IHS

IHS50

GHS

GHS50

Symmetric
Sphere
Rosenbrock
Ackley
Griewank

3.2303EÀ17
6.2439EÀ10
4.9400EÀ30
9.6199EÀ40

3.8285EÀ18
5.2750EÀ10
1.8338EÀ26
2.0410EÀ45

3.1999EÀ19
3.1505EÀ17
2.9539EÀ37
9.2651EÀ36

4.3924EÀ20
9.1461EÀ10
1.9701EÀ28
7.1500EÀ44

2.3398EÀ13
3.5985EÀ17
2.2961EÀ32
1.3926EÀ36

1.4179EÀ17
2.9430EÀ07
1.1950EÀ28
2.0839EÀ46

Pos-Asy.
Sphere
Rosenbrock
Ackley
Griewank

1.5341EÀ18
2.9840EÀ10
5.0770EÀ33
1.5399EÀ36

1.2043EÀ13
7.7133EÀ10
1.3667EÀ27
1.2627EÀ41

1.6041EÀ18
2.3986EÀ22
2.2209EÀ31
5.2996EÀ35

5.3723EÀ18
8.5648EÀ12
4.1375EÀ28
9.0001EÀ45

8.5214EÀ20
8.0411EÀ11
5.1102EÀ32
3.1917EÀ32

7.5778EÀ15
5.0367EÀ09
1.7255EÀ25
6.5546EÀ42

Neg-Asy.
Sphere
Rosenbrock
Ackley
Griewank

3.2354EÀ19
3.9576EÀ13
6.7701EÀ34
6.1617EÀ36

7.0979EÀ17
2.3973EÀ10
1.6363EÀ26
7.4059EÀ42

2.2567EÀ18
1.1496EÀ17
6.7336EÀ35
1.0666EÀ33

5.8945EÀ19
4.9527EÀ08
4.4744EÀ28
2.9773EÀ44

8.4402EÀ21
2.7994EÀ12
7.5300EÀ35
7.2148EÀ39

4.7238EÀ17
5.6821EÀ09
1.9042EÀ30
1.9789EÀ44

instead of the pseudo-random number5 to initialize the harmony
memory. The reason can be easily realized from Fig. 3.
Note the obvious differences between the random data and the
low-discrepancy sequences. This is exactly how a uniformly-distributed random generator is expected to behave, since it generates
the random numbers independently. In contrast, the low-discrepancy sequences are much more evenly distributed although there
are some correlations between variables. However, this is the distribution we desired.
5

We used the famous Mersenne Twister (MT19937) (Matsumoto & Nishimura,
1998) as a pseudo-random number generator in this work.

4. Empirical results
In this section, four common numerical optimization problems
were chosen to compare the performance of our method against
the original harmony search (HS), the improved harmony search
(IHS), and the global-best harmony search (GHS).
4.1. Standard test functions
The four numerical optimization problems used in our empirical studies are the unimodal Sphere and Rosenbrock functions,
and the multimodal Ackley and Griewank functions. All functions

2833

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837
Table 7
The p-values of the two-tailed t-test between our method and others in 100 dimension. The results were averaged over 30 runs.
HS

HS50

IHS

IHS50

GHS

GHS50

Symmetric
Sphere
Rosenbrock
Ackley
Griewank

1.6542EÀ32
7.7982EÀ30
1.9902EÀ52
4.4317EÀ34

8.5389EÀ24
3.4624EÀ16
7.8589EÀ30
3.4987EÀ47

2.8863EÀ33
9.9802EÀ31
9.4161EÀ49
2.0304EÀ32

7.8362EÀ22
1.6062EÀ18
1.0753EÀ33
3.6086EÀ48

1.0802EÀ36
1.5450EÀ32
1.8224EÀ52
4.8442EÀ32

7.3672EÀ26
2.4572EÀ19
1.1638EÀ29
1.5103EÀ47

Pos-Asy.
Sphere
Rosenbrock
Ackley
Griewank

9.4889EÀ33
3.4312EÀ30
7.7079EÀ50
3.0935EÀ31

4.1760EÀ26
4.6108EÀ21
2.5707EÀ32
4.4414EÀ47

5.7852EÀ30
5.6134EÀ31
5.5785EÀ49
2.7488EÀ37

2.9988EÀ27
2.7389EÀ17
2.8172EÀ26
2.8502EÀ48

5.0012EÀ32
1.4266EÀ32
2.4585EÀ49
5.6721EÀ32

3.2196EÀ24
2.6360EÀ19
1.4223EÀ30
1.9435EÀ51

Neg-Asy.
Sphere
Rosenbrock
Ackley
Griewank

9.4377EÀ35
5.8275EÀ30
4.3787EÀ49
1.8032EÀ33

4.7495EÀ25
2.7392EÀ18
1.1126EÀ31
9.4274EÀ53

2.0149EÀ31
4.9567EÀ31
9.8231EÀ49
1.1208EÀ32

8.9764EÀ24
1.6670EÀ16
6.1775EÀ33
3.7473EÀ63

1.0530EÀ34
5.1031EÀ31
9.7353EÀ53
2.8158EÀ32

1.0822EÀ24
1.3939EÀ20
1.6700EÀ29
5.0948EÀ53

Sphere

Rosenbrock

5

4

4

3

3

2

2

1

1

0
0

2

4

6

8

10

x 10

x 10

10

1
2

4

6

8

10
4

2

4

6

8

10

x 10

4

6

3

15

2

10

1

5
2

4

6

8

10
4

0
0

10

4

6

0
0

8

10
4

x 10
4000
3000
2000
1000

2

4

6

8

10

0
0

4

x 10

x 10

400

2

4

6

8

10
4

x 10

4

15

300

1

2

4

20

0
0

8

25

2000

x 10

10

200
0.5

1000
0
0

2

x 10

4

1.5

3000

0
0

x 10

x 10
4000

1000

5

4

5

2

2000

15

4

3

3000

20

0
0

x 10

5

0
0

Griewank

25

4

4

Ackley

4

x 10

2

4

6

8

10
4

x 10

0
0

5

100
2

4

6

8

10
4

x 10

0
0

2

4

6

8

10
4

x 10

0
0

2

4

6

8

10
4

x 10

Fig. 8. Comparisons among seven methods with positive symmetric initial range on four test functions in 100 dimensions. The dash lines indicate the results of the proposed
method. The apparent red lines indicate the results of the original HS, IHS and GHS, and the green lines indicate the HS50 ; IHS50 and GHS50 . They are too close to be distinct.
Three rows respectively indicate the best result, the average result, and the diversity. The vertical axes of the ﬁrst two rows and the third row are the function values and the
diversity, respectively. The horizontal axis is the number of function evaluations. All results have been averaged over 30 runs.

were implemented in 30 and 100 dimensions. Table 1 summarizes
the information of these test functions.
4.2. Design of experiments

and 100 dimensions, and the results are presented in Fig. 4 and
Fig. 7, respectively. All results have been averaged over 50 runs,
and each run was allowed to run for 100,000 evaluations of the
objective functions in 100 dimensions so as to preserve convergence.
It is obvious from Fig. 4 that increasing the HMCR value improves the
performance for all functions, and an appropriate value of the HMS is
around 50. Similar results were also obtained in 30 dimensions (see
Fig. 7).

In order to reveal the impact of control parameters, we conducted full-factorial experiments6 (Montgomery & Runger, 2006)
for our method, where HMS = 3, 5, 10, 20, 30, 50, 100, 150, 200 (9
levels) and HMCR = 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99 (7 levels). We
conducted two sets of experiments on each test functions in 30

4.3. Experimental settings

6
Some might argue, of course, that the fractional factorial experiment is a much
more experimentally efﬁcient method, but there is no need to save little time for
computer simulations.

For all tests presented in this paper, the default parameters of
all methods are listed in Table 2. These values were suggested by
their authors Lee and Geem (2005), Mahdavi et al. (2007), Omran

2834

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Sphere

Rosenbrock

5

4

4

3

3

2

2

1

1

0
0

2

4

6

8

4

x 10

5

2
1
2

Griewank

4

6

8

25

10
4

3000

20
2000

15
10

1000

5
2

4

6

8

10

0
0

2

4

6

x 10

20

3

15

2

10

1

5

0
0

0
0

2

4

6

8

10

8

10
4

2000
1000
2

4

6

8

0
0

10
4

x 10

2

4

6

15

8

10
4

x 10

4

300

2000

6

x 10

400

1

4

3000

x 10
1.5

2

4000

4

4

3000

0
0

10

x 10
25

x 10
4000

8

4

4

x 10

4

3

0
0

x 10

0
0

10

x 10

5

4

Ackley

4

x 10

x 10

10

200
0.5

1000
0
0

2

4

6

8

0
0

10
4

5

100
2

4

6

8

10

0
0

2

4

6

4

x 10

8

0
0

10
4

x 10

2

4

6

8

10
4

x 10

x 10

Fig. 9. Comparisons among seven methods with negative symmetric initial range on four test functions in 100 dimensions. The dash lines indicate the results of the
proposed method. The apparent red lines indicate the results of the original HS, IHS and GHS, and the green lines indicate the HS50 ; IHS50 and GHS50 . They are too close to be
distinct. Three rows respectively indicate the best result, the average result, and the diversity. The vertical axes of the ﬁrst two rows and the third row are the function values
and the diversity, respectively. The horizontal axis is the number of function evaluations. All results have been averaged over 30 runs.

Sphere

Rosenbrock

Ackley

Griewank

4

10

x 10

10000

25

8

8000

20

6

6000

15

4

4000

10

2

2000

5

800
600
400

0
0

1

2

3

0
0

1

2

4

10

x 10

15000

10000

4

5000

2
0
0

1

2

3

0
0

1

2

4

3

3

1000

20

800

15

600

10

400

5

200

0
0

1

2

3

3
4

0
0

1

2

4

3
4

x 10

x 10

4

15

x 10

10

200
0.5

1000
0
0

2
x 10

25

300

2000

1

x 10

400

1

0
0

4

x 10
1.5

3000

2

4

x 10
4000

1

x 10

8
6

0
0

4

x 10

4

3

200

5

100
1

2

3
4

x 10

0
0

1

2

3
4

x 10

0
0

1

2

3
4

x 10

0
0

1

2

3
4

x 10

Fig. 10. Comparisons among seven methods with symmetric initial range on four test functions in 30 dimensions. The dash lines indicate the results of the proposed method.
The apparent red lines indicate the results of the original HS, IHS and GHS, and the green lines indicate the HS50 ; IHS50 and GHS50 . They are too close to be distinct. Three rows
respectively indicate the best result, the average result, and the diversity. The vertical axes of the ﬁrst two rows and the third row are the function values and the diversity,
respectively. The horizontal axis is the number of function evaluations. All results have been averaged over 30 runs.

2835

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Sphere

Rosenbrock

Ackley

Griewank

4

x 10

10000

25

8

8000

20

6

6000

15

4

4000

10

2

2000

5

10

800
600
400

0
0

1

2

3

0
0

1

2

10

15000

8
10000

6
4

5000

2
0
0

1

2

3

0
0

1

2

4

3

1.5

2

1000

20

800

15

600

10

400

5

200

0
0

1

2

3
4

x 10

25

0
0

3

1

2

4

3
4

x 10

x 10

4

15

300

2000

1

x 10

400

1

0
0

3
4

x 10

3000

x 10

10

200
0.5

1000
0
0

2

4

x 10
4000

1

x 10

x 10

x 10

0
0

4

4

4

3

200

5

100
1

2

3
4

x 10

0
0

1

2

3

0
0

1

4

2

3
4

x 10

x 10

0
0

1

2

3
4

x 10

Fig. 11. Comparisons among seven methods with positive symmetric initial range on four test functions in 30 dimensions. The dash lines indicate the results of the proposed
method. The apparent red lines indicate the results of the original HS, IHS and GHS, and the green lines indicate the HS50 ; IHS50 and GHS50 . They are too close to be distinct.
Three rows respectively indicate the best result, the average result, and the diversity. The vertical axes of the ﬁrst two rows and the third row are the function values and the
diversity, respectively. The horizontal axis is the number of function evaluations. All results have been averaged over 30 runs.

and Mahdavi (2008), and the parameters for our method were
obtained from the observations of factorial experiments. Additionally, in order to ensure a fair comparison, we also raised the HMS
and the HMCR of the original HS, IHS and GHS to 50 and 0.99,
respectively.
Following the advices of Fogel and Beyer (1995) and Angeline
(1998) for those problems with optima at or near the origin, the
HM was asymmetrically initialized in the search space. Three initialization methods were used in this paper, and the initial HM
of all methods was generated by the low-discrepancy sequence
generator in the ranges speciﬁed in Table 3.

4.4. Numerical results
The ﬁnal numerical results averaged over 30 runs in 30 and 100
dimensions are given in Tables 4 and 5. As shown in these tables,
the numerical results clearly indicate that the proposed method
signiﬁcantly outperforms all other methods under 99% conﬁdence
interval t-test in all test functions. From the perspective of initialization ranges, the performances of all methods in all test functions
are generally quite consistent. In other words, the harmony search
and its variants are not affected by the different initialization
ranges. Since the location of the optima in the real-world problem
are generally not known, this property guarantees a consistent result if the optima are not included in the initialization range. Besides, it is worth pointing out that the HS50 ; IHS50 and GHS50 get
better performances than those methods with suggested
parameters!

4.5. Analysis of dynamics
In order to realize the dynamic behavior of the harmony search,
we deﬁned a diversity measure as follow:

Diversity ¼

D
1 X
VARðHMi Þ
Á
D i¼1

ð3Þ

where D is the dimension of the HM, and VAR is the variance of each
variable (dimension). The total variance of the HM is divided by D to
reduce the effect of the dimension.
As shown in Fig. 5, we plotted the learning curve of the best and
average of the function values over iterations for all test functions
in the ﬁrst two rows. In addition, the third row is the diversity of
the HM across iterations. Although these lines are overlapped,
three apparent groups still can be distinguished. It is obvious from
the third row that our method preserves a certain degree of diversity, and ﬁnally converges to a better solution. However, all other
methods have a strong tendency to converge prematurely to relatively poor solutions. Especially for the Ackley function, the original HS, IHS and GHS get stuck in the local optimum very soon, so
that they are hopeless to reach a better solution near the global
optimum. Despite that the HS50 ; IHS50 and GHS50 beneﬁt from the
larger HMCR and HMS, they still have a tendency to converge to
non-optimal solutions. It is clearly evident by the diversity curve
of the Ackley function and the numerical results. In general, our
method decreases the function values and the diversity gracefully
in all test functions, i.e., it could continuously ﬁnd better solutions

2836

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837

Sphere

Rosenbrock

Ackley

Griewank

4

x 10

10000

25

1000

8

8000

20

800

6

6000

15

600

4

4000

10

400

2

2000

5

200

10

0
0

1

2

0
0

1

2

x 10

3

15000
10000

4

5000

2
0
0

1

2

3

0
0

1

2

4

3

2

25

1000

20

800

15

600

10

400

5

200

0
0

1

2

3
4

x 10

3
4

0
0

x 10

1

2

3
4

x 10

4

15

300

1

1

x 10

400

2000

x 10

10

200
0.5

1000
0
0

0
0

3
4

x 10
1.5

3000

2

4

x 10
4000

1

x 10

8
6

0
0

4

x 10

4

10

3
4

1

2

3
4

x 10

0
0

5

100
1

2

3

0
0

1

2

3
4

4

x 10

x 10

0
0

1

2

3
4

x 10

Fig. 12. Comparisons among seven methods with negative symmetric initial range on four test functions in 30 dimensions. The dash lines indicate the results of the
proposed method. The apparent red lines indicate the results of the original HS, IHS and GHS, and the green lines indicate the HS50 ; IHS50 and GHS50 . They are too close to be
distinct. Three rows respectively indicate the best result, the average result, and the diversity. The vertical axes of the ﬁrst two rows and the third row are the function values
and the diversity, respectively. The horizontal axis is the number of function evaluations. All results have been averaged over 30 runs.

through the iterations, and guarantee the convergence to a better
solution.

Appendix A
A.1. Standard test functions
See Fig. 6.

5. Conclusions
In this paper, we proposed a new harmony search variant, the
parameters in which are automatically adapted based on past
experiences. This automatic adjustment mechanism not only alleviates the difﬁculties of parameter setting, but also enhances the
precision of solutions. In addition, we carefully conducted a set
of experiments to understand the impact of control parameters.
It was observed that a large value of HMCR and a moderate size
of harmony memory (i.e., 50) are a suitable set of parameters.
The results remained consistent in high dimensional problems.
Based on these observations, the proposed method was compared with the original HS, IHS, and GHS that used two set of
parameters; i.e., the default parameters provided by their authors
and the parameters obtained from the full-factorial experiments.
The numerical results indicated that our method offers much superior performance to the existing methods on four optimization
problems. Besides, it is interesting to note that these three methods can get better performance with the parameters obtained from
the factorial experiments. This circumstance conﬁrmed our criticisms on existing conclusions and heuristic values of parameters
in Section 1. Furthermore, our method in contrast to others has a
better diversity and always converges to a better solution eventually. We are currently investigating our method with more difﬁcult
problems, especially in higher dimensions.

A.2. Design of experiments
See Fig. 7.
A.3. Numerical results
See Tables 6 and 7.
A.4. Analysis of dynamics
See Figs. 8–12.
References
Angeline, P. J. (1998). Evolutionary optimization versus particle swarm
optimization: Philosophy and performance differences. In EP ’98: Proceedings
of the 7th international conference on evolutionary programming VII (pp. 601–
610). London, UK: Springer-Verlag.
Czitrom, V. (1999). One-factor-at-a-time versus designed experiments. The
American Statistician, 53(2), 126–131.
Fogel, D. B., & Beyer, H.-G. (1995). A note on the empirical evaluation of
intermediate recombination. Evolution Computing, 3(4), 491–495.
Geem, Z. (2007). Harmony search algorithm for solving sudoku. In Knowledge-based
intelligent information and engineering systems of lecture notes in computer science
(Vol. 4692, pp. 371–378).

C.-M. Wang, Y.-F. Huang / Expert Systems with Applications 37 (2010) 2826–2837
Geem, Z. W. (2008). Harmony search applications in industry. Soft Computing
Applications in Industry, 226, 117–134.
Geem, Z. W., Kim, J. H., & Loganathan, G. V. (2001). A new heuristic optimization
algorithm: Harmony search. SIMULATION, 76(2), 60–68.
Glover, F. (1989). Tabu search – Part I. ORSA Journal on Computing, 1(3), 190–206.
Haykin, S. (2007). Neural networks: A comprehensive foundation (3rd ed.). Upper
Saddle River, NJ, USA: Prentice-Hall, Inc..
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated
annealing. Science, 220(4598), 671–680 (Number 4598, 13 May 1983).
Lecot, C. (1989). An algorithm for generating low discrepancy sequences on vector
computers. Parallel Computing, 11(1), 113–116.
Lee, K. S., & Geem, Z. W. (2005). A new meta-heuristic algorithm for continuous
engineering optimization: Harmony search theory and practice. Computer
Methods in Applied Mechanics and Engineering, 194(36–38), 3902–3933.

2837

Mahdavi, M., Fesanghary, M., & Damangir, E. (2007). An improved harmony search
algorithm for solving optimization problems. Applied Mathematics and
Computation, 188, 1567–1579.
Matsumoto, M., & Nishimura, T. (1998). Mersenne twister: A 623-dimensionally
equidistributed uniform pseudo-random number generator. ACM Transactions
on Modeling Computer Simulation, 8(1), 3–30.
Montgomery, D. C., & Runger, G. C. (2006). Applied statistics and probability for
engineers. Wiley.
Omran, M. G., & Mahdavi, M. (2008). Global-best harmony search. Applied
Mathematics and Computation, 198(2), 643–656.
Pardalos, P. M., & Resende, M. G. C. (Eds.). (2002). Handbook of applied optimization.
Oxford University Press.

